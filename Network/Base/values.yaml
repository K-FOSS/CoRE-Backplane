peers: []

clusterName: 'k0s-dc1'
pubPool: '66.165.222.101/32'

envoy-gw:
  enabled: true
  deployment:
    envoyGateway:
      cert:
        expiryDays: 365
      image:
        repository: docker.io/envoyproxy/gateway
        tag: 'v0.6.0'
      imagePullPolicy: Always
      resources:
        limits:
          cpu: 500m
          memory: 1024Mi
        requests:
          cpu: 100m
          memory: 256Mi
    ports:
      - name: grpc
        port: 18000
        targetPort: 18000
      - name: ratelimit
        port: 18001
        targetPort: 18001
    replicas: 1
    pod:
      annotations: {}
      labels: {}

  config:
    envoyGateway:
      gateway:
        controllerName: gateway.envoyproxy.io/gatewayclass-controller
      provider:
        type: Kubernetes
      logging:
        level:
          default: info

  envoyGatewayMetricsService:
    ports:
      - name: http
        port: 19001
        protocol: TCP
        targetPort: 19001

  createNamespace: false

  kubernetesClusterDomain: cluster.local

  certgen:
    job:
      annotations: {}
      ttlSecondsAfterFinished: 0
    rbac:
      annotations: {}
      labels: {}


vmtools:
  enabled: false

submariner-k8s-broker:
  enabled: true

  fullnameOverride: 'submariner-broker'

  rbac:
    create: true

  crd:
    create: true

  serviceAccounts:
    client:
      create: true
      name: ''

submariner-operator:
  submariner:
    loadBalancerEnabled: false
    natEnabled: false
    colorCodes: blue
    debug: true
    serviceDiscovery: true
    cableDriver: "libreswan"
    healthcheckEnabled: true
    coreDNSCustomConfig: {}
    images:
      repository: quay.io/submariner
      tag: ""

  broker:
    server: '10.1.1.40:6443'
    namespace: submariner-k8s-broker
  
    insecure: false
    
    globalnet: false

  rbac:
    create: true
  images: {}

  ipsec:
    psk: <path:CORE0_SITE1/data/Network/Submariner#Token>
    debug: false
    forceUDPEncaps: false
    ikePort: 500
    natPort: 4500

  leadership:
    leaseDuration: 10
    renewDeadline: 5
    retryPeriod: 2

  operator:
    image:
      repository: quay.io/submariner/submariner-operator
      tag: ""
      pullPolicy: IfNotPresent
    resources: {}
    tolerations: []
    affinity: {}

cilium:
  enabled: true

  # upgradeCompatibility helps users upgrading to ensure that the configMap for
  # Cilium will not change critical values to ensure continued operation
  # This is flag is not required for new installations.
  # For example: 1.7, 1.8, 1.9
  # upgradeCompatibility: '1.8'

  securityContext:
    # -- User to run the pod with
    # runAsUser: 0
    # -- Run the pod with elevated privileges
    privileged: true
    capabilities:
      # -- Capabilities for the `cilium-agent` container
      ciliumAgent:
        # Use to set socket permission
        - CHOWN
        # Used to terminate envoy child process
        - KILL
        # Used since cilium modifies routing tables, etc...
        - NET_ADMIN
        # Used since cilium creates raw sockets, etc...
        - NET_RAW
        # Used since cilium monitor uses mmap
        - IPC_LOCK
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # We need it for now but might not need it for >= 5.11 specially
        # for the 'SYS_RESOURCE'.
        # In >= 5.8 there's already BPF and PERMON capabilities
        - SYS_ADMIN
        # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
        - SYS_RESOURCE
        # Both PERFMON and BPF requires kernel 5.8, container runtime
        # cri-o >= v1.22.0 or containerd >= v1.5.0.
        # If available, SYS_ADMIN can be removed.
        #- PERFMON
        #- BPF
        # Allow discretionary access control (e.g. required for package installation)
        - DAC_OVERRIDE
        # Allow to set Access Control Lists (ACLs) on arbitrary files (e.g. required for package installation)
        - FOWNER
        # Allow to execute program that changes GID (e.g. required for package installation)
        - SETGID
        # Allow to execute program that changes UID (e.g. required for package installation)
        - SETUID
      # -- Capabilities for the `mount-cgroup` init container
      mountCgroup:
        # Only used for 'mount' cgroup
        - SYS_ADMIN
        # Used for nsenter
        - SYS_CHROOT
        - SYS_PTRACE
      # -- capabilities for the `apply-sysctl-overwrites` init container
      applySysctlOverwrites:
        # Required in order to access host's /etc/sysctl.d dir
        - SYS_ADMIN
        # Used for nsenter
        - SYS_CHROOT
        - SYS_PTRACE
      # -- Capabilities for the `clean-cilium-state` init container
      cleanCiliumState:
        # Most of the capabilities here are the same ones used in the
        # cilium-agent's container because this container can be used to
        # uninstall all Cilium resources, and therefore it is likely that
        # will need the same capabilities.
        # Used since cilium modifies routing tables, etc...
        - NET_ADMIN
        # Used in iptables. Consider removing once we are iptables-free
        - SYS_MODULE
        # We need it for now but might not need it for >= 5.11 specially
        # for the 'SYS_RESOURCE'.
        # In >= 5.8 there's already BPF and PERMON capabilities
        - SYS_ADMIN
        # Could be an alternative for the SYS_ADMIN for the RLIMIT_NPROC
        - SYS_RESOURCE
        # Both PERFMON and BPF requires kernel 5.8, container runtime
        # cri-o >= v1.22.0 or containerd >= v1.5.0.
        # If available, SYS_ADMIN can be removed.
        #- PERFMON
        #- BPF

  debug:
    # -- Enable debug logging
    enabled: true
    # verbose:

  rbac:
    # -- Enable creation of Resource-Based Access Control configuration.
    create: true

  # -- Configure image pull secrets for pulling container images
  imagePullSecrets:
  # - name: "image-pull-secret"

  # kubeConfigPath: ~/.kube/config
  # k8sServiceHost:
  # k8sServicePort:

  cluster:
    # -- Name of the cluster. Only required for Cluster Mesh.
    name: default
    # -- (int) Unique ID of the cluster. Must be unique across all connected
    # clusters and in the range of 1 to 255. Only required for Cluster Mesh,
    # may be 0 if Cluster Mesh is not used.
    id: 0

  # -- Configure termination grace period for cilium-agent DaemonSet.
  terminationGracePeriodSeconds: 1

  # -- Install the cilium agent resources.
  agent: true

  # -- Agent container name.
  name: cilium

  # -- Roll out cilium agent pods automatically when configmap is updated.
  rollOutCiliumPods: false

  # -- Affinity for cilium-agent.
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: liqo.io/type
                operator: DoesNotExist

    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - topologyKey: kubernetes.io/hostname
        labelSelector:
          matchLabels:
            k8s-app: cilium

  # -- Node selector for cilium-agent.
  nodeSelector:
    kubernetes.io/os: linux

  # -- Node tolerations for agent scheduling to nodes with taints
  # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  tolerations:
  - operator: Exists
    # - key: "key"
    #   operator: "Equal|Exists"
    #   value: "value"
    #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

  # -- Annotations to be added to agent pods
  podAnnotations: {}

  # -- Labels to be added to agent pods
  podLabels:
    logs: loki-myloginspace

  # -- Agent resource limits & requests
  # ref: https://kubernetes.io/docs/user-guide/compute-resources/
  resources: {}
    # limits:
    #   cpu: 4000m
    #   memory: 4Gi
    # requests:
    #   cpu: 100m
    #   memory: 512Mi

  # -- Cilium agent update strategy
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 2

  # Configuration Values for cilium-agent

  aksbyocni:
    # -- Enable AKS BYOCNI integration.
    # Note that this is incompatible with AKS clusters not created in BYOCNI mode:
    # use Azure integration (`azure.enabled`) instead.
    enabled: false

  # -- Enable installation of PodCIDR routes between worker
  # nodes if worker nodes share a common L2 network segment.
  autoDirectNodeRoutes: false

  # -- Annotate k8s node upon initialization with Cilium's metadata.
  annotateK8sNode: false

  # -- Enable bandwidth manager to optimize TCP and UDP workloads and allow
  # for rate-limiting traffic from individual Pods with EDT (Earliest Departure
  # Time) through the "kubernetes.io/egress-bandwidth" Pod annotation.
  bandwidthManager:
    # -- Enable bandwidth manager infrastructure (also prerequirement for BBR)
    enabled: false
    # -- Activate BBR TCP congestion control for Pods
    bbr: false


  # -- Configure standalone NAT46/NAT64 gateway
  nat46x64Gateway:
    # -- Enable RFC8215-prefixed translation
    enabled: false
  
  # -- EnableHighScaleIPcache enables the special ipcache mode for high scale
  # clusters. The ipcache content will be reduced to the strict minimum and
  # traffic will be encapsulated to carry security identities.
  highScaleIPcache:
    # -- Enable the high scale mode for the ipcache.
    enabled: false
  
  # -- Configure L2 announcements
  l2announcements:
    # -- Enable L2 announcements
    enabled: false
    # -- If a lease is not renewed for X duration, the current leader is considered dead, a new leader is picked
    # leaseDuration: 15s
    # -- The interval at which the leader will renew the lease
    # leaseRenewDeadline: 5s
    # -- The timeout between retries if renewal fails
    # leaseRetryPeriod: 2s
  
  # -- Configure L2 pod announcements
  l2podAnnouncements:
    # -- Enable L2 pod announcements
    enabled: false
    # -- Interface used for sending Gratuitous ARP pod announcements
    interface: "eth0"

  # -- This feature set enables virtual BGP routers to be created via
  # CiliumBGPPeeringPolicy CRDs.
  bgpControlPlane:
    # -- Enables the BGP control plane.
    enabled: true

  pmtuDiscovery:
    # -- Enable path MTU discovery to send ICMP fragmentation-needed replies to
    # the client.
    enabled: true

  bpf:
    autoMount:
      # -- Enable automatic mount of BPF filesystem
      # When `autoMount` is enabled, the BPF filesystem is mounted at
      # `bpf.root` path on the underlying host and inside the cilium agent pod.
      # If users disable `autoMount`, it's expected that users have mounted
      # bpffs filesystem at the specified `bpf.root` volume, and then the
      # volume will be mounted inside the cilium agent pod at the same path.
      enabled: true

    lbExternalClusterIP: true

    hostLegacyRouting: false

    masquerade: true
    tproxy: false

    # -- Enables pre-allocation of eBPF map values. This increases
    # memory usage but can reduce latency.
    preallocateMaps: false


  # -- Clean all eBPF datapath state from the initContainer of the cilium-agent
  # DaemonSet.
  #
  # WARNING: Use with care!
  cleanBpfState: false

  # -- Clean all local Cilium state from the initContainer of the cilium-agent
  # DaemonSet. Implies cleanBpfState: true.
  #
  # WARNING: Use with care!
  cleanState: false

  # -- Wait for KUBE-PROXY-CANARY iptables rule to appear in "wait-for-kube-proxy"
  # init container before launching cilium-agent.
  # More context can be found in the commit message of below PR
  # https://github.com/cilium/cilium/pull/20123
  waitForKubeProxy: false

  cni:
    # -- Install the CNI configuration and binary files into the filesystem.
    install: true

    # -- Remove the CNI configuration and binary files on agent shutdown. Enable this
    # if you're removing Cilium from the cluster. Disable this to prevent the CNI
    # configuration file from being removed during agent upgrade, which can cause
    # nodes to go unmanageable.
    uninstall: false

    # -- Configure chaining on top of other CNI plugins. Possible values:
    #  - none
    #  - aws-cni
    #  - flannel
    #  - generic-veth
    #  - portmap
    chainingMode: ~

    # -- A CNI network name in to which the Cilium plugin should be added as a chained plugin.
    # This will cause the agent to watch for a CNI network with this network name. When it is
    # found, this will be used as the basis for Cilium's CNI configuration file. If this is
    # set, it assumes a chaining mode of generic-veth. As a special case, a chaining mode
    # of aws-cni implies a chainingTarget of aws-cni.
    chainingTarget: ~

    # -- Make Cilium take ownership over the `/etc/cni/net.d` directory on the
    # node, renaming all non-Cilium CNI configurations to `*.cilium_bak`.
    # This ensures no Pods can be scheduled using other CNI plugins during Cilium
    # agent downtime.
    exclusive: true

    # -- Configure the log file for CNI logging with retention policy of 7 days.
    # Disable CNI file logging by setting this field to empty explicitly.
    logFile: /var/run/cilium/cilium-cni.log

    # -- Skip writing of the CNI configuration. This can be used if
    # writing of the CNI configuration is performed by external automation.
    customConf: false

    # -- Configure the path to the CNI configuration directory on the host.
    confPath: /etc/cni/net.d

    # -- Configure the path to the CNI binary directory on the host.
    binPath: /opt/cni/bin

    # -- Specify the path to a CNI config to read from on agent start.
    # This can be useful if you want to manage your CNI
    # configuration outside of a Kubernetes environment. This parameter is
    # mutually exclusive with the 'cni.configMap' parameter. The agent will
    # write this to 05-cilium.conflist on startup.
    # readCniConf: /host/etc/cni/net.d/05-sample.conflist.input

    # -- When defined, configMap will mount the provided value as ConfigMap and
    # interpret the cniConf variable as CNI configuration file and write it
    # when the agent starts up
    # configMap: cni-configuration

    # -- Configure the key in the CNI ConfigMap to read the contents of
    # the CNI configuration from.
    configMapKey: cni-config

    # -- Configure the path to where to mount the ConfigMap inside the agent pod.
    confFileMountPath: /tmp/cni-configuration

    # -- Configure the path to where the CNI configuration directory is mounted
    # inside the agent pod.
    hostConfDirMountPath: /host/etc/cni/net.d

  # -- Tail call hooks for custom eBPF programs.
  #customCalls:
  #  # -- Enable tail call hooks for custom eBPF programs.
  #  enabled: true

  daemon:
    # -- Configure where Cilium runtime state should be stored.
    runPath: "/var/run/cilium"

    # -- Configure a custom list of possible configuration override sources
    # The default is "config-map:cilium-config,cilium-node-config". For supported
    # values, see the help text for the build-config subcommand.
    # Note that this value should be a comma-separated string.
    configSources: ~

    # -- allowedConfigOverrides is a list of config-map keys that can be overridden.
    # That is to say, if this value is set, config sources (excepting the first one) can
    # only override keys in this list.
    #
    # This takes precedence over blockedConfigOverrides.
    #
    # By default, all keys may be overridden. To disable overrides, set this to "none" or
    # change the configSources variable.
    allowedConfigOverrides: ~

    # -- blockedConfigOverrides is a list of config-map keys that may not be overridden.
    # In other words, if any of these keys appear in a configuration source excepting the
    # first one, they will be ignored
    #
    # This is ignored if allowedConfigOverrides is set.
    #
    # By default, all keys may be overridden.
    blockedConfigOverrides: ~

  # -- Enables experimental support for the detection of new and removed datapath
  # devices. When devices change the eBPF datapath is reloaded and services updated.
  # If "devices" is set then only those devices, or devices matching a wildcard will
  # be considered.
  enableRuntimeDeviceDetection: false

  # -- Chains to ignore when installing feeder rules.
  # disableIptablesFeederRules: ""

  # -- Limit egress masquerading to interface selector.
  #egressMasqueradeInterfaces: ""


  # -- Enable setting identity mark for local traffic.
  # enableIdentityMark: true

  # -- Enable Kubernetes EndpointSlice feature in Cilium if the cluster supports it.
  #enableK8sEndpointSlice: true

  # -- Enable CiliumEndpointSlice feature.
  #enableCiliumEndpointSlice: false


  envoyConfig:
    # -- Enable CiliumEnvoyConfig CRD
    # CiliumEnvoyConfig CRD can also be implicitly enabled by other options.
    enabled: true

    # -- SecretsNamespace is the namespace in which envoy SDS will retrieve secrets from.
    secretsNamespace:
      # -- Create secrets namespace for CiliumEnvoyConfig CRDs.
      create: true

      # -- The name of the secret namespace to which Cilium agents are given read access.
      name: cilium-secrets

  ingressController:
    # -- Enable cilium ingress controller
    # This will automatically set enable-envoy-config as well.
    enabled: false

    # -- Set cilium ingress controller to be the default ingress controller
    # This will let cilium ingress controller route entries without ingress class set
    default: false

    # -- Default ingress load balancer mode
    # Supported values: shared, dedicated
    # For granular control, use the following annotations on the ingress resource
    # ingress.cilium.io/loadbalancer-mode: shared|dedicated,
    loadbalancerMode: dedicated

    # -- Enforce https for host having matching TLS host in Ingress.
    # Incoming traffic to http listener will return 308 http error code with respective location in header.
    enforceHttps: true

    # -- Enable proxy protocol for all Ingress listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
    enableProxyProtocol: false

    # -- IngressLBAnnotations are the annotation and label prefixes, which are used to filter annotations and/or labels to propagate from Ingress to the Load Balancer service
    ingressLBAnnotationPrefixes: ['service.beta.kubernetes.io', 'service.kubernetes.io', 'cloud.google.com']

    # -- Default secret namespace for ingresses without .spec.tls[].secretName set.
    defaultSecretNamespace:

    # -- Default secret name for ingresses without .spec.tls[].secretName set.
    defaultSecretName:

    # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
    secretsNamespace:
      # -- Create secrets namespace for Ingress.
      create: true

      # -- Name of Ingress secret namespace.
      name: cilium-secrets

      # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
      # If disabled, TLS secrets must be maintained externally.
      sync: true

    # -- Load-balancer service in shared mode.
    # This is a single load-balancer service for all Ingress resources.
    service:
      # -- Service name
      name: cilium-ingress
      # -- Labels to be added for the shared LB service
      labels: {}
      # -- Annotations to be added for the shared LB service
      annotations: {}
      # -- Service type for the shared LB service
      type: LoadBalancer
      # -- Configure a specific nodePort for insecure HTTP traffic on the shared LB service
      insecureNodePort: ~
      # -- Configure a specific nodePort for secure HTTPS traffic on the shared LB service
      secureNodePort : ~
      # -- Configure a specific loadBalancerClass on the shared LB service (requires Kubernetes 1.24+)
      loadBalancerClass: ~
      # -- Configure a specific loadBalancerIP on the shared LB service
      loadBalancerIP : ~
      # -- Configure if node port allocation is required for LB service
      # ref: https://kubernetes.io/docs/concepts/services-networking/service/#load-balancer-nodeport-allocation
      allocateLoadBalancerNodePorts: ~

  gatewayAPI:
    # -- Enable support for Gateway API in cilium
    # This will automatically set enable-envoy-config as well.
    enabled: true

    # -- SecretsNamespace is the namespace in which envoy SDS will retrieve TLS secrets from.
    secretsNamespace:
      # -- Create secrets namespace for Gateway API.
      create: true

      # -- Name of Gateway API secret namespace.
      name: cilium-secrets

      # -- Enable secret sync, which will make sure all TLS secrets used by Ingress are synced to secretsNamespace.name.
      # If disabled, TLS secrets must be maintained externally.
      sync: true

  # -- Enables the fallback compatibility solution for when the xt_socket kernel
  # module is missing and it is needed for the datapath L7 redirection to work
  # properly. See documentation for details on when this can be disabled:
  # https://docs.cilium.io/en/stable/operations/system_requirements/#linux-kernel.
  #enableXTSocketFallback: false

  encryption:
    # -- Enable transparent network encryption.
    enabled: false

    # -- Encryption method. Can be either ipsec or wireguard.
    type: ipsec

    # -- Enable encryption for pure node to node traffic.
    # This option is only effective when encryption.type is set to ipsec.
    nodeEncryption: false

    ipsec:
      # -- Name of the key file inside the Kubernetes secret configured via secretName.
      keyFile: ""

      # -- Path to mount the secret inside the Cilium pod.
      mountPath: ""

      # -- Name of the Kubernetes secret containing the encryption keys.
      secretName: ""

      # -- The interface to use for encrypted traffic.
      interface: ""

    wireguard:
      # -- Enables the fallback to the user-space implementation.
      userspaceFallback: false

    # -- Deprecated in favor of encryption.ipsec.keyFile.
    # Name of the key file inside the Kubernetes secret configured via secretName.
    # This option is only effective when encryption.type is set to ipsec.
    keyFile: keys

    # -- Deprecated in favor of encryption.ipsec.mountPath.
    # Path to mount the secret inside the Cilium pod.
    # This option is only effective when encryption.type is set to ipsec.
    mountPath: /etc/ipsec

    # -- Deprecated in favor of encryption.ipsec.secretName.
    # Name of the Kubernetes secret containing the encryption keys.
    # This option is only effective when encryption.type is set to ipsec.
    secretName: cilium-ipsec-keys

    # -- Deprecated in favor of encryption.ipsec.interface.
    # The interface to use for encrypted traffic.
    # This option is only effective when encryption.type is set to ipsec.
    interface: ""

  externalIPs:
    # -- Enable ExternalIPs service support.
    enabled: true

  # fragmentTracking enables IPv4 fragment tracking support in the datapath.
  fragmentTracking: true

  # -- Configure the host firewall.
  #hostFirewall:
    # -- Enables the enforcement of host policies in the eBPF datapath.
  #  enabled: false

  hostPort:
    # -- Enable hostPort service support.
    enabled: true

  # -- Configure socket LB
  #socketLB:
  #  # -- Enable socket LB
  #  enabled: false

    # -- Disable socket lb for non-root ns. This is used to enable Istio routing rules.
  #  hostNamespaceOnly: false

  # -- Configure certificate generation for Hubble integration.
  # If hubble.tls.auto.method=cronJob, these values are used
  # for the Kubernetes CronJob which will be scheduled regularly to
  # (re)generate any certificates not provided manually.
  certgen:
    # -- Seconds after which the completed job pod will be deleted
    ttlSecondsAfterFinished: 1800
    # -- Labels to be added to hubble-certgen pods
    podLabels: {}
    # -- Node tolerations for pod assignment on nodes with taints
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations: []

  hubble:
    # -- Enable Hubble (true by default).
    enabled: false


  # -- Method to use for identity allocation (`crd` or `kvstore`).
  identityAllocationMode: "crd"

  # -- Time to wait before using new identity on endpoint identity change.
  # identityChangeGracePeriod: "5s"

  # -- GC interval for security identities.
  # identityGCInterval: "15m0s"

  # -- Timeout after which identity expires on lack of heartbeat.
  # identityHeartbeatTimeout: "30m0s"

  # -- Install Iptables rules to skip netfilter connection tracking on all pod
  # traffic. This option is only effective when Cilium is running in direct
  # routing and full KPR mode. Moreover, this option cannot be enabled when Cilium
  # is running in a managed Kubernetes environment or in a chained CNI setup.
  #installNoConntrackIptablesRules: true

  ipam:
    # -- Configure IP Address Management mode.
    # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
    mode: kubernetes
    # -- Maximum rate at which the CiliumNode custom resource is updated.
    ciliumNodeUpdateRate: "15s"
    operator:
      # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
      clusterPoolIPv4PodCIDRList: ["10.0.0.0/8"]
      # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
      clusterPoolIPv4MaskSize: 24
      # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
      clusterPoolIPv6PodCIDRList: ["fd00::/104"]
      # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
      clusterPoolIPv6MaskSize: 120
      # -- IP pools to auto-create in multi-pool IPAM mode.
      autoCreateCiliumPodIPPools: {}
        # default:
        #   ipv4:
        #     cidrs:
        #       - 10.10.0.0/8
        #     maskSize: 24
        # other:
        #   ipv6:
        #     cidrs:
        #       - fd00:100::/80
        #     maskSize: 96
      # -- The maximum burst size when rate limiting access to external APIs.
      # Also known as the token bucket capacity.
      # @default -- `20`
      externalAPILimitBurstSize: ~
      # -- The maximum queries per second when rate limiting access to
      # external APIs. Also known as the bucket refill rate, which is used to
      # refill the bucket up to the burst size capacity.
      # @default -- `4.0`
      externalAPILimitQPS: ~

  # -- Configure the eBPF-based ip-masq-agent
  ipMasqAgent:
    enabled: false

    # the config of nonMasqueradeCIDRs
    # config:
      # nonMasqueradeCIDRs: []
      # masqLinkLocal: false

  # iptablesLockTimeout defines the iptables "--wait" option when invoked from Cilium.
  # iptablesLockTimeout: "5s"

  ipv4:
    # -- Enable IPv4 support.
    enabled: true

  ipv6:
    # -- Enable IPv6 support.
    enabled: false

  # -- Configure Kubernetes specific configuration
  k8s: {}
    # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
    # range via the Kubernetes node resource
    # requireIPv4PodCIDR: false

    # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
    # range via the Kubernetes node resource
    # requireIPv6PodCIDR: false

  # Valid options are "true", "false", "disabled" (deprecated), "partial" (deprecated), "strict" (deprecated).
  # ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
  kubeProxyReplacement: true

  # -- healthz server bind address for the kube-proxy replacement.
  # To enable set the value to '0.0.0.0:10256' for all ipv4
  # addresses and this '[::]:10256' for all ipv6 addresses.
  # By default it is disabled.
  kubeProxyReplacementHealthzBindAddr: ""

  l2NeighDiscovery:
    # -- Enable L2 neighbor discovery in the agent
    enabled: false

    # -- Override the agent's default neighbor resolution refresh period.
    refreshPeriod: '30s'

  # -- Enable Layer 7 network policy.
  #l7Proxy: true


  # To include or exclude matched resources from cilium identity evaluation
  # labels: ""

  # logOptions allows you to define logging options. eg:
  logOptions:
    format: json

  # -- Enables periodic logging of system load
  logSystemLoad: false


  # -- Configure maglev consistent hashing
  maglev: {}
    # -- tableSize is the size (parameter M) for the backend table of one
    # service entry
    # tableSize:

    # -- hashSeed is the cluster-wide base64 encoded seed for the hashing
    # hashSeed:

  # -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
  enableIPv4Masquerade: true

  #enableIPv6BIGTCP: true

  # -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
  enableIPv6Masquerade: false

  # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
  # cluster.
  egressGateway:
    enabled: false

  vtep:
  # -- Enables VXLAN Tunnel Endpoint (VTEP) Integration (beta) to allow
  # Cilium-managed pods to talk to third party VTEP devices over Cilium tunnel.
    enabled: false

  # -- A space separated list of VTEP device endpoint IPs, for example "1.1.1.1  1.1.2.1"
    endpoint: ""
  # -- A space separated list of VTEP device CIDRs, for example "1.1.1.0/24 1.1.2.0/24"
    cidr: ""
  # -- VTEP CIDRs Mask that applies to all VTEP CIDRs, for example "255.255.255.0"
    mask: ""
  # -- A space separated list of VTEP device MAC addresses (VTEP MAC), for example "x:x:x:x:x:x  y:y:y:y:y:y:y"
    mac: ""

  # -- Allows to explicitly specify the IPv4 CIDR for native routing.
  # When specified, Cilium assumes networking for this CIDR is preconfigured and
  # hands traffic destined for that range to the Linux network stack without
  # applying any SNAT.
  # Generally speaking, specifying a native routing CIDR implies that Cilium can
  # depend on the underlying networking stack to route packets to their
  # destination. To offer a concrete example, if Cilium is configured to use
  # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
  # the user must configure the routes to reach pods, either manually or by
  # setting the auto-direct-node-routes flag.
  ipv4NativeRoutingCIDR: 0.0.0.0/0

  # -- Allows to explicitly specify the IPv6 CIDR for native routing.
  # When specified, Cilium assumes networking for this CIDR is preconfigured and
  # hands traffic destined for that range to the Linux network stack without
  # applying any SNAT.
  # Generally speaking, specifying a native routing CIDR implies that Cilium can
  # depend on the underlying networking stack to route packets to their
  # destination. To offer a concrete example, if Cilium is configured to use
  # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
  # the user must configure the routes to reach pods, either manually or by
  # setting the auto-direct-node-routes flag.
  # ipv6NativeRoutingCIDR:

  # -- cilium-monitor sidecar.
  monitor:
    # -- Enable the cilium-monitor sidecar.
    enabled: true

  # -- Configure service load balancing
  loadBalancer:
    # -- standalone enables the standalone L4LB which does not connect to
    # kube-apiserver.
    # standalone: false

    # -- algorithm is the name of the load balancing algorithm for backend
    # selection e.g. random or maglev
    # algorithm: random

    # -- mode is the operation mode of load balancing for remote backends
    # e.g. snat, dsr, hybrid
    mode: dsr

    # -- acceleration is the option to accelerate service handling via XDP
    # Applicable values can be: disabled (do not use XDP), native (XDP BPF
    # program is run directly out of the networking driver's early receive
    # path), or best-effort (use native mode XDP acceleration on devices
    # that support it).
    acceleration: disabled

    # -- dsrDispatch configures whether IP option or IPIP encapsulation is
    # used to pass a service IP and port to remote backend
    # dsrDispatch: opt

    # -- serviceTopology enables K8s Topology Aware Hints -based service
    # endpoints filtering
    # serviceTopology: false

    # -- L7 LoadBalancer
    l7:
      # -- Enable L7 service load balancing via envoy proxy.
      # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
      # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
      # Please refer to docs for supported annotations for more configuration.
      #
      # Applicable values:
      #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
      #   - disabled: Disable L7 load balancing by way of service annotation.
      backend: envoy
      # -- List of ports from service to be automatically redirected to above backend.
      # Any service exposing one of these ports will be automatically redirected.
      # Fine-grained control can be achieved by using the service annotation.
      ports: []
      # -- Default LB algorithm
      # The default LB algorithm to be used for services, which can be overridden by the
      # service annotation (e.g. service.cilium.io/lb-l7-algorithm)
      # Applicable values: round_robin, least_request, random
      algorithm: round_robin

  # -- Configure N-S k8s service loadbalancing
  nodePort:
    # -- Enable the Cilium NodePort service implementation.
    enabled: true

    # -- Port range to use for NodePort services.
    # range: "30000,32767"

    # -- Set to true to prevent applications binding to service ports.
    bindProtection: true

    # -- Append NodePort range to ip_local_reserved_ports if clash with ephemeral
    # ports is detected.
    autoProtectPortRange: true

  # policyAuditMode: false

  # -- The agent can be put into one of the three policy enforcement modes:
  # default, always and never.
  # ref: https://docs.cilium.io/en/stable/policy/intro/#policy-enforcement-modes
  policyEnforcementMode: 'never'

  pprof:
    # -- Enable Go pprof debugging
    enabled: false

  # -- Configure prometheus metrics on the configured port at /metrics
  prometheus:
    enabled: true
    port: 9962
    serviceMonitor:
      # -- Enable service monitors.
      # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
      enabled: true
      # -- Labels to add to ServiceMonitor cilium-agent
      labels:
        resolvemy.host/metrics: mimir
      # -- Annotations to add to ServiceMonitor cilium-agent
      annotations: {}
      # -- Specify the Kubernetes namespace where Prometheus expects to find
      # service monitors configured.
      namespace: core-prod
    # -- Metrics that should be enabled or disabled from the default metric
    # list. (+metric_foo to enable metric_foo , -metric_bar to disable
    # metric_bar).
    # ref: https://docs.cilium.io/en/stable/operations/metrics/#exported-metrics
    metrics: ~



  # -- Enable use of the remote node identity.
  # ref: https://docs.cilium.io/en/v1.7/install/upgrade/#configmap-remote-node-identity
  remoteNodeIdentity: true


  # Configure Cilium Envoy options.
  envoy:
    # -- Enable Envoy Proxy in standalone DaemonSet.
    enabled: true

    log:
      # -- The format string to use for laying out the log message metadata of Envoy.
      format: "[%Y-%m-%d %T.%e][%t][%l][%n] [%g:%#] %v"
      # -- Path to a separate Envoy log file, if any. Defaults to /dev/stdout.
      path: ""

    # -- Time in seconds after which a TCP connection attempt times out
    connectTimeoutSeconds: 2
    # -- ProxyMaxRequestsPerConnection specifies the max_requests_per_connection setting for Envoy
    maxRequestsPerConnection: 0
    # -- Set Envoy HTTP option max_connection_duration seconds. Default 0 (disable)
    maxConnectionDurationSeconds: 0
    # -- Set Envoy upstream HTTP idle connection timeout seconds.
    # Does not apply to connections with pending requests. Default 60s
    idleTimeoutDurationSeconds: 60

    # -- Envoy container image.
    image:
      override: ~
      repository: "quay.io/cilium/cilium-envoy"
      tag: "v1.27.2-f19708f3d0188fe39b7e024b4525b75a9eeee61f"
      pullPolicy: "IfNotPresent"
      digest: "sha256:80de27c1d16ab92923cc0cd1fff90f2e7047a9abf3906fda712268d9cbc5b950"
      useDigest: true

    # -- Additional containers added to the cilium Envoy DaemonSet.
    extraContainers: []

    # -- Additional envoy container arguments.
    extraArgs: []

    # -- Additional envoy container environment variables.
    extraEnv: []

    # -- Additional envoy hostPath mounts.
    extraHostPathMounts: []
      # - name: host-mnt-data
      #   mountPath: /host/mnt/data
      #   hostPath: /mnt/data
      #   hostPathType: Directory
      #   readOnly: true
      #   mountPropagation: HostToContainer

    # -- Additional envoy volumes.
    extraVolumes: []

    # -- Additional envoy volumeMounts.
    extraVolumeMounts: []

    # -- Configure termination grace period for cilium-envoy DaemonSet.
    terminationGracePeriodSeconds: 1

    # -- TCP port for the health API.
    healthPort: 9878

    # -- cilium-envoy update strategy
    # ref: https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/#updating-a-daemonset
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 2
    # -- Roll out cilium envoy pods automatically when configmap is updated.
    rollOutPods: false

    # -- Annotations to be added to all top-level cilium-envoy objects (resources under templates/cilium-envoy)
    annotations: {}

    # -- Security Context for cilium-envoy pods.
    podSecurityContext: {}

    # -- Annotations to be added to envoy pods
    podAnnotations: {}

    # -- Labels to be added to envoy pods
    podLabels:
      logs: loki-myloginspace

    # -- Envoy resource limits & requests
    # ref: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/
    resources: {}
      # limits:
      #   cpu: 4000m
      #   memory: 4Gi
      # requests:
      #   cpu: 100m
      #   memory: 512Mi

    startupProbe:
      # -- failure threshold of startup probe.
      # 105 x 2s translates to the old behaviour of the readiness probe (120s delay + 30 x 3s)
      failureThreshold: 105
      # -- interval between checks of the startup probe
      periodSeconds: 2
    livenessProbe:
      # -- failure threshold of liveness probe
      failureThreshold: 10
      # -- interval between checks of the liveness probe
      periodSeconds: 30
    readinessProbe:
      # -- failure threshold of readiness probe
      failureThreshold: 3
      # -- interval between checks of the readiness probe
      periodSeconds: 30

    securityContext:
      # -- User to run the pod with
      # runAsUser: 0
      # -- Run the pod with elevated privileges
      privileged: true
      capabilities:
        # -- Capabilities for the `cilium-envoy` container
        envoy:
          # Used since cilium proxy uses setting IPPROTO_IP/IP_TRANSPARENT
          - NET_ADMIN
          # We need it for now but might not need it for >= 5.11 specially
          # for the 'SYS_RESOURCE'.
          # In >= 5.8 there's already BPF and PERMON capabilities
          - SYS_ADMIN
          # Both PERFMON and BPF requires kernel 5.8, container runtime
          # cri-o >= v1.22.0 or containerd >= v1.5.0.
          # If available, SYS_ADMIN can be removed.
          #- PERFMON
          #- BPF

    # -- Affinity for cilium-envoy.
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: liqo.io/type
                  operator: DoesNotExist

      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium-envoy

    # -- Node selector for cilium-envoy.
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for envoy scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/
    tolerations:
    - operator: Exists
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

    # -- The priority class to use for cilium-envoy.
    priorityClassName: ~

    # -- DNS policy for Cilium envoy pods.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: ~

    prometheus:
      # -- Enable prometheus metrics for cilium-envoy
      enabled: true
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: false
        # -- Labels to add to ServiceMonitor cilium-envoy
        labels: {}
        # -- Annotations to add to ServiceMonitor cilium-envoy
        annotations: {}
        # -- Interval for scrape metrics.
        interval: "10s"
        # -- Specify the Kubernetes namespace where Prometheus expects to find
        # service monitors configured.
        # namespace: ""
        # -- Relabeling configs for the ServiceMonitor cilium-envoy
        relabelings:
          - sourceLabels:
              - __meta_kubernetes_pod_node_name
            targetLabel: node
            replacement: ${1}
        # -- Metrics relabeling configs for the ServiceMonitor cilium-envoy
        metricRelabelings: ~
      # -- Serve prometheus metrics for cilium-envoy on the configured port
      port: "9964"

  # -- Enable resource quotas for priority classes used in the cluster.
  resourceQuotas:
    enabled: false
    cilium:
      hard:
        # 5k nodes * 2 DaemonSets (Cilium and cilium node init)
        pods: "10k"
    operator:
      hard:
        # 15 "clusterwide" Cilium Operator pods for HA
        pods: "15"

  # Need to document default
  ##################
  #sessionAffinity: false

  # -- Do not run Cilium agent when running with clean mode. Useful to completely
  # uninstall Cilium as it will stop Cilium from starting and create artifacts
  # in the node.
  sleepAfterInit: false

  # -- Configure BPF socket operations configuration
  sockops:
    # enabled enables installation of socket options acceleration.
    enabled: false

  # -- Enable check of service source ranges (currently, only for LoadBalancer).
  svcSourceRangeCheck: true

  # -- Synchronize Kubernetes nodes to kvstore and perform CNP GC.
  synchronizeK8sNodes: true

  # -- Configure TLS configuration in the agent.
  tls:
    # -- This configures how the Cilium agent loads the secrets used TLS-aware CiliumNetworkPolicies
    # (namely the secrets referenced by terminatingTLS and originatingTLS).
    # Possible values:
    #   - local
    #   - k8s
    secretsBackend: local

    # -- Base64 encoded PEM values for the CA certificate and private key.
    # This can be used as common CA to generate certificates used by hubble and clustermesh components
    ca:
      # -- Optional CA cert. If it is provided, it will be used by cilium to
      # generate all other certificates. Otherwise, an ephemeral CA is generated.
      cert: ""

      # -- Optional CA private key. If it is provided, it will be used by cilium to
      # generate all other certificates. Otherwise, an ephemeral CA is generated.
      key: ""

      # -- Generated certificates validity duration in days. This will be used for auto generated CA.
      certValidityDuration: 1095

  # -- Tunneling protocol to use in tunneling mode and for ad-hoc tunnels.
  # Possible values:
  #   - ""
  #   - vxlan
  #   - geneve
  # @default -- `"vxlan"`
  tunnelProtocol: ''

  # -- Enable native-routing mode or tunneling mode.
  # Possible values:
  #   - ""
  #   - native
  #   - tunnel
  # @default -- `"tunnel"`
  routingMode: native

  # -- Configure VXLAN and Geneve tunnel port.
  # @default -- Port 8472 for VXLAN, Port 6081 for Geneve
  tunnelPort: 0

  # -- Configure what the response should be to traffic for a service without backends.
  # "reject" only works on kernels >= 5.10, on lower kernels we fallback to "drop".
  # Possible values:
  #  - reject (default)
  #  - drop
  serviceNoBackendResponse: reject

  # -- Configure the underlying network MTU to overwrite auto-detected MTU.
  MTU: 9000

  # -- Disable the usage of CiliumEndpoint CRD.
  disableEndpointCRD: false

  wellKnownIdentities:
    # -- Enable the use of well-known identities.
    enabled: false

  etcd:
    # -- Enable etcd mode for the agent.
    enabled: false

  operator:
    # -- Enable the cilium-operator component (required).
    enabled: true

    # -- Roll out cilium-operator pods automatically when configmap is updated.
    rollOutPods: false

    # -- Number of replicas to run for the cilium-operator deployment
    replicas: 1

    # -- The priority class to use for cilium-operator
    priorityClassName: ""

    # -- DNS policy for Cilium operator pods.
    # Ref: https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: ""

    # -- cilium-operator update strategy
    updateStrategy:
      type: RollingUpdate
      rollingUpdate:
        maxSurge: 1
        maxUnavailable: 1

    # -- Affinity for cilium-operator
    affinity:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
                - key: liqo.io/type
                  operator: DoesNotExist

      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - topologyKey: kubernetes.io/hostname
            labelSelector:
              matchLabels:
                io.cilium/app: operator

    # -- Node labels for cilium-operator pod assignment
    # ref: https://kubernetes.io/docs/user-guide/node-selection/
    #
    nodeSelector:
      kubernetes.io/os: linux
      #liqo.io/type: virtual-node

    # -- Node tolerations for cilium-operator scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations:
    - operator: Exists
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

    # -- Additional cilium-operator container arguments.
    extraArgs: []

    # -- Additional cilium-operator environment variables.
    extraEnv: []

    # -- Additional cilium-operator hostPath mounts.
    extraHostPathMounts: []
      # - name: host-mnt-data
      #   mountPath: /host/mnt/data
      #   hostPath: /mnt/data
      #   hostPathType: Directory
      #   readOnly: true
      #   mountPropagation: HostToContainer

    # -- Additional cilium-operator volumes.
    extraVolumes: []

    # -- Additional cilium-operator volumeMounts.
    extraVolumeMounts: []

    # -- Annotations to be added to cilium-operator pods
    podAnnotations: {}

    # -- Labels to be added to cilium-operator pods
    podLabels: {}

    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- cilium-operator resource limits & requests
    # ref: https://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}
      # limits:
      #   cpu: 1000m
      #   memory: 1Gi
      # requests:
      #   cpu: 100m
      #   memory: 128Mi

    # -- Security context to be added to cilium-operator pods
    securityContext: {}
      # runAsUser: 0

    # -- Interval for endpoint garbage collection.
    endpointGCInterval: "5m0s"

    # -- Interval for cilium node garbage collection.
    nodeGCInterval: "5m0s"

    # -- Interval for identity garbage collection.
    identityGCInterval: "15m0s"

    # -- Timeout for identity heartbeats.
    identityHeartbeatTimeout: "30m0s"

    # -- Enable prometheus metrics for cilium-operator on the configured port at
    # /metrics
    prometheus:
      enabled: true
      port: 9963
      serviceMonitor:
        # -- Enable service monitors.
        # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/master/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
        enabled: true
        # -- Labels to add to ServiceMonitor cilium-operator
        labels:
          resolvemy.host/metrics: mimir
        # -- Annotations to add to ServiceMonitor cilium-operator
        annotations: {}

    # -- Skip CRDs creation for cilium-operator
    skipCRDCreation: false

    # -- Remove Cilium node taint from Kubernetes nodes that have a healthy Cilium
    # pod running.
    removeNodeTaints: true

    # -- Set Node condition NetworkUnavailable to 'false' with the reason
    # 'CiliumIsUp' for nodes that have a healthy Cilium pod.
    setNodeNetworkStatus: true

    unmanagedPodWatcher:
      # -- Restart any pod that are not managed by Cilium.
      restart: true
      # -- Interval, in seconds, to check if there are any pods that are not
      # managed by Cilium.
      intervalSeconds: 15

  nodeinit:
    # -- Enable the node initialization DaemonSet
    enabled: false

    # -- The priority class to use for the nodeinit pod.
    priorityClassName: ""

    # -- node-init update strategy
    updateStrategy:
      type: RollingUpdate

    # -- Additional nodeinit environment variables.
    extraEnv: []

    # -- Affinity for cilium-nodeinit
    affinity: {}

    # -- Node labels for nodeinit pod assignment
    # ref: https://kubernetes.io/docs/user-guide/node-selection/
    #
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for nodeinit scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations:
    - operator: Exists
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

    # -- Annotations to be added to node-init pods.
    podAnnotations: {}

    # -- Labels to be added to node-init pods.
    podLabels: {}

    # -- nodeinit resource limits & requests
    # ref: https://kubernetes.io/docs/user-guide/compute-resources/
    resources:
      requests:
        cpu: 100m
        memory: 100Mi

    # -- Security context to be added to nodeinit pods.
    securityContext:
      privileged: true
      seLinuxOptions:
        level: 's0'
        # Running with spc_t since we have removed the privileged mode.
        # Users can change it to a different type as long as they have the
        # type available on the system.
        type: 'spc_t'
      capabilities:
        add:
          # Used in iptables. Consider removing once we are iptables-free
          - SYS_MODULE
          # Used for nsenter
          - NET_ADMIN
          - SYS_ADMIN
          - SYS_CHROOT
          - SYS_PTRACE

    # -- bootstrapFile is the location of the file where the bootstrap timestamp is
    # written by the node-init DaemonSet
    bootstrapFile: "/tmp/cilium-bootstrap.d/cilium-bootstrap-time"

  preflight:
    # -- Enable Cilium pre-flight resources (required for upgrade)
    enabled: false

    # -- The priority class to use for the preflight pod.
    priorityClassName: ""

    # -- preflight update strategy
    updateStrategy:
      type: RollingUpdate

    # -- Additional preflight environment variables.
    extraEnv: []

    # -- Affinity for cilium-preflight
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - topologyKey: kubernetes.io/hostname
          labelSelector:
            matchLabels:
              k8s-app: cilium

    # -- Node labels for preflight pod assignment
    # ref: https://kubernetes.io/docs/user-guide/node-selection/
    #
    nodeSelector:
      kubernetes.io/os: linux

    # -- Node tolerations for preflight scheduling to nodes with taints
    # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
    tolerations:
    - key: node.kubernetes.io/not-ready
      effect: NoSchedule
    - key: node-role.kubernetes.io/master
      effect: NoSchedule
    - key: node.cloudprovider.kubernetes.io/uninitialized
      effect: NoSchedule
      value: "true"
    - key: CriticalAddonsOnly
      operator: "Exists"
      # - key: "key"
      #   operator: "Equal|Exists"
      #   value: "value"
      #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

    # -- Annotations to be added to preflight pods
    podAnnotations: {}

    # -- Labels to be added to the preflight pod.
    podLabels: {}

    # PodDisruptionBudget settings
    podDisruptionBudget:
      # -- enable PodDisruptionBudget
      # ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
      enabled: false
      # -- Minimum number/percentage of pods that should remain scheduled.
      # When it's set, maxUnavailable must be disabled by `maxUnavailable: null`
      minAvailable: null
      # -- Maximum number/percentage of pods that may be made unavailable
      maxUnavailable: 1

    # -- preflight resource limits & requests
    # ref: https://kubernetes.io/docs/user-guide/compute-resources/
    resources: {}
      # limits:
      #   cpu: 4000m
      #   memory: 4Gi
      # requests:
      #   cpu: 100m
      #   memory: 512Mi

    # -- Security context to be added to preflight pods
    securityContext: {}
      # runAsUser: 0

    # -- Path to write the `--tofqdns-pre-cache` file to.
    tofqdnsPreCache: ""

    # -- Configure termination grace period for preflight Deployment and DaemonSet.
    terminationGracePeriodSeconds: 1

    # -- By default we should always validate the installed CNPs before upgrading
    # Cilium. This will make sure the user will have the policies deployed in the
    # cluster with the right schema.
    validateCNPs: true

  # -- Explicitly enable or disable priority class.
  # .Capabilities.KubeVersion is unsettable in `helm template` calls,
  # it depends on k8s libraries version that Helm was compiled against.
  # This option allows to explicitly disable setting the priority class, which
  # is useful for rendering charts for gke clusters in advance.
  enableCriticalPriorityClass: true

  # disableEnvoyVersionCheck removes the check for Envoy, which can be useful
  # on AArch64 as the images do not currently ship a version of Envoy.
  #disableEnvoyVersionCheck: false

  clustermesh:
    # -- Deploy clustermesh-apiserver for clustermesh
    useAPIServer: false

  # -- Configure external workloads support
  externalWorkloads:
    # -- Enable support for external workloads, such as VMs (false by default).
    enabled: false

  # -- Configure cgroup related configuration
  cgroup:
    autoMount:
      # -- Enable auto mount of cgroup2 filesystem.
      # When `autoMount` is enabled, cgroup2 filesystem is mounted at
      # `cgroup.hostRoot` path on the underlying host and inside the cilium agent pod.
      # If users disable `autoMount`, it's expected that users have mounted
      # cgroup2 filesystem at the specified `cgroup.hostRoot` volume, and then the
      # volume will be mounted inside the cilium agent pod at the same path.
      enabled: true
    # -- Configure cgroup root where cgroup2 filesystem is mounted on the host (see also: `cgroup.autoMount`)
    hostRoot: /run/cilium/cgroupv2

  # -- Configure whether to enable auto detect of terminating state for endpoints
  # in order to support graceful termination.
  enableK8sTerminatingEndpoint: true

  # -- Configure whether to unload DNS policy rules on graceful shutdown
  # dnsPolicyUnloadOnShutdown: false

  # -- Configure the key of the taint indicating that Cilium is not ready on the node.
  # When set to a value starting with `ignore-taint.cluster-autoscaler.kubernetes.io/`, the Cluster Autoscaler will ignore the taint on its decisions, allowing the cluster to scale up.
  agentNotReadyTaintKey: "node.cilium.io/agent-not-ready"

  dnsProxy:
    # -- DNS response code for rejecting DNS requests, available options are '[nameError refused]'.
    dnsRejectResponseCode: refused
    # -- Allow the DNS proxy to compress responses to endpoints that are larger than 512 Bytes or the EDNS0 option, if present.
    enableDnsCompression: true
    # -- Maximum number of IPs to maintain per FQDN name for each endpoint.
    endpointMaxIpPerHostname: 50
    # -- Time during which idle but previously active connections with expired DNS lookups are still considered alive.
    idleConnectionGracePeriod: 0s
    # -- Maximum number of IPs to retain for expired DNS lookups with still-active connections.
    maxDeferredConnectionDeletes: 10000
    # -- The minimum time, in seconds, to use DNS data for toFQDNs policies.
    minTtl: 3600
    # -- DNS cache data at this path is preloaded on agent startup.
    preCache: ""
    # -- Global port on which the in-agent DNS proxy should listen. Default 0 is a OS-assigned port.
    proxyPort: 0
    # -- The maximum time the DNS proxy holds an allowed DNS response before sending it along. Responses are sent as soon as the datapath is updated with the new IP information.
    proxyResponseMaxDelay: 100ms


konnectivity:
  enabled: false

kube-router:
  # Default values for kube-router.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.
  enabled: true

  image:
    # image.repository -- Kube-Router image
    repository: docker.io/cloudnativelabs/kube-router
    # image.pullPolicy -- Kube-Router image pull policy
    pullPolicy: IfNotPresent

  # imagePullSecrets -- Docker-registry secret names as an array
  imagePullSecrets: []

  # nameOverride -- String to partially override kube-router.fullname template with a string (will prepend the release name)
  nameOverride: ""

  # fullnameOverride --  String to fully override mosquitto.fullname template with a string
  fullnameOverride: 'kube-router'

  # livenessProbe -- Liveness probe for the kube-router workload
  livenessProbe:
    httpGet:
      path: /healthz
      port: 20244
    initialDelaySeconds: 10
    periodSeconds: 3

  # readinessProbe -- Readiness probe for the kube-router workload
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 3

  # updateStrategy -- Update strategy to use when upgrading workload
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1

  kubeRouter:
    # kubeRouter.apiServerUrl -- URL of the API server. If you use Kube-Router as service-proxy, use a reliable way to contact your masters
    apiServerUrl: ''

    # kubeRouter.enablePprof -- Enables pprof for debugging performance and memory leak issues
    enablePprof: false

    # kubeRouter.cacheSyncTimeout -- The timeout for cache synchronization (e.g. '5s', '1m'). Must be greater than 0
    cacheSyncTimeout: 15s

    # kubeRouter.healthPort -- Health check port, 0 = Disabled
    healthPort:

    # kubeRouter.extraArgs -- Extra arguments to pass to kube-router
    extraArgs: []

    cni:
      # cni.install -- Install the CNI plugins tools
      install: false

      # cni.version -- Version of the CNI plugins tools to install
      version: v0.7.5

      # cni.installPath -- Path to install the CNI plugins tools
      installPath: /opt/cni/bin

      # cni.downloadUrl -- CNI plugins tools download URL
      downloadUrl: https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-amd64-${CNI_VERSION}.tgz

      # cni.config -- Set CNI configuration
      config: |
        {
          "cniVersion":"0.3.0",
          "name":"mynet",
          "plugins":[
              {
                "name":"kubernetes",
                "type":"bridge",
                "bridge":"kube-bridge",
                "isDefaultGateway":true,
                "hairpinMode":true,
                "ipam":{
                    "type":"host-local"
                }
              },
              {
                "type":"portmap",
                "capabilities":{
                    "snat":true,
                    "portMappings":true
                }
              }
          ]
        }

    metrics:
      # kubeRouter.metrics.path -- Prometheus metrics path
      path:
      # kubeRouter.metrics.port -- Prometheus metrics port (set 0 to disable)
      port:

    router:
      # kubeRouter.router.enabled -- Enables Pod Networking, Advertises and learns the routes to Pods via iBGP
      enabled: true

      # kubeRouter.router.bgpRouterId -- BGP router-id. Must be specified in a ipv6 only cluster
      bgpRouterId:

      # kubeRouter.router.routesSyncPeriod -- The delay between route updates and advertisements (e.g. '5s', '1m', '2h22m'). Must be greater than 0
      routesSyncPeriod: '30s'

      # kubeRouter.router.peerRouterMultihopTtl -- Enable eBGP multihop supports (Relevant only if ttl >= 2)
      #peerRouterMultihopTtl: 

      # kubeRouter.router.overrideNexthop -- Override the next-hop in bgp routes sent to peers with the local ip
      overrideNexthop: true

      # kubeRouter.router.overlayType -- Topology of overlay network. Possible values: subnet or full.
      overlayType:

      # kubeRouter.router.nodesFullMesh -- Each node in the cluster will setup BGP peering with rest of the nodes (true or false)
      nodesFullMesh: false

      # kubeRouter.router.enablePodEgress -- SNAT traffic from Pods to destinations outside the cluster (true or false)
      enablePodEgress: false

      # kubeRouter.router.enableOverlay -- Enable IP-in-IP tunneling for pod-to-pod networking across nodes in different subnets (true or false)
      enableOverlay: false

      # kubeRouter.router.enableIbgp -- Enables peering with nodes with the same ASN, if disabled will only peer with external BGP peers (true or false)
      enableIbgp: false

      # kubeRouter.router.enableCni -- Enable CNI plugin. Disable if you want to use kube-router features alongside another CNI plugin (true or false)
      enableCni: false

      # kubeRouter.router.disableSourceDestCheck -- Disable the source-dest-check attribute for AWS EC2 instances. When this option is false, it must be set some other way (true or false)
      disableSourceDestCheck:

      # kubeRouter.router.clusterAsn -- ASN number under which cluster nodes will run iBGP
      clusterAsn: 64512

      # kubeRouter.router.bgpPort -- The port open for incoming BGP connections and to use for connecting with other BGP peers
      bgpPort: 179

      # kubeRouter.router.bgpGracefulRestartDeferralTime -- BGP Graceful restart deferral time according to RFC4724 4.1, maximum 18h
      bgpGracefulRestartDeferralTime: '1m'

      # kubeRouter.router.bgpGracefulRestart -- Enables the BGP Graceful Restart capability so that routes are preserved on unexpected restarts
      bgpGracefulRestart: true

      # kubeRouter.router.advertisePodCidr -- Add Node's POD cidr to the RIB so that it gets advertised to the BGP peers (true or false)
      advertisePodCidr: false

      # kubeRouter.router.advertiseLoadbalancerIp -- Add LoadbBalancer IP of service status as set by the LB provider to the RIB so that it gets advertised to the BGP peers (true or false)
      advertiseLoadbalancerIp: true

      # kubeRouter.router.advertiseExternalIp -- Add External IP of service to the RIB so that it gets advertised to the BGP peers (true or false)
      advertiseExternalIp: true

      # kubeRouter.router.advertiseClusterIp -- Add Cluster IP of the service to the RIB so that it gets advertises to the BGP peers (true or false)
      advertiseClusterIp: true

    firewall:
      # kubeRouter.firewall.enabled -- Enables Network Policy, sets up iptables to provide ingress firewall for pods
      enabled: false

      # kubeRouter.firewall.iptablesSyncPeriod -- The delay between iptables rule synchronizations (e.g. '5s', '1m'). Must be greater than 0
      iptablesSyncPeriod:

    serviceProxy:
      # kubeRouter.serviceProxy.enabled -- Enables Service Proxy, sets up IPVS for Kubernetes Services
      enabled: false

      # kubeRouter.serviceProxy.nodeportBindonAllIp -- For service of NodePort type create IPVS service that listens on all IP's of the node (true or false)
      nodeportBindonAllIp: false

      # kubeRouter.serviceProxy.masqueradeAll -- SNAT all traffic to cluster IP/node port (true or false)
      masqueradeAll: false

      # kubeRouter.serviceProxy.ipvsSyncPeriod -- The delay between ipvs config synchronizations (e.g. '5s', '1m', '2h22m'). Must be greater than 0
      ipvsSyncPeriod:

      # kubeRouter.serviceProxy.ipvsPermitAll -- Enables rule to accept all incoming traffic to service VIP's on the node (true or false)
      ipvsPermitAll:

      # kubeRouter.serviceProxy.ipvsGracefulTermination -- Enables the experimental IPVS graceful terminaton capability (true or false)
      ipvsGracefulTermination:

      # kubeRouter.serviceProxy.ipvsGracefulPeriod -- The graceful period before removing destinations from IPVS services (e.g. '5s', '1m', '2h22m'). Must be greater than 0
      ipvsGracefulPeriod:

      # kubeRouter.serviceProxy.hairpinMode -- Add iptables rules for every Service Endpoint to support hairpin traffic (true or false)
      hairpinMode:

      # kubeRouter.serviceProxy.excludedCidrs -- Excluded CIDRs are used to exclude IPVS rules from deletion
      excludedCidrs:

      # kubeRouter.serviceProxy.runtimeEndpoint -- Path to CRI compatible container runtime socket (used for DSR mode).
      runtimeEndpoint:

  podMonitor:
    # podMonitor.enabled -- Set a Prometheus operator PodMonitor ressource (true or false)
    enabled: false

  # resources -- CPU/Memory resource requests/limits
  resources:
    requests:
      cpu: 250m
      memory: 250Mi
    limits:
      cpu: 250m
      memory: 250Mi

  # nodeSelector -- Kube-Router labels for pod assignment
  nodeSelector: {}

  # tolerations -- Kube-Router labels for tolerations pod assignment
  tolerations:
    - key: CriticalAddonsOnly
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/master
      operator: Exists
    - effect: NoSchedule
      key: node.kubernetes.io/not-ready
      operator: Exists
    - effect: NoSchedule
      key: node-role.kubernetes.io/controlplane
      operator: Exists
    - effect: NoExecute
      key: node-role.kubernetes.io/etcd
      operator: Exists

purelb:
  # Default values for purelb.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  # Docker image configuration
  image:
    repository: registry.gitlab.com/purelb/purelb
    pullPolicy: Always

  nameOverride: ""
  fullnameOverride: ""

  Prometheus:
    allocator:
      # Metrics service
      Metrics:
        enabled: false

      ## ServiceMonitor
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md
      ## Note: requires Prometheus Operator to be able to work, for example:
      ## helm install prometheus prometheus-community/kube-prometheus-stack \
      ##   --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
      ##   --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
      serviceMonitor:
        ## Toggle the ServiceMonitor true if you have Prometheus Operator installed and configured
        enabled: false

        ## Specify the labels to add to the ServiceMonitors to be selected for target discovery
        extraLabels: {}

        ## Specify the endpoints
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md#servicemonitor
        endpoints: []
        ## Sample
        ## endpoints:
        ## - port: metrics
        ##   path: /metrics
        ##   scheme: http
      prometheusRules:
        ## Toggle the prometheusRules to true if you have Prometheus Operator installed and configured
        ##
        enabled: false
        ## Specify the namespace where to add to the prometheusRules
        ##
        namespace: ""
        ## Specify the labels to add to the prometheusRules to be selected for target discovery
        extraLabels: {}
        ## Define here the Custom Prometheus rules
        ## e.g:
        ## rules:
        ##   - alert: PurelbServiceGroupHigh
        ##     expr: purelb_address_pool_addresses_in_use * 100 / purelb_address_pool_size > 90
        ##     for: 2m
        ##     labels:
        ##       severity: critical
        ##     annotations:
        ##       summary: PureLB allocator {{`{{`}} $labels.instance {{`}}`}} as high usage of pool
        ##       description: PureLB allocator {{`{{`}} $labels.instance {{`}}`}} as high usage of pool
        rules: []

    lbnodeagent:
      # Metrics service
      Metrics:
        enabled: false

      ## ServiceMonitor
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md
      ## Note: requires Prometheus Operator to be able to work, for example:
      ## helm install prometheus prometheus-community/kube-prometheus-stack \
      ##   --set prometheus.prometheusSpec.podMonitorSelectorNilUsesHelmValues=false \
      ##   --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false
      serviceMonitor:
        ## Toggle the ServiceMonitor true if you have Prometheus Operator installed and configured
        enabled: false

        ## Specify the labels to add to the ServiceMonitors to be selected for target discovery
        extraLabels: {}

        ## Specify the endpoints
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/design.md#servicemonitor
        endpoints: []
        ## Sample
        ## endpoints:
        ## - port: metrics
        ##   path: /metrics
        ##   scheme: http
      prometheusRules:
        ## Toggle the prometheusRules true if you have Prometheus Operator installed and configured
        ##
        enabled: false
        ## Specify the namespace where to add to the prometheusRules
        ##
        namespace: ""
        ## Specify the labels to add to the prometheusRules to be selected for target discovery
        extraLabels: {}
        ## Define here the Custom Prometheus rules
        ## e.g:
        ## rules:
        ##   - alert: PurelbServiceGroupHigh
        ##     expr: purelb_address_pool_addresses_in_use * 100 / purelb_address_pool_size > 90
        ##     for: 2m
        ##     labels:
        ##       severity: critical
        ##     annotations:
        ##       summary: PureLB instance {{ "{{ $labels.instance }}" }} down
        ##       description: Redis&trade; instance {{ "{{ $labels.instance }}" }} is down
        rules: []

  # You may define a valid spec and set create: true to create a ServiceGroup.
  # See https://purelb.gitlab.io/docs/install/config/
  serviceGroup:
    name: 'default'
    create: false
    # For example, to configure a "default" service group, comment the above line and uncomment the following lines:
    # create: true
    # spec:
    #   local:
    #     subnet: '192.168.254.0/24'
    #     pool: '192.168.254.200-192.168.254.201'
    #     aggregation: default

  # This can be used to define a list of arbitrary extra Kubernetes objects to be created (configmaps, serviceGroups, etc.).
  # Example:
  #   extraObjects:
  #     - |
  #       apiVersion: v1
  #       kind: ConfigMap
  #       metadata:
  #         name: {{ .Release.Name }}-extra
  #       data:
  #         hello: world
  #     - |
  #       apiVersion: purelb.io/v1
  #       kind: ServiceGroup
  #       metadata:
  #         name: private
  #       spec:
  #         local:
  #           pool: 192.168.0.0-192.168.0.5
  #           subnet: 192.168.0.0/27
  extraObjects: []

  # PureLB will act as the default service announcer if this value is
  # "PureLB". This means that PureLB will handle services that do not
  # have a Spec.LoadBalancerClass field. If this is other than "PureLB"
  # then PureLB will handle only those services that have a
  # Spec.LoadBalancerClass explictly set to "purelb.io/purelb".
  defaultAnnouncer: "PureLB"

  # This value is passed into the memberlist package. Quoting from the
  # memberlist docs:
  #
  #   SecretKey is used to initialize the primary encryption key in a
  #   keyring.  The primary encryption key is the only key used to
  #   encrypt messages and the first key used while attempting to
  #   decrypt messages. Providing a value for this primary key will
  #   enable message-level encryption and verification, and
  #   automatically install the key onto the keyring.  The value should
  #   be either 16, 24, or 32 bytes to select AES-128, AES-192, or
  #   AES-256.
  #
  # See PureLB's internal/election package docs for more info on how
  # PureLB uses memberlist.
  #
  # This random default value is fine for most purposes but for extra
  # security you can replace it with your own value.
  memberlistSecretKey: "8sb7ikA5qHwQQqxc"

  # Optional priorityClass to use for both allocator and lbnodeagent pods.
  # https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/
  priorityClassName: ""

  # Configurable values specific to lbnodeagent.
  # See https://purelb.gitlab.io/docs/install/config/
  lbnodeagent:
    localint: default
    extlbint: kube-lb0
    sendgarp: false
    podSecurityPolicy:
      enabled: false
    resources:
      limits:
        memory: 100Mi
    # Tolerations: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    tolerations: []
    # For example, to allow lbnodeagents to run on master nodes, comment the above line and uncomment the following lines:
    # tolerations:
    # - effect: NoSchedule
    #   key: node-role.kubernetes.io/master

  # Configurable values specific to allocator.
  allocator:
    podSecurityPolicy:
      enabled: false
    resources:
      limits:
        memory: 100Mi


int-dns:
  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
  ##

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []

  ## @section Common parameters
  ##

  ## @param nameOverride String to partially override external-dns.fullname template (will maintain the release name)
  ##
  nameOverride: ''
  
  ## @param fullnameOverride String to fully override external-dns.fullname template
  ##
  fullnameOverride: int-dns

  ## @param clusterDomain Kubernetes Cluster Domain
  ##
  clusterDomain: cluster.local

  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ##
  ## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template).
  ##
  extraDeploy: []
  ## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)
  ##
  kubeVersion: ""

  ## @param watchReleaseNamespace Watch only namepsace used for the release
  ##
  watchReleaseNamespace: false

  ## @section external-dns parameters
  ##

  ## Bitnami external-dns image version
  ## ref: https://hub.docker.com/r/bitnami/external-dns/tags/
  ## @param image.registry ExternalDNS image registry
  ## @param image.repository ExternalDNS image repository
  ## @param image.tag ExternalDNS Image tag (immutable tags are recommended)
  ## @param image.digest ExternalDNS image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy ExternalDNS image pull policy
  ## @param image.pullSecrets ExternalDNS image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/external-dns

    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []

  ## @param hostAliases Deployment pod host aliases
  ## https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/
  ##
  hostAliases: []

  ## @param updateStrategy update strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#update-strategies
  ##
  updateStrategy: {}

  ## @param command Override kiam default command
  ##
  command: []

  ## @param args Override kiam default args
  ##
  args: []

  ## @param sources [array] K8s resources type to be observed for new DNS entries by ExternalDNS
  ##
  sources:
    - service
    - gateway-httproute
    - gateway-grpcroute
    - gateway-tlsroute
    - gateway-tcproute
    - gateway-udproute


  ## @param provider DNS provider where the DNS records will be created.
  ## Available providers are:
  ## - alibabacloud, aws, azure, azure-private-dns, cloudflare, coredns, designate, digitalocean, google, hetzner, infoblox, linode, rfc2136, transip, oci
  ##
  provider: pdns

  ## @param fqdnTemplates Templated strings that are used to generate DNS names from sources that don't define a hostname themselves
  ##
  fqdnTemplates: []
  ## @param containerPorts.http HTTP Container port
  ##
  containerPorts:
    http: 7979

  ## @param combineFQDNAnnotation Combine FQDN template and annotations instead of overwriting
  ##
  combineFQDNAnnotation: false

  ## @param ignoreHostnameAnnotation Ignore hostname annotation when generating DNS names, valid only when fqdn-template is set
  ##
  ignoreHostnameAnnotation: false

  ## @param publishInternalServices Allow external-dns to publish DNS records for ClusterIP services
  ##
  publishInternalServices: true

  ## @param publishHostIP Allow external-dns to publish host-ip for headless services
  ##
  publishHostIP: false


  ## PowerDNS configuration to be set via arguments/env. variables
  ##
  pdns:
    ## @param pdns.apiUrl When using the PowerDNS provider, specify the API URL of the server.
    ##
    apiUrl: http://ns-core-nsadmin.core-prod.svc.cluster.local
    
    ## @param pdns.apiPort When using the PowerDNS provider, specify the API port of the server.
    ##
    apiPort: '80'
    
    ## @param pdns.secretName When using the PowerDNS provider, specify as secret name containing the API Key
    ##
    secretName: powerdns-api

  ## @param labelFilter Select sources managed by external-dns using label selector (optional)
  ##
  labelFilter: 'lan-mode=private'

  ## @param dryRun When enabled, prints DNS record changes rather than actually performing them (optional)
  ##
  dryRun: false

  ## @param triggerLoopOnEvent When enabled, triggers run loop on create/update/delete events in addition to regular interval (optional)
  ##
  triggerLoopOnEvent: false

  ## @param interval Interval update period to use
  ##
  interval: "1m"

  ## @param logLevel Verbosity of the logs (options: panic, debug, info, warning, error, fatal, trace)
  ##
  logLevel: trace

  ## @param logFormat Which format to output logs in (options: text, json)
  ##
  logFormat: json

  ## @param policy Modify how DNS records are synchronized between sources and providers (options: sync, upsert-only )
  ##
  policy: upsert-only

  ## @param registry Registry method to use (options: txt, aws-sd, noop)
  ## ref: https://github.com/kubernetes-sigs/external-dns/blob/master/docs/proposal/registry.md
  ##
  registry: txt

  ## @param txtPrefix When using the TXT registry, a prefix for ownership records that avoids collision with CNAME entries (optional)<CNAME record> (Mutual exclusive with txt-suffix)
  ##
  txtPrefix: ""

  ## @param txtSuffix When using the TXT registry, a suffix for ownership records that avoids collision with CNAME entries (optional)<CNAME record>.suffix (Mutual exclusive with txt-prefix)
  ##
  txtSuffix: ""

  ## @param txtOwnerId A name that identifies this instance of ExternalDNS. Currently used by registry types: txt & aws-sd (optional)
  ## But other registry types might be added in the future.
  ##
  txtOwnerId: ""

  ## @param forceTxtOwnerId (backward compatibility) When using the non-TXT registry, it will pass the value defined by `txtOwnerId` down to the application (optional)
  ## This setting added for backward compatibility for
  ## customers who already used bitnami/external-dns helm chart
  ## to privision 'aws-sd' registry type.
  ## Previously bitnami/external-dns helm chart did not pass
  ## txtOwnerId value down to the external-dns application
  ## so the app itself sets that value to be a string 'default'.
  ## If existing customers force the actual txtOwnerId value to be
  ## passed properly, their external-dns updates will stop working
  ## because the owner's value for exting DNS records in
  ## AWS Service Discovery would remain 'default'.
  ## NOTE: It is up to the end user to update AWS Service Discovery
  ## 'default' values in description fields to make it work with new
  ## value passed as txtOwnerId when forceTxtOwnerId=true
  ##
  forceTxtOwnerId: false

  ## @param extraArgs Extra arguments to be passed to external-dns
  ##
  extraArgs: {}

  ## @param extraEnvVars An array to add extra env vars
  ##
  extraEnvVars: []

  ## @param extraEnvVarsCM ConfigMap containing extra env vars
  ##
  extraEnvVarsCM: ""

  ## @param extraEnvVarsSecret Secret containing extra env vars (in case of sensitive data)
  ##
  extraEnvVarsSecret: ""

  ## @param lifecycleHooks [object] Override default etcd container hooks
  ##
  lifecycleHooks: {}

  ## @param schedulerName Alternative scheduler
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""

  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment
  ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ## The value is evaluated as a template
  ##
  topologySpreadConstraints: []

  ## @param replicaCount Desired number of ExternalDNS replicas
  ##
  replicaCount: 1

  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""

  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ## Allowed values: soft, hard
  ##
  podAntiAffinityPreset: soft

  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""

    ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""

    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []

  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}

  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}

  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []

  ## @param podAnnotations Additional annotations to apply to the pod.
  ##
  podAnnotations: {}

  ## @param podLabels Additional labels to be added to pods
  ##
  podLabels: {}

  ## @param priorityClassName priorityClassName
  ##
  priorityClassName: ""

  ## @param secretAnnotations Additional annotations to apply to the secret
  ##
  secretAnnotations: {}

  ## Options for the source type "crd"
  ##
  crd:
    ## @param crd.create Install and use the integrated DNSEndpoint CRD
    ##
    create: false
    ## @param crd.apiversion Sets the API version for the CRD to watch
    ##
    apiversion: ""
    ## @param crd.kind Sets the kind for the CRD to watch
    ##
    kind: ""

  ## Kubernetes svc configutarion
  ##
  service:
    ## @param service.enabled Whether to create Service resource or not
    ##
    enabled: true
    ## @param service.type Kubernetes Service type
    ##
    type: ClusterIP
    ## @param service.ports.http ExternalDNS client port
    ##
    ports:
      http: 7979
    ## @param service.nodePorts.http Port to bind to for NodePort service type (client port)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      http: ""

    ## @param service.clusterIP IP address to assign to service
    ##
    clusterIP: ""

    ## @param service.externalIPs Service external IP addresses
    ##
    externalIPs: []

    ## @param service.loadBalancerIP IP address to assign to load balancer (if supported)
    ##
    loadBalancerIP: ""

    ## @param service.loadBalancerSourceRanges List of IP CIDRs allowed access to load balancer (if supported)
    ##
    loadBalancerSourceRanges: []

    ## @param service.externalTrafficPolicy Enable client source IP preservation
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster

    ## @param service.extraPorts Extra ports to expose in the service (normally used with the `sidecar` value)
    ##
    extraPorts: []

    ## @param service.annotations Annotations to add to service
    ## set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}

    ## @param service.labels Provide any additional labels which may be required.
    ## This can be used to have external-dns show up in `kubectl cluster-info`
    ##  kubernetes.io/cluster-service: "true"
    ##  kubernetes.io/name: "external-dns"
    ##
    labels: {}

    ## @param service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
    ## If "ClientIP", consecutive client requests will be directed to the same Pod
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
    ##
    sessionAffinity: None

    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}

  ## ServiceAccount parameters
  ## https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  ##
  serviceAccount:
    ## @param serviceAccount.create Determine whether a Service Account should be created or it should reuse a exiting one.
    ##
    create: true
    ## @param serviceAccount.name ServiceAccount to use. A name is generated using the external-dns.fullname template if it is not set
    ##
    name: ""
    ## @param serviceAccount.annotations Additional Service Account annotations
    ##
    annotations: {}
    ## @param serviceAccount.automountServiceAccountToken Automount API credentials for a service account.
    ##
    automountServiceAccountToken: true
    ## @param serviceAccount.labels [object] Additional labels to be included on the service account
    ##
    labels: {}

  ## RBAC parameters
  ## https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: true

    ## @param rbac.clusterRole Whether to create Cluster Role. When set to false creates a Role in `namespace`
    ##
    clusterRole: true

    ## @param rbac.apiVersion Version of the RBAC API
    ##
    apiVersion: v1

    ## @param rbac.pspEnabled Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    pspEnabled: false


  ## Container resource requests and limits
  ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
  ## We usually recommend not to specify default resources and to leave this as a conscious
  ## choice for the user. This also increases chances charts run on environments with little
  ## resources, such as Minikube. If you do want to specify resources, uncomment the following
  ## lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  ## @param resources.limits The resources limits for the container
  ## @param resources.requests The requested resources for the container
  ##
  resources:
    ## Example:
    ## limits:
    ##    cpu: 50m
    ##    memory: 50Mi
    ##
    limits: {}
    ## Examples:
    ## requests:
    ##    cpu: 10m
    ##    memory: 50Mi
    ##
    requests: {}


  ## Configure extra options for liveness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param livenessProbe.enabled Enable livenessProbe
  ## @param livenessProbe.initialDelaySeconds Initial delay seconds for livenessProbe
  ## @param livenessProbe.periodSeconds Period seconds for livenessProbe
  ## @param livenessProbe.timeoutSeconds Timeout seconds for livenessProbe
  ## @param livenessProbe.failureThreshold Failure threshold for livenessProbe
  ## @param livenessProbe.successThreshold Success threshold for livenessProbe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 2
    successThreshold: 1


  ## Configure extra options for readiness probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#configure-probes
  ## @param readinessProbe.enabled Enable readinessProbe
  ## @param readinessProbe.initialDelaySeconds Initial delay seconds for readinessProbe
  ## @param readinessProbe.periodSeconds Period seconds for readinessProbe
  ## @param readinessProbe.timeoutSeconds Timeout seconds for readinessProbe
  ## @param readinessProbe.failureThreshold Failure threshold for readinessProbe
  ## @param readinessProbe.successThreshold Success threshold for readinessProbe
  ##
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1


  ## Configure extra options for startup probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-startup-probes/#configure-probes
  ## @param startupProbe.enabled Enable startupProbe
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1


  ## @param customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}

  ## @param customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}

  ## @param customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}

  ## @param extraVolumes A list of volumes to be added to the pod
  ##
  extraVolumes: []

  ## @param extraVolumeMounts A list of volume mounts to be added to the pod
  ##
  extraVolumeMounts: []

  ## @param podDisruptionBudget Configure PodDisruptionBudget
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ##

  podDisruptionBudget: {}

  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param metrics.enabled Enable prometheus to access external-dns metrics endpoint
    ##
    enabled: false

    ## @param metrics.podAnnotations Annotations for enabling prometheus to access the metrics endpoint
    ##
    podAnnotations: {}

    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor object
      ##
      enabled: false

      ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
      ##
      namespace: ""

      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""

      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""

      ## @param metrics.serviceMonitor.selector Additional labels for ServiceMonitor object
      ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration
      ## e.g:
      ## selector:
      ##   prometheus: my-prometheus
      ##
      selector: {}

      ## @param metrics.serviceMonitor.metricRelabelings Specify Metric Relabelings to add to the scrape endpoint
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []

      ## @param metrics.serviceMonitor.relabelings [array] Prometheus relabeling rules
      ##
      relabelings: []

      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false

      ## DEPRECATED metrics.serviceMonitor.additionalLabels will be removed in a future release - Please use metrics.serviceMonitor.labels instead
      ## @param metrics.serviceMonitor.labels Used to pass Labels that are required by the installed Prometheus Operator
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      labels: {}

      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""



ext-dns:
  ## @section Global parameters
  ## Global Docker image parameters
  ## Please, note that this will override the image parameters, including dependencies, configured to use the global value
  ## Current available global Docker image parameters: imageRegistry, imagePullSecrets and storageClass
  ##

  ## @param global.imageRegistry Global Docker image registry
  ## @param global.imagePullSecrets Global Docker registry secret names as an array
  ##
  global:
    imageRegistry: ""
    ## E.g.
    ## imagePullSecrets:
    ##   - myRegistryKeySecretName
    ##
    imagePullSecrets: []

  ## @section Common parameters
  ##

  ## @param nameOverride String to partially override external-dns.fullname template (will maintain the release name)
  ##
  nameOverride: ''

  ## @param fullnameOverride String to fully override external-dns.fullname template
  ##
  fullnameOverride: ext-dns

  ## @param clusterDomain Kubernetes Cluster Domain
  ##
  clusterDomain: cluster.local

  ## @param commonLabels Labels to add to all deployed objects
  ##
  commonLabels: {}
  ## @param commonAnnotations Annotations to add to all deployed objects
  ##
  commonAnnotations: {}
  ##
  ## @param extraDeploy Array of extra objects to deploy with the release (evaluated as a template).
  ##
  extraDeploy: []
  ## @param kubeVersion Force target Kubernetes version (using Helm capabilities if not set)
  ##
  kubeVersion: ""

  ## @param watchReleaseNamespace Watch only namepsace used for the release
  ##
  watchReleaseNamespace: false

  ## @section external-dns parameters
  ##

  ## Bitnami external-dns image version
  ## ref: https://hub.docker.com/r/bitnami/external-dns/tags/
  ## @param image.registry ExternalDNS image registry
  ## @param image.repository ExternalDNS image repository
  ## @param image.tag ExternalDNS Image tag (immutable tags are recommended)
  ## @param image.digest ExternalDNS image digest in the way sha256:aa.... Please note this parameter, if set, will override the tag
  ## @param image.pullPolicy ExternalDNS image pull policy
  ## @param image.pullSecrets ExternalDNS image pull secrets
  ##
  image:
    registry: docker.io
    repository: bitnami/external-dns

    ## Specify a imagePullPolicy
    ## Defaults to 'Always' if image tag is 'latest', else set to 'IfNotPresent'
    ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
    ##
    pullPolicy: IfNotPresent

    ## Optionally specify an array of imagePullSecrets.
    ## Secrets must be manually created in the namespace.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ## e.g:
    ## pullSecrets:
    ##   - myRegistryKeySecretName
    ##
    pullSecrets: []


  ## @param updateStrategy update strategy type
  ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#update-strategies
  ##
  updateStrategy: {}

  ## @param command Override kiam default command
  ##
  command: []

  ## @param args Override kiam default args
  ##
  args: []

  ## @param sources [array] K8s resources type to be observed for new DNS entries by ExternalDNS
  ##
  sources:
    - service
    - ingress
    - gateway-httproute
    - gateway-grpcroute
    - gateway-tlsroute
    - gateway-tcproute
    - gateway-udproute


  ## @param provider DNS provider where the DNS records will be created.
  ## Available providers are:
  ## - alibabacloud, aws, azure, azure-private-dns, cloudflare, coredns, designate, digitalocean, google, hetzner, infoblox, linode, rfc2136, transip, oci
  ##
  provider: cloudflare

  ## @param fqdnTemplates Templated strings that are used to generate DNS names from sources that don't define a hostname themselves
  ##
  fqdnTemplates: []

  ## @param combineFQDNAnnotation Combine FQDN template and annotations instead of overwriting
  ##
  combineFQDNAnnotation: false

  ## @param ignoreHostnameAnnotation Ignore hostname annotation when generating DNS names, valid only when fqdn-template is set
  ##
  ignoreHostnameAnnotation: false

  ## @param publishInternalServices Allow external-dns to publish DNS records for ClusterIP services
  ##
  publishInternalServices: false

  ## @param publishHostIP Allow external-dns to publish host-ip for headless services
  ##
  publishHostIP: false

  ## @param serviceTypeFilter The service types to take care about (default: all, options: ClusterIP, NodePort, LoadBalancer, ExternalName)
  ##
  serviceTypeFilter: []


  ## Cloudflare configuration to be set via arguments/env. variables
  ##
  cloudflare:
    ## @param cloudflare.secretName When using the Cloudflare provider, it's the name of the secret containing cloudflare_api_token or cloudflare_api_key.
    ## This ignores cloudflare.apiToken, and cloudflare.apiKey
    ##
    secretName: kjdev-cloudflare

    proxied: false

  ## @param domainFilters Limit possible target zones by domain suffixes (optional)
  ##
  domainFilters: []

  ## @param excludeDomains Exclude subdomains (optional)
  ##
  excludeDomains: []

  ## @param regexDomainFilter Limit possible target zones by regex domain suffixes (optional)
  ## If regexDomainFilter is specified, domainFilters will be ignored
  ##
  regexDomainFilter: ""

  ## @param regexDomainExclusion Exclude subdomains by using regex pattern (optional)
  ## If regexDomainFilter is specified, excludeDomains will be ignored and external-dns will use regexDomainExclusion even though regexDomainExclusion is empty
  ##
  regexDomainExclusion: ""

  ## @param zoneNameFilters Filter target zones by zone domain (optional)
  ##
  zoneNameFilters: []

  ## @param zoneIdFilters Limit possible target zones by zone id (optional)
  ##
  zoneIdFilters: []

  ## @param annotationFilter Filter sources managed by external-dns via annotation using label selector (optional)
  ##
  annotationFilter: ""

  ## @param labelFilter Select sources managed by external-dns using label selector (optional)
  ##
  labelFilter: 'wan-mode=public'

  ## @param dryRun When enabled, prints DNS record changes rather than actually performing them (optional)
  ##
  dryRun: false

  ## @param triggerLoopOnEvent When enabled, triggers run loop on create/update/delete events in addition to regular interval (optional)
  ##
  triggerLoopOnEvent: false

  ## @param interval Interval update period to use
  ##
  interval: '1m'

  ## @param logLevel Verbosity of the logs (options: panic, debug, info, warning, error, fatal, trace)
  ##
  logLevel: trace

  ## @param logFormat Which format to output logs in (options: text, json)
  ##
  logFormat: json

  ## @param policy Modify how DNS records are synchronized between sources and providers (options: sync, upsert-only )
  ##
  policy: upsert-only

  ## @param registry Registry method to use (options: txt, aws-sd, noop)
  ## ref: https://github.com/kubernetes-sigs/external-dns/blob/master/docs/proposal/registry.md
  ##
  registry: "txt"

  ## @param txtPrefix When using the TXT registry, a prefix for ownership records that avoids collision with CNAME entries (optional)<CNAME record> (Mutual exclusive with txt-suffix)
  ##
  txtPrefix: ""

  ## @param txtSuffix When using the TXT registry, a suffix for ownership records that avoids collision with CNAME entries (optional)<CNAME record>.suffix (Mutual exclusive with txt-prefix)
  ##
  txtSuffix: ""

  ## @param txtOwnerId A name that identifies this instance of ExternalDNS. Currently used by registry types: txt & aws-sd (optional)
  ## But other registry types might be added in the future.
  ##
  txtOwnerId: k0s-dc1

  ## @param forceTxtOwnerId (backward compatibility) When using the non-TXT registry, it will pass the value defined by `txtOwnerId` down to the application (optional)
  ## This setting added for backward compatibility for
  ## customers who already used bitnami/external-dns helm chart
  ## to privision 'aws-sd' registry type.
  ## Previously bitnami/external-dns helm chart did not pass
  ## txtOwnerId value down to the external-dns application
  ## so the app itself sets that value to be a string 'default'.
  ## If existing customers force the actual txtOwnerId value to be
  ## passed properly, their external-dns updates will stop working
  ## because the owner's value for exting DNS records in
  ## AWS Service Discovery would remain 'default'.
  ## NOTE: It is up to the end user to update AWS Service Discovery
  ## 'default' values in description fields to make it work with new
  ## value passed as txtOwnerId when forceTxtOwnerId=true
  ##
  forceTxtOwnerId: false

  ## @param extraArgs Extra arguments to be passed to external-dns
  ##
  extraArgs: {}
  ## @param extraEnvVars An array to add extra env vars
  ##
  extraEnvVars: []
  ## @param extraEnvVarsCM ConfigMap containing extra env vars
  ##
  extraEnvVarsCM: ""
  ## @param extraEnvVarsSecret Secret containing extra env vars (in case of sensitive data)
  ##
  extraEnvVarsSecret: ""
  ## @param lifecycleHooks [object] Override default etcd container hooks
  ##
  lifecycleHooks: {}
  ## @param schedulerName Alternative scheduler
  ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
  ##
  schedulerName: ""
  ## @param topologySpreadConstraints Topology Spread Constraints for pod assignment
  ## https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
  ## The value is evaluated as a template
  ##
  topologySpreadConstraints: []
  ## @param replicaCount Desired number of ExternalDNS replicas
  ##
  replicaCount: 1
  ## @param podAffinityPreset Pod affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ##
  podAffinityPreset: ""
  ## @param podAntiAffinityPreset Pod anti-affinity preset. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  ## Allowed values: soft, hard
  ##
  podAntiAffinityPreset: soft
  ## Node affinity preset
  ## Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#node-affinity
  ##
  nodeAffinityPreset:
    ## @param nodeAffinityPreset.type Node affinity preset type. Ignored if `affinity` is set. Allowed values: `soft` or `hard`
    ##
    type: ""
    ## @param nodeAffinityPreset.key Node label key to match Ignored if `affinity` is set.
    ## E.g.
    ## key: "kubernetes.io/e2e-az-name"
    ##
    key: ""
    ## @param nodeAffinityPreset.values Node label values to match. Ignored if `affinity` is set.
    ## E.g.
    ## values:
    ##   - e2e-az1
    ##   - e2e-az2
    ##
    values: []
  ## @param affinity Affinity for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
  ## Note: podAffinityPreset, podAntiAffinityPreset, and  nodeAffinityPreset will be ignored when it's set
  ##
  affinity: {}
  ## @param nodeSelector Node labels for pod assignment
  ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
  ##
  nodeSelector: {}
  ## @param tolerations Tolerations for pod assignment
  ## Ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
  ##
  tolerations: []
  ## @param podAnnotations Additional annotations to apply to the pod.
  ##
  podAnnotations: {}
  ## @param podLabels Additional labels to be added to pods
  ##
  podLabels: {}
  ## @param priorityClassName priorityClassName
  ##
  priorityClassName: ""
  ## @param secretAnnotations Additional annotations to apply to the secret
  ##
  secretAnnotations: {}
  ## Options for the source type "crd"
  ##
  crd:
    ## @param crd.create Install and use the integrated DNSEndpoint CRD
    ##
    create: false
    ## @param crd.apiversion Sets the API version for the CRD to watch
    ##
    apiversion: ""
    ## @param crd.kind Sets the kind for the CRD to watch
    ##
    kind: ""

  ## Kubernetes svc configutarion
  ##
  service:
    ## @param service.enabled Whether to create Service resource or not
    ##
    enabled: true
    ## @param service.type Kubernetes Service type
    ##
    type: ClusterIP
    ## @param service.ports.http ExternalDNS client port
    ##
    ports:
      http: 7979
    ## @param service.nodePorts.http Port to bind to for NodePort service type (client port)
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    ##
    nodePorts:
      http: ""
    ## @param service.clusterIP IP address to assign to service
    ##
    clusterIP: ""
    ## @param service.externalIPs Service external IP addresses
    ##
    externalIPs: []
    ## @param service.loadBalancerIP IP address to assign to load balancer (if supported)
    ##
    loadBalancerIP: ""
    ## @param service.loadBalancerSourceRanges List of IP CIDRs allowed access to load balancer (if supported)
    ##
    loadBalancerSourceRanges: []
    ## @param service.externalTrafficPolicy Enable client source IP preservation
    ## ref http://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip
    ##
    externalTrafficPolicy: Cluster
    ## @param service.extraPorts Extra ports to expose in the service (normally used with the `sidecar` value)
    ##
    extraPorts: []
    ## @param service.annotations Annotations to add to service
    ## set the LoadBalancer service type to internal only.
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer
    ##
    annotations: {}
    ## @param service.labels Provide any additional labels which may be required.
    ## This can be used to have external-dns show up in `kubectl cluster-info`
    ##  kubernetes.io/cluster-service: "true"
    ##  kubernetes.io/name: "external-dns"
    ##
    labels: {}
    ## @param service.sessionAffinity Session Affinity for Kubernetes service, can be "None" or "ClientIP"
    ## If "ClientIP", consecutive client requests will be directed to the same Pod
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#virtual-ips-and-service-proxies
    ##
    sessionAffinity: None
    ## @param service.sessionAffinityConfig Additional settings for the sessionAffinity
    ## sessionAffinityConfig:
    ##   clientIP:
    ##     timeoutSeconds: 300
    ##
    sessionAffinityConfig: {}

  ## RBAC parameters
  ## https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  ##
  rbac:
    ## @param rbac.create Whether to create & use RBAC resources or not
    ##
    create: true

    ## @param rbac.clusterRole Whether to create Cluster Role. When set to false creates a Role in `namespace`
    ##
    clusterRole: true

    ## @param rbac.apiVersion Version of the RBAC API
    ##
    apiVersion: v1

    ## @param rbac.pspEnabled Whether to create a PodSecurityPolicy. WARNING: PodSecurityPolicy is deprecated in Kubernetes v1.21 or later, unavailable in v1.25 or later
    ##
    pspEnabled: false







  ## Configure extra options for startup probe
  ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-startup-probes/#configure-probes
  ## @param startupProbe.enabled Enable startupProbe
  ## @param startupProbe.initialDelaySeconds Initial delay seconds for startupProbe
  ## @param startupProbe.periodSeconds Period seconds for startupProbe
  ## @param startupProbe.timeoutSeconds Timeout seconds for startupProbe
  ## @param startupProbe.failureThreshold Failure threshold for startupProbe
  ## @param startupProbe.successThreshold Success threshold for startupProbe
  ##
  startupProbe:
    enabled: false
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1


  ## @param customLivenessProbe Override default liveness probe
  ##
  customLivenessProbe: {}

  ## @param customReadinessProbe Override default readiness probe
  ##
  customReadinessProbe: {}

  ## @param customStartupProbe Override default startup probe
  ##
  customStartupProbe: {}

  ## @param extraVolumes A list of volumes to be added to the pod
  ##
  extraVolumes: []

  ## @param extraVolumeMounts A list of volume mounts to be added to the pod
  ##
  extraVolumeMounts: []

  ## @param podDisruptionBudget Configure PodDisruptionBudget
  ## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb/
  ##

  podDisruptionBudget: {}

  ## Prometheus Exporter / Metrics
  ##
  metrics:
    ## @param metrics.enabled Enable prometheus to access external-dns metrics endpoint
    ##
    enabled: false

    ## @param metrics.podAnnotations Annotations for enabling prometheus to access the metrics endpoint
    ##
    podAnnotations: {}

    ## Prometheus Operator ServiceMonitor configuration
    ##
    serviceMonitor:
      ## @param metrics.serviceMonitor.enabled Create ServiceMonitor object
      ##
      enabled: false

      ## @param metrics.serviceMonitor.namespace Namespace in which Prometheus is running
      ##
      namespace: ""

      ## @param metrics.serviceMonitor.interval Interval at which metrics should be scraped
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      interval: ""

      ## @param metrics.serviceMonitor.scrapeTimeout Timeout after which the scrape is ended
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#endpoint
      ##
      scrapeTimeout: ""

      ## @param metrics.serviceMonitor.selector Additional labels for ServiceMonitor object
      ## ref: https://github.com/bitnami/charts/tree/master/bitnami/prometheus-operator#prometheus-configuration
      ## e.g:
      ## selector:
      ##   prometheus: my-prometheus
      ##
      selector: {}

      ## @param metrics.serviceMonitor.metricRelabelings Specify Metric Relabelings to add to the scrape endpoint
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#relabelconfig
      ##
      metricRelabelings: []

      ## @param metrics.serviceMonitor.relabelings [array] Prometheus relabeling rules
      ##
      relabelings: []

      ## @param metrics.serviceMonitor.honorLabels Specify honorLabels parameter to add the scrape endpoint
      ##
      honorLabels: false

      ## DEPRECATED metrics.serviceMonitor.additionalLabels will be removed in a future release - Please use metrics.serviceMonitor.labels instead
      ## @param metrics.serviceMonitor.labels Used to pass Labels that are required by the installed Prometheus Operator
      ## ref: https://github.com/coreos/prometheus-operator/blob/master/Documentation/api.md#prometheusspec
      ##
      labels: {}

      ## @param metrics.serviceMonitor.jobLabel The name of the label on the target service to use as the job name in prometheus.
      ##
      jobLabel: ""


metallb:
  # Default values for metallb.
  # This is a YAML-formatted file.
  # Declare variables to be passed into your templates.

  imagePullSecrets: []
  nameOverride: ""
  fullnameOverride: ""
  loadBalancerClass: ""

  # To configure MetalLB, you must specify ONE of the following two
  # options.

  rbac:
    # create specifies whether to install and use RBAC rules.
    create: true

  prometheus:
    # scrape annotations specifies whether to add Prometheus metric
    # auto-collection annotations to pods. See
    # https://github.com/prometheus/prometheus/blob/release-2.1/documentation/examples/prometheus-kubernetes.yml
    # for a corresponding Prometheus configuration. Alternatively, you
    # may want to use the Prometheus Operator
    # (https://github.com/coreos/prometheus-operator) for more powerful
    # monitoring configuration. If you use the Prometheus operator, this
    # can be left at false.
    scrapeAnnotations: false

    # port both controller and speaker will listen on for metrics
    metricsPort: 7472

    # if set, enables rbac proxy on the controller and speaker to expose
    # the metrics via tls.
    # secureMetricsPort: 9120

    # the name of the secret to be mounted in the speaker pod
    # to expose the metrics securely. If not present, a self signed
    # certificate to be used.
    speakerMetricsTLSSecret: ""

    # the name of the secret to be mounted in the controller pod
    # to expose the metrics securely. If not present, a self signed
    # certificate to be used.
    controllerMetricsTLSSecret: ""

    # prometheus doens't have the permission to scrape all namespaces so we give it permission to scrape metallb's one
    rbacPrometheus: true

    # the service account used by prometheus
    # required when " .Values.prometheus.rbacPrometheus == true " and " .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true "
    serviceAccount: ""

    # the namespace where prometheus is deployed
    # required when " .Values.prometheus.rbacPrometheus == true " and " .Values.prometheus.podMonitor.enabled=true or prometheus.serviceMonitor.enabled=true "
    namespace: ""

    # the image to be used for the kuberbacproxy container
    rbacProxy:
      repository: gcr.io/kubebuilder/kube-rbac-proxy

    # Prometheus Operator PodMonitors
    podMonitor:
      # enable support for Prometheus Operator
      enabled: false

      # optional additionnal labels for podMonitors
      additionalLabels: {}

      # optional annotations for podMonitors
      annotations: {}

      # Job label for scrape target
      jobLabel: "app.kubernetes.io/name"

      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:

      # 	metric relabel configs to apply to samples before ingestion.
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

    # Prometheus Operator ServiceMonitors. To be used as an alternative
    # to podMonitor, supports secure metrics.
    serviceMonitor:
      # enable support for Prometheus Operator
      enabled: false

      speaker:
        # optional additional labels for the speaker serviceMonitor
        additionalLabels: {}
        # optional additional annotations for the speaker serviceMonitor
        annotations: {}
        # optional tls configuration for the speaker serviceMonitor, in case
        # secure metrics are enabled.
        tlsConfig:
          insecureSkipVerify: true

      controller:
        # optional additional labels for the controller serviceMonitor
        additionalLabels: {}
        # optional additional annotations for the controller serviceMonitor
        annotations: {}
        # optional tls configuration for the controller serviceMonitor, in case
        # secure metrics are enabled.
        tlsConfig:
          insecureSkipVerify: true

      # Job label for scrape target
      jobLabel: "app.kubernetes.io/name"

      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval:

      # 	metric relabel configs to apply to samples before ingestion.
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # 	relabel configs to apply to samples before ingestion.
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   target_label: nodename
      #   replacement: $1
      #   action: replace

    # Prometheus Operator alertmanager alerts
    prometheusRule:
      # enable alertmanager alerts
      enabled: false

      # optional additionnal labels for prometheusRules
      additionalLabels: {}

      # optional annotations for prometheusRules
      annotations: {}

      # MetalLBStaleConfig
      staleConfig:
        enabled: true
        labels:
          severity: warning

      # MetalLBConfigNotLoaded
      configNotLoaded:
        enabled: true
        labels:
          severity: warning

      # MetalLBAddressPoolExhausted
      addressPoolExhausted:
        enabled: true
        labels:
          severity: alert

      addressPoolUsage:
        enabled: true
        thresholds:
          - percent: 75
            labels:
              severity: warning
          - percent: 85
            labels:
              severity: warning
          - percent: 95
            labels:
              severity: alert

      # MetalLBBGPSessionDown
      bgpSessionDown:
        enabled: true
        labels:
          severity: alert

      extraAlerts: []

  # controller contains configuration specific to the MetalLB cluster
  # controller.
  controller:
    enabled: true

    # -- Controller log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`
    logLevel: info

    # command: /controller
    # webhookMode: enabled

    image:
      repository: quay.io/metallb/controller
      tag:
      pullPolicy:

    ## @param controller.updateStrategy.type Metallb controller deployment strategy type.
    ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy
    ## e.g:
    ## strategy:
    ##  type: RollingUpdate
    ##  rollingUpdate:
    ##    maxSurge: 25%
    ##    maxUnavailable: 25%
    ##
    strategy:
      type: RollingUpdate

    serviceAccount:
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use. If not set and create is
      # true, a name is generated using the fullname template
      name: ""
      annotations: {}

    securityContext:
      runAsNonRoot: true
      # nobody
      runAsUser: 65534
      fsGroup: 65534

    resources: {}
      # limits:
        # cpu: 100m
        # memory: 100Mi

    nodeSelector: {}

    tolerations: []

    priorityClassName: ""

    runtimeClassName: ""

    affinity: {}

    podAnnotations: {}

    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

  # speaker contains configuration specific to the MetalLB speaker
  # daemonset.
  speaker:
    enabled: false

    # command: /speaker

    # -- Speaker log level. Must be one of: `all`, `debug`, `info`, `warn`, `error` or `none`
    logLevel: info

    tolerateMaster: true

    memberlist:
      enabled: true
      mlBindPort: 7946

    image:
      repository: quay.io/metallb/speaker
      tag:
      pullPolicy:

    ## @param speaker.updateStrategy.type Speaker daemonset strategy type
    ## ref: https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/
    ##
    updateStrategy:
      ## StrategyType
      ## Can be set to RollingUpdate or OnDelete
      ##
      type: RollingUpdate

    serviceAccount:
      # Specifies whether a ServiceAccount should be created
      create: true
      # The name of the ServiceAccount to use. If not set and create is
      # true, a name is generated using the fullname template
      name: ""
      annotations: {}

    ## Defines a secret name for the controller to generate a memberlist encryption secret
    ## By default secretName: {{ "metallb.fullname" }}-memberlist
    ##
    # secretName:
    resources: {}
      # limits:
        # cpu: 100m
        # memory: 100Mi

    nodeSelector: {}

    tolerations: []

    priorityClassName: ""

    affinity: {}

    ## Selects which runtime class will be used by the pod.
    runtimeClassName: ""

    podAnnotations: {}

    livenessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    readinessProbe:
      enabled: true
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1

    # frr contains configuration specific to the MetalLB FRR container,
    # for speaker running alongside FRR.
    frr:
      enabled: true

      image:
        repository: frrouting/frr
        tag: v7.5.1
        pullPolicy:

      metricsPort: 7473

      resources: {}

      # if set, enables a rbac proxy sidecar container on the speaker to
      # expose the frr metrics via tls.
      # secureMetricsPort: 9121

    reloader:
      resources: {}

    frrMetrics:
      resources: {}

  crds:
    enabled: true
    validationFailurePolicy: Fail
