artifact-hub:
  enabled: true

  # Artifact Hub Chart default configuration values

  # Enable dynamic resource name prefix
  #
  # Enabling the dynamic resource name prefix ensures that the resources are named dynamically based on the Helm
  # installation's name. This allows multiple installations of this chart in a single Kubernetes namespace. The prefix
  # can be defined by using the `fullnameOverride`.
  dynamicResourceNamePrefixEnabled: true

  # Overwrites the installation's fullname generation (used for the dynamic resource name prefix)
  fullnameOverride: artifacthub-prod

  # Enable restricted HTTP client
  #
  # Artifact Hub makes external HTTP requests for several purposes, like getting repositories metadata, dispatching
  # webhooks, etc. When this option is enabled, requests to the private network space as well as to some other special
  # addresses won't be allowed.
  restrictedHTTPClient: false

  # Logging configuration
  log:
    # Log level
    # Options: "trace", "debug", "info", "warn", "error", "fatal", "panic"
    level: panic
    # Enable pretty logging
    pretty: false

  # Database configuration
  db:
    host: 'psql.mylogin.space'
    port: '5432'
    database: <path:CORE0_SITE1/data/ArtifactHub/Database#Database>
    user: <path:CORE0_SITE1/data/ArtifactHub/Database#Username>
    password: <path:CORE0_SITE1/data/ArtifactHub/Database#Password>
    sslmode: prefer

  # Email configuration
  email:
    # From name used in emails
    fromName: ""
    # From address used in emails. This field is required if you want to enable email sending in Artifact Hub
    from: bots@mylogin.space
    # Reply-to address used in emails
    replyTo: bots@mylogin.space
    # SMTP server configuration
    smtp:
      # Authentication mechanism
      # Options: "login", "plain"
      auth: plain
      # SMTP host. This field is required if you want to enable email sending in Artifact Hub
      host: mail.mylogin.space
      # SMTP port. This field is required if you want to enable email sending in Artifact Hub
      port: 587
      username: ""
      password: ""

  # Credentials
  creds:
    # Docker registry username
    dockerUsername: <path:CORE0_SITE1/data/ArtifactHub/Docker#Username>
    # Docker registry password
    dockerPassword: <path:CORE0_SITE1/data/ArtifactHub/Docker#Password>

  # Images configuration
  images:
    # Images store
    # Options: "pg"
    store: pg

  # Events configuration
  events:
    # Enable repository scanning errors events
    scanningErrors: false
    # Enable repository tracking errors events
    trackingErrors: true

  # Database migrator configuration
  dbMigrator:
    job:
      image:
        # Database migrator image repository (without the tag)
        repository: artifacthub/db-migrator

      # Limits the lifetime of the job after it has finished execution
      ttlSecondsAfterFinished: null
      securityContext: {}
      resources: {}
      # If you do want to specify resources, uncomment the following
      # lines and adjust them as necessary.
      #   limits:
      #     cpu: 100m
      #     memory: 128Mi
      #   requests:
      #     cpu: 100m
      #     memory: 128Mi
    # Load demo user and sample repositories
    loadSampleData: false
    # Directory path where the configuration files should be mounted
    configDir: "/home/db-migrator/.cfg"

  # Hub configuration
  hub:
    ingress:
      enabled: false

    service:
      type: ClusterIP
      port: 80

    # Service account for Artifact Hub to use
    serviceAccount:
      # Enable creation of ServiceAccount for Artifact Hub pod
      create: true

      # Allows auto mount of ServiceAccountToken on the serviceAccount created
      automountServiceAccountToken: true

    # Create and bind role for service account
    rbac:
      # Enable creation and binding of role
      create: true
    deploy:
      readinessGates: []
      replicaCount: 1
      image:
        # Hub image repository (without the tag)
        repository: artifacthub/hub
      securityContext: {}
      resources:
        requests:
          cpu: 15m
          memory: 127M
        limits:
          cpu: 15m
          memory: 127M

      initContainers:
        checkDbMigrator:
          image:
            repository: bitnami/kubectl
            # tag: 1.21
          resources: {}
          # If you do want to specify resources, uncomment the following
          # lines and adjust them as necessary.
          #   limits:
          #     cpu: 100m
          #     memory: 128Mi
          #   requests:
          #     cpu: 100m
          #     memory: 128Mi
        checkDbIsReady:
          resources: {}
          # If you do want to specify resources, uncomment the following
          # lines and adjust them as necessary.
          #   limits:
          #     cpu: 100m
          #     memory: 128Mi
          #   requests:
          #     cpu: 100m
          #     memory: 128Mi
      # Optionally specify extra list of additional labels for the hub deployment
      extraDeploymentLabels: {}
      # Optionally specify extra list of additional labels for hub pods
      extraPodLabels: {}
      # Optionally specify extra list of additional volumes for the hub deployment
      extraVolumes: []
      # Optionally specify extra list of additional volume mounts for the hub deployment
      extraVolumeMounts: []
    server:
      # Allow adding private repositories to the Hub
      allowPrivateRepositories: true

      # Allow new Users to Sign Up
      allowUserSignUp: true

      # Cache directory path. If set, the cache directory for the Helm client will be explicitly set (otherwise
      # defaults to $HOME/.cache), and the directory will be mounted as ephemeral volume (emptyDir)
      cacheDir: ""

      # Directory path where the configuration files should be mounted
      configDir: "/home/hub/.cfg"

      # Banners configuration file url
      bannersURL: ""

      # Hub server base url
      baseURL: ""

      # Hub server shutdown timeout
      shutdownTimeout: 10s

      # Message of the day. The message of the day will be displayed in a banner on the top of the Artifact Hub UI
      motd: ""

      # Message of the day severity. The color used for the banner will be based on the severity selected
      # Options: "info", "warning", "error"
      motdSeverity: info

      basicAuth:
        # Enable Hub basic auth
        enabled: false
        # Hub basic auth username
        username: hub
        # Hub basic auth password
        password: changeme
      cookie:
        # Hub cookie hash key
        hashKey: default-unsafe-key

        # Enable Hub secure cookies
        secure: false

      csrf:
        # CSRF authentication key
        authKey: default-unsafe-key
        # CSRF secure cookie
        secure: false

      oauth:
        github:
          # Enable GitHub OAuth
          enabled: false

        google:
          # Enable Google oauth
          enabled: false

        oidc:
          # Enable OIDC
          enabled: true

          # OpenID connect issuer url
          issuerURL: https://idp.mylogin.space/application/o/k0s-dc1/

          # OpenID connect oauth client id
          clientID: <path:CORE0_SITE1/data/ArtifactHub/OIDC#ClientID>

          # OpenID connect oauth client secret
          clientSecret: <path:CORE0_SITE1/data/ArtifactHub/OIDC#ClientSecret>

          # OpenID connect oauth redirect url
          redirectURL: https://artifacthub.int.mylogin.space/oauth/oidc/callback

          # OpenID connect oauth scopes
          scopes:
            - openid
            - profile
            - email

          # Skip email verified check
          skipEmailVerifiedCheck: true

      # X-Forwarded-For IP index
      xffIndex: 0

    analytics:
      # Google Analytics tracking id
      gaTrackingID: ""

  # Scanner configuration
  scanner:
    enabled: false
    cronjob:
      image:
        # Scanner image repository (without the tag)
        repository: artifacthub/scanner
      securityContext: {}
      resources: {}
      # If you do want to specify resources, uncomment the following
      # lines and adjust them as necessary.
      #   limits:
      #     cpu: 100m
      #     memory: 128Mi
      #   requests:
      #     cpu: 100m
      #     memory: 128Mi
      # Optionally specify extra list of additional labels for the scanner cronjob
      extraCronJobLabels: {}
      # Optionally specify extra list of additional labels for the jobs spawned by the scanner cronjob
      extraJobLabels: {}
      # Optionally specify extra list of additional volumes for the scanner cronjob
      extraVolumes: []
      # Optionally specify extra list of additional volume mounts for the scanner cronjob
      extraVolumeMounts: []
    # Number of snapshots to process concurrently
    concurrency: 3
    # Trivy server url. Defaults to the Trivy service's internal URL
    trivyURL: ""
    # Cache directory path. If set, the cache directory for the Trivy client will be explicitly set (otherwise defaults
    # to $HOME/.cache), and the directory will be mounted as ephemeral volume (emptyDir)
    cacheDir: ""
    # Directory path where the configuration files should be mounted
    configDir: "/home/scanner/.cfg"

  # Tracker configuration
  tracker:
    cronjob:
      image:
        # Tracker image repository (without the tag)
        repository: artifacthub/tracker
      securityContext: {}
      resources: {}
      # If you do want to specify resources, uncomment the following
      # lines and adjust them as necessary.
      #   limits:
      #     cpu: 100m
      #     memory: 128Mi
      #   requests:
      #     cpu: 100m
      #     memory: 128Mi
      # Optionally specify extra list of additional labels for the tracker cronjob
      extraCronJobLabels: {}
      # Optionally specify extra list of additional labels for the Jobs spawned by the tracker cronjob
      extraJobLabels: {}
      # Optionally specify extra list of additional volumes for the tracker cronjob
      extraVolumes: []
      # Optionally specify extra list of additional volume mounts for the tracker cronjob
      extraVolumeMounts: []
    # Cache directory path. If set, the cache directory for the Helm client will be explicitly set (otherwise defaults
    # to $HOME/.cache), and the directory will be mounted as ephemeral volume (emptyDir)
    cacheDir: ""
    # Directory path where the configuration files should be mounted
    configDir: "/home/tracker/.cfg"
    # Number of repositories to process concurrently
    concurrency: 5
    # Maximum duration for the tracking of a single repository
    repositoryTimeout: 12h
    # Repositories names to process ([] = all)
    repositoriesNames: []
    # Repositories kinds to process ([] = all)
    repositoriesKinds: []
    # Bypass digest check. Use this option to force already indexed packages to be reprocessed (use with caution)
    bypassDigestCheck: false

  # Trivy configuration
  trivy:
    enabled: false
    deploy:
      image: aquasec/trivy:0.43.1
      securityContext: {}
      resources: {}
      # If you do want to specify resources, uncomment the following
      # lines and adjust them as necessary.
      #   limits:
      #     cpu: 100m
      #     memory: 128Mi
      #   requests:
      #     cpu: 100m
      #     memory: 128Mi
      # Optionally specify extra list of additional labels for the trivy deployment
      extraDeploymentLabels: {}
      # Optionally specify extra list of additional labels for trivy pods
      extraPodLabels: {}
      # Optionally specify extra list of additional volumes for the trivy deployment
      extraVolumes: []
      # Optionally specify extra list of additional volume mounts for the trivy deployment
      extraVolumeMounts: []
    persistence:
      enabled: false
      size: 10Gi

  # Values for postgresql chart dependency
  postgresql:
    enabled: false
    image:
      repository: artifacthub/postgres
      tag: latest
    persistence:
      mountPath: /data
    postgresqlUsername: postgres
    postgresqlPassword: postgres
    postgresqlDatabase: hub
    postgresqlDataDir: /data/pgdata

  # Extra objects to deploy (value evaluated as a template)
  extraDeploy: []

gitlab:
  enabled: true

  ## The global properties are used to configure multiple charts at once.
  ## https://docs.gitlab.com/charts/charts/globals
  global:
    priorityClassName: 'system-cluster-critical'

    ## Supplemental Pod labels. Will not be used for selectors.
    pod:
      labels:
        logs: loki-myloginspace

    ## https://docs.gitlab.com/charts/installation/deployment#deploy-the-community-edition
    edition: ce

    ## https://docs.gitlab.com/charts/charts/globals#application-resource
    application:
      create: false
      links: []
      allowClusterRoles: true

    ## https://docs.gitlab.com/charts/charts/globals#configure-host-settings
    hosts:
      domain: git.writemy.codes

      https: true

    ## https://docs.gitlab.com/charts/charts/globals#configure-ingress-settings
    ingress:
      enabled: false

      configureCertmanager: false

    ## https://docs.gitlab.com/charts/charts/globals#configure-postgresql-settings
    psql:
      host: psql-int.mylogin.space
      load_balancing:
        hosts:
          - psql.mylogin.space
          - psql-int-rep.mylogin.space

      database: gl-core

      username: gl-core

      keepalives: 1
      keepalivesIdle: 5
      keepalivesInterval: 3
      keepalivesCount: 3
      tcpUserTimeout: 13000
      connectTimeout: 5
      preparedStatements: false



      password:
        useSecret: true

        secret: gitlab-user
        key: password


    ## https://docs.gitlab.com/charts/charts/globals#configure-gitaly-settings
    gitaly:
      enabled: true
      authToken: {}
        # secret:
        # key:
      # serviceName:
      internal:
        names: [default]
      external: []
      service:
        name: gitaly
        type: ClusterIP
        externalPort: 8075
        internalPort: 8075
        tls:
          externalPort: 8076
          internalPort: 8076
      tls:
        enabled: false
        # secretName:

    ## https://docs.gitlab.com/charts/charts/globals#configure-redis-settings
    redis:
      auth:
        enabled: false

      host: redis.mylogin.space
      port: 6379

    praefect:
      enabled: true

      replaceInternalGitaly: true

      authToken: {}

      autoMigrate: true



      psql:
        host: psql.mylogin.space
        port: 5432

        dbName: gl-repos

        user: gl-praefect

      dbSecret:
        secret: gl-praefect-creds
        key: password

      virtualStorages:
        - name: default
          gitalyReplicas: 3
          maxUnavailable: 1



      service:
        type: ClusterIP

        externalPort: 8075
        internalPort: 8075

        tls:
          externalPort: 8076
          internalPort: 8076

      tls:
        enabled: false
        # secretName:


    ## https://docs.gitlab.com/charts/charts/globals#configure-minio-settings
    minio:
      enabled: false

    ## https://docs.gitlab.com/charts/charts/globals#configure-appconfig-settings
    ## Rails based portions of this chart share many settings
    appConfig:
      ## https://docs.gitlab.com/charts/charts/globals#general-application-settings
      enableUsagePing: true

      enableSeatLink: true

      applicationSettingsCacheSeconds: 60

      defaultCanCreateGroup: true

      usernameChangingEnabled: true

      defaultProjectsFeatures:
        issues: true
        mergeRequests: true
        wiki: true
        snippets: true
        builds: true
        containerRegistry: false

      webhookTimeout:

      maxRequestDurationSeconds:

      ## https://docs.gitlab.com/charts/charts/globals#cron-jobs-related-settings
      cron_jobs: {}

      ## https://docs.gitlab.com/charts/charts/globals#content-security-policy
      contentSecurityPolicy:
        enabled: false
        report_only: true
        # directives: {}


      ## https://docs.gitlab.com/charts/charts/globals#lfs-artifacts-uploads-packages-external-mr-diffs-and-dependency-proxy
      object_store:
        enabled: false

        proxy_download: false

        connection:
          key: connection
          secret: gitlab-s3-prod

      lfs:
        enabled: false
        proxy_download: false
        bucket: 623579a1-8adc-0b12-dd3f-f24793c3a00f-imy3gs

      artifacts:
        enabled: false
        proxy_download: false
        bucket: 56c4bfd0-20f8-93d7-476f-a86435dde7df-w8qn40

      uploads:
        enabled: false
        proxy_download: false
        bucket: gitlab-uploads

      packages:
        enabled: false
        proxy_download: false
        bucket: gitlab-packages

      externalDiffs:
        enabled: false
        proxy_download: false
        bucket: 4781a816-7416-d012-9a2a-6dbe65624e17-ykv7qk

      terraformState:
        enabled: false
        proxy_download: false
        bucket: 29f1e5ee-1b9c-14da-cc79-6ebe911caa7b-p3e18b

      ciSecureFiles:
        enabled: false
        proxy_download: false
        bucket: gitlab-ci-secure-files

      dependencyProxy:
        enabled: false
        proxy_download: false
        bucket: b5085eee-d81c-6a15-4a33-6a32a2c325d0-9ejn31

      backups:
        bucket: gitlab-backups
        tmpBucket: tmp

      ## https://docs.gitlab.com/charts/charts/globals#ldap
      ldap:
        # prevent the use of LDAP for sign-in via web.
        preventSignin: false
        servers:
          main:
            label: Authentik LDAP

            host: ldap.mylogin.space
            port: 636

            bind_dn: cn=gl-core,ou=users,dc=ldap,dc=mylogin,dc=space

            password:
              secret: gitlab-user
              key: password

            uid: cn

            base: ou=users,dc=ldap,dc=mylogin,dc=space

            group_base: ou=groups,dc=ldap,dc=mylogin,dc=space

            active_directory: false
            attributes:
              username: cn

              first_name: name
              last_name: lastname

              name: displayName

              email: mail

            encryption: simple_tls


      ## https://docs.gitlab.com/charts/charts/globals#kas-settings
      gitlab_kas:
        enabled: true
        secret: gitlab-kas-prod
        key: kas_shared_secret

      ## https://docs.gitlab.com/charts/charts/globals#omniauth
      omniauth:
        enabled: true
        autoSignInWithProvider:
        syncProfileAttributes: [uid]
        blockAutoCreatedUsers: true
        autoLinkLdapUser: true
        autoLinkSamlUser: false
        autoLinkUser: []
        externalProviders: []
        allowBypassTwoFactor: []

        allowSingleSignOn:
          - openid_connect

        providers:
          - secret: gitlab-oidc-prod

      gitlab_docs:
        enabled: false
        host: ""


    ## https://docs.gitlab.com/charts/advanced/geo/
    geo:
      enabled: false

      registry:
        replication:
          enabled: false
          primaryApiUrl:
          ## Consumes global.registry.notificationSecret

    ## https://docs.gitlab.com/charts/charts/gitlab/kas/
    kas:
      enabled: true
      service:
        apiExternalPort: 8153 # port for connections from the GitLab backend

    ## https://docs.gitlab.com/charts/charts/globals#configure-gitlab-shell
    shell:
      authToken:
        key: shared_secret
        secret: gitlab-shell-prod

      hostKeys: {}
        # secret:
      ## https://docs.gitlab.com/charts/charts/globals#tcp-proxy-protocol
      tcp:
        proxyProtocol: false

    ## Rails application secrets
    ## Secret created according to https://docs.gitlab.com/charts/installation/secrets#gitlab-rails-secret
    ## If allowing shared-secrets generation, this is OPTIONAL.
    railsSecrets: {}
      # secret:

    ## https://docs.gitlab.com/charts/charts/globals#configure-registry-settings
    registry:
      bucket: registry

      certificate: {}
        # secret:
      httpSecret: {}
        # secret:
        # key:
      notificationSecret: {}
        # secret:
        # key:
      tls:
        enabled: false
        # secretName:
      redis:
        cache:
          password: {}
      # https://docs.docker.com/registry/notifications/#configuration
      notifications: {}
        # endpoints:
        #   - name: FooListener
        #     url: https://foolistener.com/event
        #     timeout: 500ms
        #     threshold: 10
        #     backoff: 1s
        #     headers:
        #       FooBar: ['1', '2']
        #       Authorization:
        #         secret: gitlab-registry-authorization-header
        #       SpecificPassword:
        #         secret: gitlab-registry-specific-password
        #         key: password
        # events: {}

      # Settings utilized by other services referencing registry:
      enabled: false
      host:
      # port: 443
      api:
        protocol: http
        serviceName: registry
        port: 5000
      tokenIssuer: gitlab-issuer

    pages:
      enabled: false
      accessControl: false
      https: # default true
      externalHttp: []
      externalHttps: []
      artifactsServer: true
      localStore:
        enabled: false

      objectStore:
        enabled: true
        bucket: gitlab-pages
        # proxy_download: true

        connection: {}
          # secret:
          # key:


    ## GitLab Runner
    ## Secret created according to https://docs.gitlab.com/charts/installation/secrets#gitlab-runner-secret
    ## If allowing shared-secrets generation, this is OPTIONAL.
    runner:
      registrationToken: {}
        # secret:

    ## https://docs.gitlab.com/charts/charts/globals#outgoing-email
    ## Outgoing email server settings
    smtp:
      enabled: true
      address: mail.mylogin.space
      port: 465
      tls: false
      from: gl-core@mail.mylogin.space
      user_name: gl-core
      ## https://docs.gitlab.com/charts/installation/secrets#smtp-password
      password:
        secret: gitlab-user
        key: password
      domain: mail.mylogin.space
      authentication: 'login'
      starttls_auto: false
      openssl_verify_mode: "peer"
      open_timeout: 30
      read_timeout: 60
      pool: false

    ## https://docs.gitlab.com/charts/installation/deployment#outgoing-email
    ## Email persona used in email sent by GitLab
    email:
      from: gl-core@mail.mylogin.space
      display_name: GitLab
      reply_to: august@mylogin.space
      subject_suffix: ""
      smime:
        enabled: false
        secretName: ""
        keyName: "tls.key"
        certName: "tls.crt"


    ## Timezone for containers.
    time_zone: UTC

    ## https://docs.gitlab.com/charts/charts/globals#configure-workhorse-settings
    ## Global settings related to Workhorse
    workhorse:
      ## https://docs.gitlab.com/charts/installation/secrets#gitlab-workhorse-secret
      secret: gitlab-workhorse-prod
      key: shared_secret

      tls: {}
        # enabled: true

    ## https://docs.gitlab.com/charts/charts/globals#configure-webservice
    webservice:
      workerTimeout: 60
      extraEnv:
        GITLAB_RAILS_RACK_TIMEOUT: '60'
        GITLAB_RAILS_WAIT_TIMEOUT: '90'

    ## https://docs.gitlab.com/charts/charts/globals#custom-certificate-authorities
    # configuration of certificates container & custom CA injection
    certificates:
      customCAs: []

    gitlabBase:
      image:
        repository: registry.gitlab.com/gitlab-org/build/cng/gitlab-base
        # Default tag is `global.gitlabVersion` or `master` if the former one is undefined.
        # Charts using this image as init container support further overrides with `init.image.tag`.
        # tag: master
        # pullPolicy: IfNotPresent
        # pullSecrets: []

    ## https://docs.gitlab.com/charts/charts/globals#service-accounts
    serviceAccount:
      enabled: true
      create: true

    ## https://docs.gitlab.com/charts/charts/globals/#tracing
    tracing:
      connection:
        string: opentracing://jaeger?http_endpoint=http%3A%2F%2Fdc1%2Dk3s%2Dnode1%2Dcollectors%2Dalloy%2Ecore%2Dprod%2Esvc%2Ecluster%2Elocal%3A14268%2Fapi%2Ftraces&sampler=const&sampler_param=0.1
      urlTemplate: ""

    zoekt:
      gateway:
        basicAuth: {}
      indexer:
        internalApi: {}

  upgradeCheck:
    enabled: true

  ## Installation & configuration of jetstack/cert-manager
  ## See requirements.yaml for current version
  certmanager:
    installCRDs: false
    nameOverride: certmanager
    # Install cert-manager chart. Set to false if you already have cert-manager
    # installed or if you are not using cert-manager.
    install: false


  ## https://docs.gitlab.com/charts/charts/nginx/
  ## https://docs.gitlab.com/charts/architecture/decisions#nginx-ingress
  ## Installation & configuration of charts/ingress-nginx:
  nginx-ingress:
    enabled: false


  ## Installation & configuration of stable/prometheus
  ## See requirements.yaml for current version
  prometheus:
    install: false
    rbac:
      create: true
    alertmanager:
      enabled: false
    alertmanagerFiles:
      alertmanager.yml: {}
    kubeStateMetrics:
      enabled: false
    nodeExporter:
      enabled: false
    pushgateway:
      enabled: false
    server:
      retention: 15d
      strategy:
        type: Recreate

  ## Configuration of Redis
  ## https://docs.gitlab.com/charts/architecture/decisions#redis
  ## https://docs.gitlab.com/charts/installation/deployment.html#redis
  redis:
    install: false
    existingSecret: gitlab-redis-secret
    existingSecretKey: redis-password
    usePasswordFile: true
    cluster:
      enabled: false
    metrics:
      enabled: true

  ## Installation & configuration of stable/prostgresql
  ## See requirements.yaml for current version
  postgresql:
    install: false
    postgresqlDatabase: gitlabhq_production
    usePasswordFile: true
    existingSecret: bogus

    metrics:
      enabled: true

  ## Installation & configuration charts/registry
  ## https://docs.gitlab.com/charts/architecture/decisions#registry
  ## https://docs.gitlab.com/charts/charts/registry/
  registry:
    enabled: false

  ## Automatic shared secret generation
  ## https://docs.gitlab.com/charts/installation/secrets
  ## https://docs.gitlab.com/charts/charts/shared-secrets.html
  shared-secrets:
    env: production

    resources:
      requests:
        cpu: 50m

    securityContext:
      # in debian/alpine based images, this is `nobody:nogroup`
      runAsUser: 65534
      fsGroup: 65534
    tolerations: []
    podLabels: {}
    annotations: {}

  ## Installation & configuration of gitlab/gitlab-runner
  ## See requirements.yaml for current version
  gitlab-runner:
    install: false

    rbac:
      create: true

    runners:
      locked: false

      cache:
        secretName: gitlab-s3-runner

  ## Settings for individual sub-charts under GitLab
  ## Note: Many of these settings are configurable via globals
  gitlab:
    migrations:
      annotations:
        "helm.sh/hook": "pre-install"
        "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"

    webservice:
      workerProcesses: 2
      replicaCount: 1
      puma:
        threads:
          min: 2
          max: 2

      resources:
        requests:
          cpu: '1'
          memory: '4G'

    praefect:
      replicas: 1

    exporter:
      enabled: false
      metrics:
        enabled: true
        serviceMonitor:
          enabled: true

    sidekiq:
      concurrency: 10
      resources:
        requests:
          cpu: 48m
          memory: 1239M
        limits:
          cpu: '1'
          memory: 2G

    gitlab-shell:
      resources:
        requests:
          cpu: 162m
          memory: 105M
        limits:
          cpu: 261m
          memory: 105M


    toolbox:
      enabled: true
      replicas: 1
      backups:
        cron:
          enabled: false
          concurrencyPolicy: Replace
          failedJobsHistoryLimit: 1
          schedule: "0 1 * * *"
          successfulJobsHistoryLimit: 3
          suspend: false
          backoffLimit: 6
          restartPolicy: "OnFailure"
          extraArgs: '--maximum-backups=15'

          resources:
            requests:
              cpu: 50m
              memory: 350M

          persistence:
            enabled: false

        objectStorage:
          backend: s3
          config:
            secret: gitlab-s3-backups-prod
            key: config

harbor:
  enabled: false

  expose:
    # Set how to expose the service. Set the type as "ingress", "clusterIP", "nodePort" or "loadBalancer"
    # and fill the information in the corresponding section
    type: clusterIP

    tls:
      # Enable TLS or not.
      # Delete the "ssl-redirect" annotations in "expose.ingress.annotations" when TLS is disabled and "expose.type" is "ingress"
      # Note: if the "expose.type" is "ingress" and TLS is disabled,
      # the port must be included in the command when pulling/pushing images.
      # Refer to https://github.com/goharbor/harbor/issues/5291 for details.
      enabled: true

      certSource: secret

      secret:
        # The name of secret which contains keys named:
        # "tls.crt" - the certificate
        # "tls.key" - the private key
        secretName: writemycodes-default-certificates
        # The name of secret which contains keys named:
        # "tls.crt" - the certificate
        # "tls.key" - the private key
        # Only needed when the "expose.type" is "ingress".
        notarySecretName: writemycodes-default-certificates

    ingress:
      hosts:
        core: registry.writemy.codes
        notary: notary.writemy.codes

      # set to the type of ingress controller if it has specific requirements.
      # leave as `default` for most ingress controllers.
      # set to `gce` if using the GCE ingress controller
      # set to `ncp` if using the NCP (NSX-T Container Plugin) ingress controller
      controller: default

      className: ''

      annotations:
        # note different ingress controllers may require a different ssl-redirect annotation
        # for Envoy, use ingress.kubernetes.io/force-ssl-redirect: "true" and remove the nginx lines below
        ingress.kubernetes.io/ssl-redirect: "true"
        ingress.kubernetes.io/proxy-body-size: "0"
        nginx.ingress.kubernetes.io/ssl-redirect: "true"
        nginx.ingress.kubernetes.io/proxy-body-size: "0"

      notary:
        # notary ingress-specific annotations
        annotations: {}
        # notary ingress-specific labels
        labels: {}

      harbor:
        # harbor ingress-specific annotations
        annotations: {}
        # harbor ingress-specific labels
        labels: {}

    clusterIP:
      # The name of ClusterIP service
      name: harbor
      # Annotations on the ClusterIP service
      annotations: {}
      ports:
        # The service port Harbor listens on when serving HTTP
        httpPort: 80
        # The service port Harbor listens on when serving HTTPS
        httpsPort: 443
        # The service port Notary listens on. Only needed when notary.enabled
        # is set to true
        notaryPort: 4443

    nodePort:
      # The name of NodePort service
      name: harbor
      ports:
        http:
          # The service port Harbor listens on when serving HTTP
          port: 80
          # The node port Harbor listens on when serving HTTP
          nodePort: 30002
        https:
          # The service port Harbor listens on when serving HTTPS
          port: 443
          # The node port Harbor listens on when serving HTTPS
          nodePort: 30003
        # Only needed when notary.enabled is set to true
        notary:
          # The service port Notary listens on
          port: 4443
          # The node port Notary listens on
          nodePort: 30004

    loadBalancer:
      # The name of LoadBalancer service
      name: harbor
      # Set the IP if the LoadBalancer supports assigning IP
      IP: ""
      ports:
        # The service port Harbor listens on when serving HTTP
        httpPort: 80
        # The service port Harbor listens on when serving HTTPS
        httpsPort: 443
        # The service port Notary listens on. Only needed when notary.enabled
        # is set to true
        notaryPort: 4443
      annotations: {}
      sourceRanges: []

  # The external URL for Harbor core service. It is used to
  # 1) populate the docker/helm commands showed on portal
  # 2) populate the token service URL returned to docker/notary client
  #
  # Format: protocol://domain[:port]. Usually:
  # 1) if "expose.type" is "ingress", the "domain" should be
  # the value of "expose.ingress.hosts.core"
  # 2) if "expose.type" is "clusterIP", the "domain" should be
  # the value of "expose.clusterIP.name"
  # 3) if "expose.type" is "nodePort", the "domain" should be
  # the IP address of k8s node
  #
  # If Harbor is deployed behind the proxy, set it as the URL of proxy
  externalURL: https://registry.writemy.codes

  # The internal TLS used for harbor components secure communicating. In order to enable https
  # in each components tls cert files need to provided in advance.
  internalTLS:
    # If internal TLS enabled
    enabled: false

  ipFamily:
    # ipv6Enabled set to true if ipv6 is enabled in cluster, currently it affected the nginx related component
    ipv6:
      enabled: true
    # ipv4Enabled set to true if ipv4 is enabled in cluster, currently it affected the nginx related component
    ipv4:
      enabled: true

  # The persistence is enabled by default and a default StorageClass
  # is needed in the k8s cluster to provision volumes dynamically.
  # Specify another StorageClass in the "storageClass" or set "existingClaim"
  # if you already have existing persistent volumes to use
  #
  # For storing images and charts, you can also use "azure", "gcs", "s3",
  # "swift" or "oss". Set it in the "imageChartStorage" section
  persistence:
    enabled: false

    # Define which storage backend is used for registry and chartmuseum to store
    # images and charts. Refer to
    # https://github.com/docker/distribution/blob/master/docs/configuration.md#storage
    # for the detail.
    imageChartStorage:
      # Specify whether to disable `redirect` for images and chart storage, for
      # backends which not supported it (such as using minio for `s3` storage type), please disable
      # it. To disable redirects, simply set `disableredirect` to `true` instead.
      # Refer to
      # https://github.com/docker/distribution/blob/master/docs/configuration.md#redirect
      # for the detail.
      disableredirect: false

      # Specify the "caBundleSecretName" if the storage service uses a self-signed certificate.
      # The secret must contain keys named "ca.crt" which will be injected into the trust store
      # of registry's and chartmuseum's containers.
      # caBundleSecretName:

      # Specify the type of storage: "filesystem", "azure", "gcs", "s3", "swift",
      # "oss" and fill the information needed in the corresponding section. The type
      # must be "filesystem" if you want to use persistent volumes for registry
      # and chartmuseum
      type: s3

      filesystem:
        rootdirectory: /storage
        #maxthreads: 100

      s3:
        region: us-east-1 
        bucket: <path:CORE0_SITE1/data/Harbor#S3/Bucket>
        accesskey: <path:CORE0_SITE1/data/Harbor#S3/AccessKey>
        secretkey: <path:CORE0_SITE1/data/Harbor#S3/SecretKey>
        regionendpoint: https://s3.mylogin.space

  imagePullPolicy: IfNotPresent

  # Use this set to assign a list of default pullSecrets
  imagePullSecrets:
  #  - name: docker-registry-secret
  #  - name: internal-registry-secret

  # The update strategy for deployments with persistent volumes(jobservice, registry
  # and chartmuseum): "RollingUpdate" or "Recreate"
  # Set it as "Recreate" when "RWM" for volumes isn't supported
  updateStrategy:
    type: RollingUpdate

  # debug, info, warning, error or fatal
  logLevel: warning

  # The initial password of Harbor admin. Change it from portal after launching Harbor
  harborAdminPassword: "Harbor12345"

  # The name of the secret which contains key named "ca.crt". Setting this enables the
  # download link on portal to download the CA certificate when the certificate isn't
  # generated automatically
  caSecretName: ""

  # The secret key used for encryption. Must be a string of 16 chars.
  secretKey: <path:CORE0_SITE1/data/Harbor#SecretKey>

  # The proxy settings for updating trivy vulnerabilities from the Internet and replicating
  # artifacts from/to the registries that cannot be reached directly
  proxy:
    httpProxy:
    httpsProxy:
    noProxy: 127.0.0.1,localhost,.local,.internal
    components:
      - core
      - jobservice
      - trivy

  # Run the migration job via helm hook
  enableMigrateHelmHook: true

  # The custom ca bundle secret, the secret must contain key named "ca.crt"
  # which will be injected into the trust store for chartmuseum, core, jobservice, registry, trivy components
  # caBundleSecretName: ""

  ## UAA Authentication Options
  # If you're using UAA for authentication behind a self-signed
  # certificate you will need to provide the CA Cert.
  # Set uaaSecretName below to provide a pre-created secret that
  # contains a base64 encoded CA Certificate named `ca.crt`.
  # uaaSecretName:

  # If service exposed via "ingress", the Nginx will not be used
  nginx:
    image:
      repository: goharbor/nginx-photon

    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    replicas: 1

    revisionHistoryLimit: 10

    # resources:
    #  requests:
    #    memory: 256Mi
    #    cpu: 100m

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    ## The priority class to run the pod as
    priorityClassName:

  portal:
    image:
      repository: goharbor/harbor-portal

    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    replicas: 2

    revisionHistoryLimit: 10

    # resources:
    #  requests:
    #    memory: 256Mi
    #    cpu: 100m

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    ## The priority class to run the pod as
    priorityClassName:

  core:
    image:
      repository: goharbor/harbor-core

    # set the service account to be used, default if left empty
    serviceAccountName: ""
    # mount the service account token

    automountServiceAccountToken: false

    replicas: 3

    revisionHistoryLimit: 10

    ## Startup probe values
    startupProbe:
      enabled: true
      initialDelaySeconds: 10

    # resources:
    #  requests:
    #    memory: 256Mi
    #    cpu: 100m

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    # Secret is used when core server communicates with other components.
    # If a secret key is not specified, Helm will generate one.
    # Must be a string of 16 chars.
    secret: <path:CORE0_SITE1/data/Harbor#CoreSecret>

    # Fill the name of a kubernetes secret if you want to use your own
    # TLS certificate and private key for token encryption/decryption.
    # The secret must contain keys named:
    # "tls.crt" - the certificate
    # "tls.key" - the private key
    # The default key pair will be used if it isn't set
    secretName: ''

    # The XSRF key. Will be generated automatically if it isn't specified
    xsrfKey: <path:CORE0_SITE1/data/Harbor#CSRFKey>

    ## The priority class to run the pod as
    priorityClassName:

    # The time duration for async update artifact pull_time and repository
    # pull_count, the unit is second. Will be 10 seconds if it isn't set.
    # eg. artifactPullAsyncFlushDuration: 10
    artifactPullAsyncFlushDuration:

  jobservice:
    image:
      repository: goharbor/harbor-jobservice

    replicas: 2

    revisionHistoryLimit: 10

    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    maxJobWorkers: 10

    # The logger for jobs: "file", "database" or "stdout"
    jobLoggers:
      #- file
      # - database
      - stdout

    # The jobLogger sweeper duration (ignored if `jobLogger` is `stdout`)
    loggerSweeperDuration: 14 #days

    # resources:
    #   requests:
    #     memory: 256Mi
    #     cpu: 100m

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    # Secret is used when job service communicates with other components.
    # If a secret key is not specified, Helm will generate one.
    # Must be a string of 16 chars.
    secret: <path:CORE0_SITE1/data/Harbor#JobService/Secret>

    ## The priority class to run the pod as
    priorityClassName:

  registry:
    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false
    registry:
      image:
        repository: goharbor/registry-photon

      # resources:
      #  requests:
      #    memory: 256Mi
      #    cpu: 100m

    controller:
      image:
        repository: goharbor/harbor-registryctl

      # resources:
      #  requests:
      #    memory: 256Mi
      #    cpu: 100m

    replicas: 3

    revisionHistoryLimit: 10

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    ## The priority class to run the pod as
    priorityClassName:

    # Secret is used to secure the upload state from client
    # and registry storage backend.
    # See: https://github.com/docker/distribution/blob/master/docs/configuration.md#http
    # If a secret key is not specified, Helm will generate one.
    # Must be a string of 16 chars.
    secret: <path:CORE0_SITE1/data/Harbor#Registry/Secret>

    # If true, the registry returns relative URLs in Location headers. The client is responsible for resolving the correct URL.
    relativeurls: false

    credentials:
      username: 'harbor_registry_user'
      password: 'harbor_registry_password'
      # Login and password in htpasswd string format. Excludes `registry.credentials.username`  and `registry.credentials.password`. May come in handy when integrating with tools like argocd or flux. This allows the same line to be generated each time the template is rendered, instead of the `htpasswd` function from helm, which generates different lines each time because of the salt.
      htpasswdString: harbor_registry_user:$2y$10$L4LTCe2fG2H42qquClN5.e6L4PnL68.LaRAVfxPW37HAKm2vX9iX.

    middleware:
      enabled: false
      type: cloudFront
      cloudFront:
        baseurl: example.cloudfront.net
        keypairid: KEYPAIRID
        duration: 3000s
        ipfilteredby: none
        # The secret key that should be present is CLOUDFRONT_KEY_DATA, which should be the encoded private key
        # that allows access to CloudFront
        privateKeySecret: "my-secret"

    # enable purge _upload directories
    upload_purging:
      enabled: true
      # remove files in _upload directories which exist for a period of time, default is one week.
      age: 168h
      # the interval of the purge operations
      interval: 24h
      dryrun: false

  chartmuseum:
    enabled: true

    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    # Harbor defaults ChartMuseum to returning relative urls, if you want using absolute url you should enable it by change the following value to 'true'
    absoluteUrl: false

    image:
      repository: goharbor/chartmuseum-photon

    replicas: 1

    revisionHistoryLimit: 10

    # resources:
    #  requests:
    #    memory: 256Mi
    #    cpu: 100m

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    ## The priority class to run the pod as
    priorityClassName:

    ## limit the number of parallel indexers
    indexLimit: 0

  trivy:
    # enabled the flag to enable Trivy scanner
    enabled: false

    image:
      # repository the repository for Trivy adapter image
      repository: goharbor/trivy-adapter-photon
      # tag the tag for Trivy adapter image

    # set the service account to be used, default if left empty
    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    # replicas the number of Pod replicas
    replicas: 1

    # debugMode the flag to enable Trivy debug mode with more verbose scanning log
    debugMode: false

    # vulnType a comma-separated list of vulnerability types. Possible values are `os` and `library`.
    vulnType: "os,library"

    # severity a comma-separated list of severities to be checked
    severity: "UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL"

    # ignoreUnfixed the flag to display only fixed vulnerabilities
    ignoreUnfixed: false

    # insecure the flag to skip verifying registry certificate
    insecure: false

    # gitHubToken the GitHub access token to download Trivy DB
    #
    # Trivy DB contains vulnerability information from NVD, Red Hat, and many other upstream vulnerability databases.
    # It is downloaded by Trivy from the GitHub release page https://github.com/aquasecurity/trivy-db/releases and cached
    # in the local file system (`/home/scanner/.cache/trivy/db/trivy.db`). In addition, the database contains the update
    # timestamp so Trivy can detect whether it should download a newer version from the Internet or use the cached one.
    # Currently, the database is updated every 12 hours and published as a new release to GitHub.
    #
    # Anonymous downloads from GitHub are subject to the limit of 60 requests per hour. Normally such rate limit is enough
    # for production operations. If, for any reason, it's not enough, you could increase the rate limit to 5000
    # requests per hour by specifying the GitHub access token. For more details on GitHub rate limiting please consult
    # https://developer.github.com/v3/#rate-limiting
    #
    # You can create a GitHub token by following the instructions in
    # https://help.github.com/en/github/authenticating-to-github/creating-a-personal-access-token-for-the-command-line
    gitHubToken: ""


    # skipUpdate the flag to disable Trivy DB downloads from GitHub
    #
    # You might want to set the value of this flag to `true` in test or CI/CD environments to avoid GitHub rate limiting issues.
    # If the value is set to `true` you have to manually download the `trivy.db` file and mount it in the
    # `/home/scanner/.cache/trivy/db/trivy.db` path.
    skipUpdate: false


    # The offlineScan option prevents Trivy from sending API requests to identify dependencies.
    #
    # Scanning JAR files and pom.xml may require Internet access for better detection, but this option tries to avoid it.
    # For example, the offline mode will not try to resolve transitive dependencies in pom.xml when the dependency doesn't
    # exist in the local repositories. It means a number of detected vulnerabilities might be fewer in offline mode.
    # It would work if all the dependencies are in local.
    # This option doesnâ€™t affect DB download. You need to specify skipUpdate as well as offlineScan in an air-gapped environment.
    offlineScan: false
    # The duration to wait for scan completion

    timeout: 5m0s

    resources:
      requests:
        cpu: 200m
        memory: 512Mi

      limits:
        cpu: 1
        memory: 1Gi

    nodeSelector: {}

    tolerations: []

    affinity: {}

    ## Additional deployment annotations
    podAnnotations: {}

    ## The priority class to run the pod as
    priorityClassName:

  notary:
    enabled: false

    server:
      # set the service account to be used, default if left empty
      serviceAccountName: ""

      # mount the service account token
      automountServiceAccountToken: false

      image:
        repository: goharbor/notary-server-photon

      replicas: 1

      # resources:
      #  requests:
      #    memory: 256Mi
      #    cpu: 100m

      nodeSelector: {}

      tolerations: []

      affinity: {}

      ## Additional deployment annotations
      podAnnotations: {}

      ## The priority class to run the pod as
      priorityClassName:

    signer:
      # set the service account to be used, default if left empty
      serviceAccountName: ""

      # mount the service account token
      automountServiceAccountToken: false

      image:
        repository: goharbor/notary-signer-photon

      replicas: 1

      # resources:
      #  requests:
      #    memory: 256Mi
      #    cpu: 100m

      nodeSelector: {}

      tolerations: []

      affinity: {}

      ## Additional deployment annotations
      podAnnotations: {}

      ## The priority class to run the pod as
      priorityClassName:

    # Fill the name of a kubernetes secret if you want to use your own
    # TLS certificate authority, certificate and private key for notary
    # communications.
    # The secret must contain keys named ca.crt, tls.crt and tls.key that
    # contain the CA, certificate and private key.
    # They will be generated if not set.
    secretName: ""

  database:
    # if external database is used, set "type" to "external"
    # and fill the connection informations in "external" section
    type: external

    external:
      host: psql.mylogin.space

      port: '5432'
      
      username: <path:CORE0_SITE1/data/Harbor#Username>
      password: <path:CORE0_SITE1/data/Harbor#Password>
      
      coreDatabase: <path:CORE0_SITE1/data/Harbor#Database>
      
      notaryServerDatabase: "notary_server"
      
      notarySignerDatabase: "notary_signer"
      
      # "disable" - No SSL
      # "require" - Always SSL (skip verification)
      # "verify-ca" - Always SSL (verify that the certificate presented by the
      # server was signed by a trusted CA)
      # "verify-full" - Always SSL (verify that the certification presented by the
      # server was signed by a trusted CA and the server host name matches the one
      # in the certificate)
      sslmode: prefer
    
    # The maximum number of connections in the idle connection pool per pod (core+exporter).
    # If it <=0, no idle connections are retained.
    maxIdleConns: 100
    
    # The maximum number of open connections to the database per pod (core+exporter).
    # If it <= 0, then there is no limit on the number of open connections.
    # Note: the default number of connections is 1024 for postgre of harbor.
    maxOpenConns: 900

    ## Additional deployment annotations
    podAnnotations: {}

  redis:
    # if external Redis is used, set "type" to "external"
    # and fill the connection informations in "external" section
    type: external

    external:
      # support redis, redis+sentinel
      # addr for redis: <host_redis>:<port_redis>
      # addr for redis+sentinel: <host_sentinel1>:<port_sentinel1>,<host_sentinel2>:<port_sentinel2>,<host_sentinel3>:<port_sentinel3>
      addr: redis.mylogin.space:6379
      # The name of the set of Redis instances to monitor, it must be set to support redis+sentinel
      sentinelMasterSet: ""

      # The "coreDatabaseIndex" must be "0" as the library Harbor
      # used doesn't support configuring it
      coreDatabaseIndex: '0'
      jobserviceDatabaseIndex: '7'
      registryDatabaseIndex: '8'
      chartmuseumDatabaseIndex: '9'
      trivyAdapterIndex: '14'

      password: ""

    ## Additional deployment annotations
    podAnnotations: {}

  exporter:
    replicas: 1

    revisionHistoryLimit: 10

  # resources:
  #  requests:
  #    memory: 256Mi
  #    cpu: 100m

    podAnnotations: {}

    serviceAccountName: ""

    # mount the service account token
    automountServiceAccountToken: false

    image:
      repository: goharbor/harbor-exporter

    nodeSelector: {}

    tolerations: []

    affinity: {}

    cacheDuration: 23

    cacheCleanInterval: 14400

    ## The priority class to run the pod as
    priorityClassName:

  metrics:
    enabled: false

    core:
      path: /metrics
      port: 8001

    registry:
      path: /metrics
      port: 8001

    jobservice:
      path: /metrics
      port: 8001

    exporter:
      path: /metrics
      port: 8001

    ## Create prometheus serviceMonitor to scrape harbor metrics.
    ## This requires the monitoring.coreos.com/v1 CRD. Please see
    ## https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/user-guides/getting-started.md
    ##
    serviceMonitor:
      enabled: false
      additionalLabels: {}
      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      interval: ""
      # Metric relabel configs to apply to samples before ingestion.
      metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]
      # Relabel configs to apply to samples before ingestion.
      relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

  trace:
    enabled: false
    # trace provider: jaeger or otel
    # jaeger should be 1.26+
    provider: otel
    # set sample_rate to 1 if you wanna sampling 100% of trace data; set 0.5 if you wanna sampling 50% of trace data, and so forth
    sample_rate: 0.025
    # namespace used to differentiate different harbor services
    # namespace:
    # attributes is a key value dict contains user defined attributes used to initialize trace provider
    # attributes:
    #   application: harbor

    otel:
      endpoint: hostname:4318
      url_path: /v1/traces
      compression: true
      insecure: true
      timeout: 10s

renovate:
  enabled: false
  global:
    # -- Additional labels to be set on all renovate resources
    commonLabels: {}

  # -- Override the name of the chart
  nameOverride: ''
  # -- Override the fully qualified app name
  fullnameOverride: ''

  cronjob:
    # -- Schedules the job to run using cron notation
    schedule: '0 1 * * *'  # At 01:00 every day
    # -- You can specify a time zone for a CronJob by setting timeZone to the name of a valid time zone. (starting with k8s 1.27) <https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/#time-zones>
    timeZone: ''  # see https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for valid names
    # -- If it is set to true, all subsequent executions are suspended. This setting does not apply to already started executions.
    suspend: false
    # -- Annotations to set on the cronjob
    annotations: {}
    # -- Labels to set on the cronjob
    labels: {}
    # -- "Allow" to allow concurrent runs, "Forbid" to skip new runs if a previous run is still running or "Replace" to replace the previous run
    concurrencyPolicy: ''
    # -- Amount of failed jobs to keep in history
    failedJobsHistoryLimit: ''
    # -- Amount of completed jobs to keep in history
    successfulJobsHistoryLimit: ''
    # -- Set to Never to restart the job when the pod fails or to OnFailure to restart when a container fails
    jobRestartPolicy: Never
    # -- Time to keep the job after it finished before automatically deleting it
    ttlSecondsAfterFinished: ''
    # -- Deadline for the job to finish
    activeDeadlineSeconds: ''
    # -- Number of times to retry running the pod before considering the job as being failed
    jobBackoffLimit: ''
    # -- Deadline to start the job, skips execution if job misses it's configured deadline
    startingDeadlineSeconds: ''
    # -- Additional initContainers that can be executed before renovate
    initContainers: []
    # initContainers:
    # - name: INIT_CONTAINER_NAME
    #   image: INIT_CONTAINER_IMAGE

    # -- Prepend shell commands before renovate runs
    preCommand: ''
    # preCommand: |
    #   echo hello
    #   echo world

    # -- Append shell commands after renovate runs
    postCommand: ''
    # postCommand: |
    #   echo hello
    #   echo world

  pod:
    # -- Annotations to set on the pod
    annotations: {}
    # -- Labels to set on the pod
    labels: {}

  image:
    # -- Registry to pull image from
    registry: ghcr.io
    # -- Image name to pull
    repository: renovatebot/renovate

    # -- "IfNotPresent" to pull the image if no image with the specified tag exists on the node, "Always" to always pull the image or "Never" to try and use pre-pulled images
    pullPolicy: IfNotPresent

  # -- Secret to use to pull the image from the repository
  imagePullSecrets: {}

  renovate:
    # -- Custom exiting global renovate config
    existingConfigFile: renovate-config
    # -- Inline global renovate config.json
    config: 'test'
    # See https://docs.renovatebot.com/self-hosted-configuration
    # config: |
    #   {
    #     "platform": "gitlab",
    #     "endpoint": "https://gitlab.example.com/api/v4",
    #     "token": "your-gitlab-renovate-user-token",
    #     "autodiscover": "false",
    #     "dryRun": true,
    #     "printConfig": true,
    #     "repositories": ["username/repo", "orgname/repo"]
    #   }

    # -- Use the Helm tpl function on your configuration. See README for how to use this value
    configEnableHelmTpl: false

    # -- Use this to create the renovate-config as a secret instead of a configmap
    configIsSecret: true

    # -- Renovate Container-level security-context
    securityContext: {}

    # -- Options related to persistence
    persistence:
      cache:
        # -- Allow the cache to persist between runs
        enabled: false
        # -- Storage class of the cache PVC
        storageClass: ""
        # -- Storage size of the cache PVC
        storageSize: "512Mi"

  ssh_config:
    # -- Whether to enable the use and creation of a secret containing .ssh files
    enabled: false

    # Provide .ssh config file contents
    # -- Contents of the id_rsa file
    id_rsa: ''
    # -- Contents of the id_rsa_pub file
    id_rsa_pub: ''
    # -- Contents of the config file
    config: ''

    # or provide the name of an existing secret to be read instead.
    # -- Name of the existing secret containing a valid .ssh configuration
    existingSecret: ''

  # -- Environment variables that should be referenced from a k8s secret, cannot be used when existingSecret is set
  secrets: {}
  # -- k8s secret to reference environment variables from. Overrides secrets if set
  existingSecret: ''

  # -- Additional configmaps. A generated configMap name is: "renovate.fullname" + "extra" + name(below) e.g. renovate-netrc-config
  extraConfigmaps: []
  # extraConfigmaps:
  #   - name: netrc-config
  #     data:
  #       .netrc: |-
  #         machine gitlab.example.com
  #         login gitlab-ci-token
  #         password some-pass
  #   - name: yet-another-config
  #     data:
  #       ya-config.yaml: |-
  #         "key"="value"
  #         "key1"="value1"

  # -- Additional volumes to the pod
  extraVolumes: []
  # extraVolumes:
  #   - name: netrc-config
  #     configMap:
  #       name: renovate-extra-netrc-config

  # -- Additional volumeMounts to the container
  extraVolumeMounts: []
  # extraVolumeMounts:
  #   - name: netrc-config
  #     mountPath: /home/ubuntu/.netrc
  #     subPath: .netrc

  # -- Additional containers to the pod
  extraContainers: []
  # extraContainers:
  #   - name: vault-agent
  #     image: vault:1.6.2
  #     args:
  #     - agent
  #     - -config
  #     - /vault/config/config.hcl
  #     env:
  #     - name: VAULT_ADDR
  #       value: https://vault:8200
  #     - name: VAULT_SKIP_VERIFY
  #       value: "false"
  #     - name: VAULT_CACERT
  #       value: /vault/tls/ca.crt

  serviceAccount:
    # -- Specifies whether a service account should be created
    create: false
    # -- Annotations to add to the service account
    annotations: {}
    # -- The name of the service account to use
    # If not set and create is true, a name is generated using the fullname template
    name: ''

  # -- Specify resource limits and requests for the renovate container
  resources: {}
    # We usually recommend not to specify default resources and to leave this as a conscious
    # choice for the user. This also increases chances charts run on environments with little
    # resources, such as Minikube. If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #  cpu: 100m
    #  memory: 128Mi
    # requests:
    #  cpu: 100m
    #  memory: 128Mi

  # -- Environment variables to add from existing secrets/configmaps. Uses the keys as variable name
  envFrom: []
  # envFrom:
  #   - secretRef:
  #       name: env-secrets
  #   - configMapRef:
  #       name: env-configmap

  # -- Environment variables to set on the renovate container
  env: {}
  # env:
  #   VARIABLE_NAME: "value"

  # -- Additional env. Helpful too if you want to use anything other than a `value` source.
  envList: []
  # envList:
  #   - name: POD_NAME
  #     valueFrom:
  #       fieldRef:
  #         fieldPath: metadata.name

  redis:
    # Configuration for a Redis subchart. Additional documentation at
    # https://github.com/bitnami/charts/tree/master/bitnami/redis

    # -- Enable the Redis subchart?
    enabled: false

    # -- Override the prefix of the redisHost
    nameOverride: ''

    # -- Disable replication by default
    architecture: standalone

    auth:
      # -- Don't require a password by default
      enabled: false

    # -- Override Kubernetes version for redis chart
    kubeVersion: ''

  # Override APIVersions
  # If you want to template helm charts but cannot access k8s API server
  # you can set api versions here
  apiVersionOverrides:
    # -- String to override apiVersion of cronjob rendered by this helm chart
    cronjob: ''

  # -- Override hostname resolution
  hostAliases: []
  # See: https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
  # hostAliases:
  #   - ip: "your-ip"
  #     hostnames:
  #       - "your-hostname"

  # -- Pod-level security-context
  securityContext: {}

  # -- Select the node using labels to specify where the cronjob pod should run on
  nodeSelector: {}
  # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/
  # renovate: true

  # -- Configure the pod(Anti)Affinity and/or node(Anti)Affinity
  affinity: {}
  # See: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/

  # -- Configure which node taints the pod should tolerate
  tolerations: []
  # See: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/


che:
  enabled: true