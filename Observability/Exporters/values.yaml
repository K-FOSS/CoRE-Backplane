kube-prometheus-stack:
  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: false


  ## Create default rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: true
      etcd: true
      configReloaders: true
      general: true
      k8sContainerCpuUsageSecondsTotal: true
      k8sContainerMemoryCache: true
      k8sContainerMemoryRss: true
      k8sContainerMemorySwap: true
      k8sContainerResource: true
      k8sContainerMemoryWorkingSetBytes: true
      k8sPodOwner: true
      kubeApiserverAvailability: true
      kubeApiserverBurnrate: true
      kubeApiserverHistogram: true
      kubeApiserverSlos: true
      kubeControllerManager: true
      kubelet: true
      kubeProxy: true
      kubePrometheusGeneral: true
      kubePrometheusNodeRecording: true
      kubernetesApps: true
      kubernetesResources: true
      kubernetesStorage: true
      kubernetesSystem: true
      kubeSchedulerAlerting: true
      kubeSchedulerRecording: true
      kubeStateMetrics: true
      network: true
      node: true
      nodeExporterAlerting: true
      nodeExporterRecording: true
      prometheus: true
      prometheusOperator: true
      windows: true

    ## Reduce app namespace alert scope
    appNamespacesTarget: ".*"

    ## Additional labels for specific PrometheusRule alert groups
    additionalRuleGroupLabels:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}

    ## Additional annotations for specific PrometheusRule alerts groups
    additionalRuleGroupAnnotations:
      alertmanager: {}
      etcd: {}
      configReloaders: {}
      general: {}
      k8sContainerCpuUsageSecondsTotal: {}
      k8sContainerMemoryCache: {}
      k8sContainerMemoryRss: {}
      k8sContainerMemorySwap: {}
      k8sContainerResource: {}
      k8sPodOwner: {}
      kubeApiserverAvailability: {}
      kubeApiserverBurnrate: {}
      kubeApiserverHistogram: {}
      kubeApiserverSlos: {}
      kubeControllerManager: {}
      kubelet: {}
      kubeProxy: {}
      kubePrometheusGeneral: {}
      kubePrometheusNodeRecording: {}
      kubernetesApps: {}
      kubernetesResources: {}
      kubernetesStorage: {}
      kubernetesSystem: {}
      kubeSchedulerAlerting: {}
      kubeSchedulerRecording: {}
      kubeStateMetrics: {}
      network: {}
      node: {}
      nodeExporterAlerting: {}
      nodeExporterRecording: {}
      prometheus: {}
      prometheusOperator: {}

    additionalAggregationLabels: []

    ## Prefix for runbook URLs. Use this to override the first part of the runbookURLs that is common to all rules.
    runbookUrl: "https://runbooks.prometheus-operator.dev/runbooks"

    node:
      fsSelector: 'fstype!=""'
      # fsSelector: 'fstype=~"ext[234]|btrfs|xfs|zfs"'

    ## Disabled PrometheusRule alerts
    disabled: {}
    # KubeAPIDown: true
    # NodeRAIDDegraded: true

  ##
  global:
    rbac:
      create: true

      ## Create ClusterRoles that extend the existing view, edit and admin ClusterRoles to interact with prometheus-operator CRDs
      ## Ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles
      createAggregateClusterRoles: false
      pspEnabled: false
      pspAnnotations: {}
        ## Specify pod annotations
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#apparmor
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#seccomp
        ## Ref: https://kubernetes.io/docs/concepts/policy/pod-security-policy/#sysctl
        ##
        # seccomp.security.alpha.kubernetes.io/allowedProfileNames: '*'
        # seccomp.security.alpha.kubernetes.io/defaultProfileName: 'docker/default'
        # apparmor.security.beta.kubernetes.io/defaultProfileName: 'runtime/default'

    ## Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
    ##
    imageRegistry: ""

    ## Reference to one or more secrets to be used when pulling images
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
    ##
    imagePullSecrets: []
    # - name: "image-pull-secret"
    # or
    # - "image-pull-secret"

  windowsMonitoring:
    ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
    enabled: false

  ## Configuration for prometheus-windows-exporter
  ## ref: https://github.com/prometheus-community/helm-charts/tree/main/charts/prometheus-windows-exporter
  ##
  prometheus-windows-exporter:
    ## Enable ServiceMonitor and set Kubernetes label to use as a job label
    ##
    prometheus:
      monitor:
        enabled: true
        jobLabel: jobLabel

    releaseLabel: true

    ## Set job label to 'windows-exporter' as required by the default Prometheus rules and Grafana dashboards
    ##
    podLabels:
      jobLabel: windows-exporter

    ## Enable memory and container metrics as required by the default Prometheus rules and Grafana dashboards
    ##
    config: |-
      collectors:
        enabled: '[defaults],memory,container'

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: false

    ## Configuration for creating an Ingress that will map to each Alertmanager replica service
    ## alertmanager.servicePerReplica must be enabled
    ##
    ingressPerReplica:
      enabled: false

      # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
      # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
      # ingressClassName: nginx

      annotations: {}
      labels: {}

      ## Final form of the hostname for each per replica ingress is
      ## {{ ingressPerReplica.hostPrefix }}-{{ $replicaNumber }}.{{ ingressPerReplica.hostDomain }}
      ##
      ## Prefix for the per replica ingress that will have `-$replicaNumber`
      ## appended to the end
      hostPrefix: ""
      ## Domain that will be used for the per replica ingress
      hostDomain: ""

      ## Paths to use for ingress rules
      ##
      paths: []
      # - /

      ## For Kubernetes >= 1.18 you should specify the pathType (determines how Ingress paths should be matched)
      ## See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#better-path-matching-with-path-types
      # pathType: ImplementationSpecific

      ## Secret name containing the TLS certificate for alertmanager per replica ingress
      ## Secret must be manually created in the namespace
      tlsSecretName: ""

      ## Separated secret for each per replica Ingress. Can be used together with cert-manager
      ##
      tlsSecretPerReplica:
        enabled: false
        ## Final form of the secret for each per replica ingress is
        ## {{ tlsSecretPerReplica.prefix }}-{{ $replicaNumber }}
        ##
        prefix: "alertmanager"

    ## Configuration for Alertmanager service
    ##
    service:
      annotations: {}
      labels: {}
      clusterIP: ""
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"

      ## Port for Alertmanager Service to listen on
      ##
      port: 9093
      ## To be used with a proxy extraContainer port
      ##
      targetPort: 9093
      ## Port to expose on each node
      ## Only used if service.type is 'NodePort'
      ##
      nodePort: 30903
      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##

      ## Additional ports to open for Alertmanager service
      ##
      additionalPorts: []
      # - name: oauth-proxy
      #   port: 8081
      #   targetPort: 8081
      # - name: oauth-metrics
      #   port: 8082
      #   targetPort: 8082

      externalIPs: []
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## If you want to make sure that connections from a particular client are passed to the same Pod each time
      ## Accepts 'ClientIP' or 'None'
      ##
      sessionAffinity: None

      ## If you want to modify the ClientIP sessionAffinity timeout
      ## The value must be >0 && <=86400(for 1 day) if ServiceAffinity == "ClientIP"
      ##
      sessionAffinityConfig:
        clientIP:
          timeoutSeconds: 10800

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a separate Service for each statefulset Alertmanager replica
    ##
    servicePerReplica:
      enabled: false
      annotations: {}

      ## Port for Alertmanager Service per replica to listen on
      ##
      port: 9093

      ## To be used with a proxy extraContainer port
      targetPort: 9093

      ## Port to expose on each node
      ## Only used if servicePerReplica.type is 'NodePort'
      ##
      nodePort: 30904

      ## Loadbalancer source IP ranges
      ## Only used if servicePerReplica.type is "LoadBalancer"
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

      ## Service type
      ##
      type: ClusterIP

    ## Configuration for creating a ServiceMonitor for AlertManager
    ##
    serviceMonitor:
      ## If true, a ServiceMonitor will be created for the AlertManager service.
      ##
      selfMonitor: false

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## Additional labels
      ##
      additionalLabels: {}

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## scheme: HTTP scheme to use for scraping. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## enableHttp2: Whether to enable HTTP2.
      ## See https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#endpoint
      enableHttp2: true

      ## tlsConfig: TLS configuration to use when scraping the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: {}

      bearerTokenFile:

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional Endpoints
      ##
      additionalEndpoints: []
      # - port: oauth-metrics
      #   path: /metrics

    ## Settings affecting alertmanagerSpec
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#alertmanagerspec
    ##
    alertmanagerSpec:
      ## Statefulset's persistent volume claim retention policy
      ## whenDeleted and whenScaled determine whether
      ## statefulset's PVCs are deleted (true) or retained (false)
      ## on scaling down and deleting statefulset, respectively.
      ## Requires Kubernetes version 1.27.0+.
      ## Ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#persistentvolumeclaim-retention
      persistentVolumeClaimRetentionPolicy: {}
      #  whenDeleted: Retain
      #  whenScaled: Retain

      ## Standard object's metadata. More info: https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md#metadata
      ## Metadata Labels and Annotations gets propagated to the Alertmanager pods.
      ##
      podMetadata: {}

      ## Image of Alertmanager
      ##
      image:
        registry: quay.io
        repository: prometheus/alertmanager
        tag: v0.28.0
        sha: ""

      ## If true then the user will be responsible to provide a secret with alertmanager configuration
      ## So when true the config part will be ignored (including templateFiles) and the one in the secret will be used
      ##
      useExistingSecret: false

      ## Secrets is a list of Secrets in the same namespace as the Alertmanager object, which shall be mounted into the
      ## Alertmanager Pods. The Secrets are mounted into /etc/alertmanager/secrets/.
      ##
      secrets: []

      ## If false then the user will opt out of automounting API credentials.
      ##
      automountServiceAccountToken: true

      ## ConfigMaps is a list of ConfigMaps in the same namespace as the Alertmanager object, which shall be mounted into the Alertmanager Pods.
      ## The ConfigMaps are mounted into /etc/alertmanager/configmaps/.
      ##
      configMaps: []

      ## ConfigSecret is the name of a Kubernetes Secret in the same namespace as the Alertmanager object, which contains configuration for
      ## this Alertmanager instance. Defaults to 'alertmanager-' The secret is mounted into /etc/alertmanager/config.
      ##
      # configSecret:

      ## WebTLSConfig defines the TLS parameters for HTTPS
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#alertmanagerwebspec
      web: {}

      ## AlertmanagerConfigs to be selected to merge and configure Alertmanager with.
      ##
      alertmanagerConfigSelector: {}
      ## Example which selects all alertmanagerConfig resources
      ## with label "alertconfig" with values any of "example-config" or "example-config-2"
      # alertmanagerConfigSelector:
      #   matchExpressions:
      #     - key: alertconfig
      #       operator: In
      #       values:
      #         - example-config
      #         - example-config-2
      #
      ## Example which selects all alertmanagerConfig resources with label "role" set to "example-config"
      # alertmanagerConfigSelector:
      #   matchLabels:
      #     role: example-config

      ## Namespaces to be selected for AlertmanagerConfig discovery. If nil, only check own namespace.
      ##
      alertmanagerConfigNamespaceSelector: {}
      ## Example which selects all namespaces
      ## with label "alertmanagerconfig" with values any of "example-namespace" or "example-namespace-2"
      # alertmanagerConfigNamespaceSelector:
      #   matchExpressions:
      #     - key: alertmanagerconfig
      #       operator: In
      #       values:
      #         - example-namespace
      #         - example-namespace-2

      ## Example which selects all namespaces with label "alertmanagerconfig" set to "enabled"
      # alertmanagerConfigNamespaceSelector:
      #   matchLabels:
      #     alertmanagerconfig: enabled

      ## AlermanagerConfig to be used as top level configuration
      ##
      alertmanagerConfiguration: {}
      ## Example with select a global alertmanagerconfig
      # alertmanagerConfiguration:
      #   name: global-alertmanager-Configuration

      ## Defines the strategy used by AlertmanagerConfig objects to match alerts. eg:
      ##
      alertmanagerConfigMatcherStrategy: {}
      ## Example with use OnNamespace strategy
      # alertmanagerConfigMatcherStrategy:
      #   type: OnNamespace

      ## Define Log Format
      # Use logfmt (default) or json logging
      logFormat: logfmt

      ## Log level for Alertmanager to be configured with.
      ##
      logLevel: info

      ## Size is the expected size of the alertmanager cluster. The controller will eventually make the size of the
      ## running cluster equal to the expected size.
      replicas: 1

      ## Time duration Alertmanager shall retain data for. Default is '120h', and must match the regular expression
      ## [0-9]+(ms|s|m|h) (milliseconds seconds minutes hours).
      ##
      retention: 120h

      ## Storage is the definition of how storage will be used by the Alertmanager instances.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
      ##
      storage: {}
      # volumeClaimTemplate:
      #   spec:
      #     storageClassName: gluster
      #     accessModes: ["ReadWriteOnce"]
      #     resources:
      #       requests:
      #         storage: 50Gi
      #   selector: {}


      ## The external URL the Alertmanager instances will be available under. This is necessary to generate correct URLs. This is necessary if Alertmanager is not served from root of a DNS name. string  false
      ##
      externalUrl:

      ## The route prefix Alertmanager registers HTTP handlers for. This is useful, if using ExternalURL and a proxy is rewriting HTTP routes of a request, and the actual ExternalURL is still true,
      ## but the server serves requests under a different route prefix. For example for use with kubectl proxy.
      ##
      routePrefix: /

      ## scheme: HTTP scheme to use. Can be used with `tlsConfig` for example if using istio mTLS.
      scheme: ""

      ## tlsConfig: TLS configuration to use when connect to the endpoint. For example if using istio mTLS.
      ## Of type: https://github.com/coreos/prometheus-operator/blob/main/Documentation/api-reference/api.md#tlsconfig
      tlsConfig: {}

      ## If set to true all actions on the underlying managed objects are not going to be performed, except for delete actions.
      ##
      paused: false

      ## Define which Nodes the Pods are scheduled on.
      ## ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Define resources requests and limits for single Pods.
      ## ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
      # requests:
      #   memory: 400Mi

      ## Pod anti-affinity can prevent the scheduler from placing Prometheus replicas on the same node.
      ## The default value "soft" means that the scheduler should *prefer* to not schedule two replica pods onto the same node but no guarantee is provided.
      ## The value "hard" means that the scheduler is *required* to not schedule two replica pods onto the same node.
      ## The value "" will disable pod anti-affinity so that no anti-affinity rules will be configured.
      ##
      podAntiAffinity: "soft"

      ## If anti-affinity is enabled sets the topologyKey to use for anti-affinity.
      ## This can be changed to, for example, failure-domain.beta.kubernetes.io/zone
      ##
      podAntiAffinityTopologyKey: kubernetes.io/hostname

      ## Assign custom affinity rules to the alertmanager instance
      ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      affinity: {}
      # nodeAffinity:
      #   requiredDuringSchedulingIgnoredDuringExecution:
      #     nodeSelectorTerms:
      #     - matchExpressions:
      #       - key: kubernetes.io/e2e-az-name
      #         operator: In
      #         values:
      #         - e2e-az1
      #         - e2e-az2

      ## If specified, the pod's tolerations.
      ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
      ##
      tolerations: []
      # - key: "key"
      #   operator: "Equal"
      #   value: "value"
      #   effect: "NoSchedule"

      ## If specified, the pod's topology spread constraints.
      ## ref: https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/
      ##
      topologySpreadConstraints: []
      # - maxSkew: 1
      #   topologyKey: topology.kubernetes.io/zone
      #   whenUnsatisfiable: DoNotSchedule
      #   labelSelector:
      #     matchLabels:
      #       app: alertmanager

      ## SecurityContext holds pod-level security attributes and common container settings.
      ## This defaults to non root user with uid 1000 and gid 2000. *v1.PodSecurityContext  false
      ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
      ##
      securityContext:
        runAsGroup: 2000
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
        seccompProfile:
          type: RuntimeDefault

      ## ListenLocal makes the Alertmanager server listen on loopback, so that it does not bind against the Pod IP.
      ## Note this is only for the Alertmanager UI, not the gossip communication.
      ##
      listenLocal: false

      ## Containers allows injecting additional containers. This is meant to allow adding an authentication proxy to an Alertmanager pod.
      ##
      containers: []
      # containers:
      # - name: oauth-proxy
      #   image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
      #   args:
      #   - --upstream=http://127.0.0.1:9093
      #   - --http-address=0.0.0.0:8081
      #   - --metrics-address=0.0.0.0:8082
      #   - ...
      #   ports:
      #   - containerPort: 8081
      #     name: oauth-proxy
      #     protocol: TCP
      #   - containerPort: 8082
      #     name: oauth-metrics
      #     protocol: TCP
      #   resources: {}

      # Additional volumes on the output StatefulSet definition.
      volumes: []

      # Additional VolumeMounts on the output StatefulSet definition.
      volumeMounts: []

      ## InitContainers allows injecting additional initContainers. This is meant to allow doing some changes
      ## (permissions, dir tree) on mounted volumes before starting prometheus
      initContainers: []

      ## Priority class assigned to the Pods
      ##
      priorityClassName: ""

      ## AdditionalPeers allows injecting a set of additional Alertmanagers to peer with to form a highly available cluster.
      ##
      additionalPeers: []

      ## PortName to use for Alert Manager.
      ##
      portName: "http-web"

      ## ClusterAdvertiseAddress is the explicit address to advertise in cluster. Needs to be provided for non RFC1918 [1] (public) addresses. [1] RFC1918: https://tools.ietf.org/html/rfc1918
      ##
      clusterAdvertiseAddress: false

      ## clusterGossipInterval determines interval between gossip attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterGossipInterval: ""

      ## clusterPeerTimeout determines timeout for cluster peering.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPeerTimeout: ""

      ## clusterPushpullInterval determines interval between pushpull attempts.
      ## Needs to be specified as GoDuration, a time duration that can be parsed by Go’s time.ParseDuration() (e.g. 45ms, 30s, 1m, 1h20m15s)
      clusterPushpullInterval: ""

      ## clusterLabel defines the identifier that uniquely identifies the Alertmanager cluster.
      clusterLabel: ""

      ## ForceEnableClusterMode ensures Alertmanager does not deactivate the cluster mode when running with a single replica.
      ## Use case is e.g. spanning an Alertmanager cluster across Kubernetes clusters with a single replica in each.
      forceEnableClusterMode: false

      ## Minimum number of seconds for which a newly created pod should be ready without any of its container crashing for it to
      ## be considered available. Defaults to 0 (pod will be considered available as soon as it is ready).
      minReadySeconds: 0

      ## Additional configuration which is not covered by the properties above. (passed through tpl)
      additionalConfig: {}

      ## Additional configuration which is not covered by the properties above.
      ## Useful, if you need advanced templating inside alertmanagerSpec.
      ## Otherwise, use alertmanager.alertmanagerSpec.additionalConfig (passed through tpl)
      additionalConfigString: ""

    ## ExtraSecret can be used to store various data in an extra secret
    ## (use it for example to store hashed basic auth credentials)
    extraSecret:
      ## if not set, name will be auto generated
      # name: ""
      annotations: {}
      data: {}
    #   auth: |
    #     foo:$apr1$OFG3Xybp$ckL0FHDAkoXYIlH9.cysT0
    #     someoneelse:$apr1$DMZX2Z4q$6SbQIfyuLQd.xmo/P0m2c.

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: false
    forceDeployDashboards: true
    namespaceOverride: ""
    extraConfigmapMounts: []
    # - name: certs-configmap
    #   mountPath: /etc/grafana/ssl/
    #   configMap: certs-configmap
    #   readOnly: true

    deleteDatasources: []
    # - name: example-datasource
    #   orgId: 1

    ## Configure additional grafana datasources (passed through tpl)
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    additionalDataSources: []
    # - name: prometheus-sample
    #   access: proxy
    #   basicAuth: true
    #   secureJsonData:
    #       basicAuthPassword: pass
    #   basicAuthUser: daco
    #   editable: false
    #   jsonData:
    #       tlsSkipVerify: true
    #   orgId: 1
    #   type: prometheus
    #   url: https://{{ printf "%s-prometheus.svc" .Release.Name }}:9090
    #   version: 1

    # Flag to mark provisioned data sources for deletion if they are no longer configured.
    # It takes no effect if data sources are already listed in the deleteDatasources section.
    # ref: https://grafana.com/docs/grafana/latest/administration/provisioning/#example-data-source-config-file
    prune: false

    ## Passed to grafana subchart and used by servicemonitor below
    ##
    service:
      portName: http-web
      ipFamilies: []
      ipFamilyPolicy: ""

    serviceMonitor:
      # If true, a ServiceMonitor CRD is created for a prometheus operator
      # https://github.com/coreos/prometheus-operator
      #
      enabled: true

      # Path to use for scraping metrics. Might be different if server.root_url is set
      # in grafana.ini
      path: "/metrics"

      #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)

      # labels for the ServiceMonitor
      labels: {}

      # Scrape interval. If not set, the Prometheus default scrape interval is used.
      #
      interval: ""
      scheme: http
      tlsConfig: {}
      scrapeTimeout: 30s

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

  ## Flag to disable all the kubernetes component scrapers
  ##
  kubernetesServiceMonitors:
    enabled: true

  ## Component scraping the kube api server
  ##
  kubeApiServer:
    enabled: true
    tlsConfig:
      serverName: kubernetes
      insecureSkipVerify: false
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      jobLabel: component
      selector:
        matchLabels:
          component: apiserver
          provider: kubernetes

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings:
        # Drop excessively noisy apiserver buckets.
        - action: drop
          regex: (etcd_request|apiserver_request_slo|apiserver_request_sli|apiserver_request)_duration_seconds_bucket;(0\.15|0\.2|0\.3|0\.35|0\.4|0\.45|0\.6|0\.7|0\.8|0\.9|1\.25|1\.5|1\.75|2|3|3\.5|4|4\.5|6|7|8|9|15|20|30|40|45|50)(\.0)?
          sourceLabels:
            - __name__
            - le
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels:
      #     - __meta_kubernetes_namespace
      #     - __meta_kubernetes_service_name
      #     - __meta_kubernetes_endpoint_port_name
      #   action: keep
      #   regex: default;kubernetes;https
      # - targetLabel: __address__
      #   replacement: kubernetes.default.svc:443

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping the kubelet and kubelet-hosted cAdvisor
  ##
  kubelet:
    enabled: true
    namespace: kube-system

    serviceMonitor:
      ## Enable scraping /metrics from kubelet's service
      kubelet: true

      ## Attach metadata to discovered targets. Requires Prometheus v2.45 for endpoints created by the operator.
      ##
      attachMetadata:
        node: false

      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: 30s

      ## If true, Prometheus use (respect) labels provided by exporter.
      ##
      honorLabels: true

      ## If true, Prometheus ingests metrics with timestamp provided by exporter. If false, Prometheus ingests metrics with timestamp of scrape.
      ##
      honorTimestamps: true

      ## If true, defines whether Prometheus tracks staleness of the metrics that have an explicit timestamp present in scraped data. Has no effect if `honorTimestamps` is false.
      ## We recommend enabling this if you want the best possible accuracy for container_ metrics scraped from cadvisor.
      ## For more details see: https://github.com/prometheus-community/helm-charts/pull/5063#issuecomment-2545374849
      trackTimestampsStaleness: true

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## Enable scraping the kubelet over https. For requirements to enable this see
      ## https://github.com/prometheus-operator/prometheus-operator/issues/926
      ##
      https: true

      ## Skip TLS certificate validation when scraping.
      ## This is enabled by default because kubelet serving certificate deployed by kubeadm is by default self-signed
      ## ref: https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-certs/#kubelet-serving-certs
      ##
      insecureSkipVerify: true

      ## Enable scraping /metrics/probes from kubelet's service
      ##
      probes: true

      ## Enable scraping /metrics/resource from kubelet's service
      ## This is disabled by default because container metrics are already exposed by cAdvisor
      ##
      resource: false
      # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource
      resourcePath: "/metrics/resource/v1alpha1"
      ## Configure the scrape interval for resource metrics. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      resourceInterval: 10s

      ## Enable scraping /metrics/cadvisor from kubelet's service
      ##
      cAdvisor: true
      ## Configure the scrape interval for cAdvisor. This is configured to the default Kubelet cAdvisor
      ## minimum housekeeping interval in order to avoid missing samples. Note, this value is ignored
      ## if kubelet.serviceMonitor.interval is not empty.
      cAdvisorInterval: 10s
      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      cAdvisorMetricRelabelings:
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)'
        # Drop container_memory_failures_total{scope="hierarchy"} metrics,
        # we only need the container scope.
        - sourceLabels: [__name__, scope]
          action: drop
          regex: 'container_memory_failures_total;hierarchy'
        # Drop container_network_... metrics that match various interfaces that
        # correspond to CNI and similar interfaces. This avoids capturing network
        # metrics for host network containers.
        - sourceLabels: [__name__, interface]
          action: drop
          regex: 'container_network_.*;(cali|cilium|cni|lxc|nodelocaldns|tunl).*'
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      probesMetricRelabelings: []
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      cAdvisorRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path


      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      probesRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      resourceRelabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings:
        # Reduce bucket cardinality of kubelet storage operations.
        - action: drop
          sourceLabels: [__name__, le]
          regex: (csi_operations|storage_operation_duration)_seconds_bucket;(0.25|2.5|15|25|120|600)(\.0)?
      # - sourceLabels: [__name__, image]
      #   separator: ;
      #   regex: container_([a-z_]+);
      #   replacement: $1
      #   action: drop
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
      #   replacement: $1
      #   action: drop

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      ## metrics_path is required to match upstream rules and charts
      relabelings:
        - action: replace
          sourceLabels: [__metrics_path__]
          targetLabel: metrics_path
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: true

    ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeControllerManager.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.22.
      ##
      port: null
      targetPort: null
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: kube-controller-manager

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-controller-manager

      ## Enable scraping kube-controller-manager over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
      ## If null or unset, the value is determined dynamically based on target Kubernetes version.
      ##
      https: null

      # Skip TLS certificate validation when scraping
      insecureSkipVerify: null

      # Name of the server to use when validating TLS certificate
      serverName: null

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping coreDns. Use either this or kubeDns
  ##
  coreDns:
    enabled: true
    service:
      enabled: true
      port: 9153
      targetPort: 9153

      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-dns
    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-dns

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kubeDns. Use either this or coreDns
  ##
  kubeDns:
    enabled: false
    service:
      dnsmasq:
        port: 10054
        targetPort: 10054
      skydns:
        port: 10055
        targetPort: 10055
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-dns
    serviceMonitor:
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-dns

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      dnsmasqMetricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      dnsmasqRelabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping etcd
  ##
  kubeEtcd:
    enabled: false

    ## If your etcd is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      port: 2381
      targetPort: 2381
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: etcd

    ## Configure secure access to the etcd cluster by loading a secret into prometheus and
    ## specifying security configuration below. For example, with a secret named etcd-client-cert
    ##
    ## serviceMonitor:
    ##   scheme: https
    ##   insecureSkipVerify: false
    ##   serverName: localhost
    ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca
    ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client
    ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key
    ##
    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
      scheme: http
      insecureSkipVerify: false
      serverName: ""
      caFile: ""
      certFile: ""
      keyFile: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: etcd

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

    ## If your kube scheduler is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    ## If using kubeScheduler.endpoints only the port and targetPort are used
    ##
    service:
      enabled: true
      ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change
      ## of default port in Kubernetes 1.23.
      ##
      port: null
      targetPort: null
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   component: kube-scheduler

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""
      ## Enable scraping kube-scheduler over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.
      ## If null or unset, the value is determined dynamically based on target Kubernetes version.
      ##
      https: null

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    component: kube-scheduler

      ## Skip TLS certificate validation when scraping
      insecureSkipVerify: null

      ## Name of the server to use when validating TLS certificate
      serverName: null

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false

    ## If your kube proxy is not deployed as a pod, specify IPs it can be found on
    ##
    endpoints: []
    # - 10.141.4.22
    # - 10.141.4.23
    # - 10.141.4.24

    service:
      enabled: true
      port: 10249
      targetPort: 10249
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      # selector:
      #   k8s-app: kube-proxy

    serviceMonitor:
      enabled: true
      ## Scrape interval. If not set, the Prometheus default scrape interval is used.
      ##
      interval: ""

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      sampleLimit: 0

      ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
      ##
      targetLimit: 0

      ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelLimit: 0

      ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelNameLengthLimit: 0

      ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
      ##
      labelValueLengthLimit: 0

      ## proxyUrl: URL of a proxy that should be used for scraping.
      ##
      proxyUrl: ""

      ## port: Name of the port the metrics will be scraped from
      ##
      port: http-metrics

      jobLabel: jobLabel
      selector: {}
      #  matchLabels:
      #    k8s-app: kube-proxy

      ## Enable scraping kube-proxy over https.
      ## Requires proper certs (not self-signed) and delegated authentication/authorization checks
      ##
      https: false

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      metricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
      ##
      relabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## Additional labels
      ##
      additionalLabels: {}
      #  foo: bar

      ## defines the labels which are transferred from the associated Kubernetes Service object onto the ingested metrics.
      ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#servicemonitor
      targetLabels: []

  ## Component scraping kube state metrics
  ##
  kubeStateMetrics:
    enabled: true

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    namespaceOverride: ""
    rbac:
      create: true
    releaseLabel: true
    prometheus:
      monitor:
        enabled: true

        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: ""

        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0

        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0

        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0

        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0

        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0

        ## Scrape Timeout. If not set, the Prometheus default scrape timeout is used.
        ##
        scrapeTimeout: ""

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        # Keep labels from scraped data, overriding server-side labels
        ##
        honorLabels: true

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - action: keep
        #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
        #   sourceLabels: [__name__]

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

    selfMonitor:
      enabled: false

  ## Deploy node exporter as a daemonset to all nodes
  ##
  nodeExporter:
    enabled: true
    operatingSystems:
      linux:
        enabled: true
      aix:
        enabled: true
      darwin:
        enabled: true

    ## ForceDeployDashboard Create dashboard configmap even if nodeExporter deployment has been disabled
    ##
    forceDeployDashboards: true

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    namespaceOverride: kube-system
    podLabels:
      ## Add the 'node-exporter' label to be used by serviceMonitor to match standard common usage in rules and grafana dashboards
      ##
      jobLabel: node-exporter
    releaseLabel: true
    extraArgs:
      - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
      - --collector.netdev.device-exclude=^lxc.*
      - --collector.arp.device-exclude=^lxc.*
      - --collector.ethtool.device-exclude=^lxc.*
    service:
      portName: http-metrics
      ipDualStack:
        enabled: false
        ipFamilies: ["IPv6", "IPv4"]
        ipFamilyPolicy: "PreferDualStack"
      labels:
        jobLabel: node-exporter

    prometheus:
      monitor:
        enabled: true

        jobLabel: jobLabel

        ## Scrape interval. If not set, the Prometheus default scrape interval is used.
        ##
        interval: 30s

        ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
        ##
        sampleLimit: 0

        ## TargetLimit defines a limit on the number of scraped targets that will be accepted.
        ##
        targetLimit: 0

        ## Per-scrape limit on number of labels that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelLimit: 0

        ## Per-scrape limit on length of labels name that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelNameLengthLimit: 0

        ## Per-scrape limit on length of labels value that will be accepted for a sample. Only valid in Prometheus versions 2.27.0 and newer.
        ##
        labelValueLengthLimit: 0

        ## How long until a scrape request times out. If not set, the Prometheus default scape timeout is used.
        ##
        scrapeTimeout: ""

        ## proxyUrl: URL of a proxy that should be used for scraping.
        ##
        proxyUrl: ""

        ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        metricRelabelings: []
        # - sourceLabels: [__name__]
        #   separator: ;
        #   regex: ^node_mountstats_nfs_(event|operations|transport)_.+
        #   replacement: $1
        #   action: drop

        ## RelabelConfigs to apply to samples before scraping
        ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api-reference/api.md#relabelconfig
        ##
        relabelings: []
        # - sourceLabels: [__meta_kubernetes_pod_node_name]
        #   separator: ;
        #   regex: ^(.*)$
        #   targetLabel: nodename
        #   replacement: $1
        #   action: replace

        ## Attach node metadata to discovered targets. Requires Prometheus v2.35.0 and above.
        ##
        # attachMetadata:
        #   node: false

    rbac:
      ## If true, create PSPs for node-exporter
      ##
      pspEnabled: false

  ## Manages Prometheus and Alertmanager components
  ##
  prometheusOperator:
    enabled: true


    ## Admission webhook support for PrometheusRules resources added in Prometheus Operator 0.30 can be enabled to prevent incorrectly formatted
    ## rules from making their way into prometheus and potentially preventing the container from starting
    admissionWebhooks:
      ## Valid values: Fail, Ignore, IgnoreOnInstallOnly
      ## IgnoreOnInstallOnly - If Release.IsInstall returns "true", set "Ignore" otherwise "Fail"
      failurePolicy: ""
      ## The default timeoutSeconds is 10 and the maximum value is 30.
      timeoutSeconds: 10
      enabled: true
      ## A PEM encoded CA bundle which will be used to validate the webhook's server certificate.
      ## If unspecified, system trust roots on the apiserver are used.
      caBundle: ""
      ## If enabled, generate a self-signed certificate, then patch the webhook configurations with the generated data.
      ## On chart upgrades (or if the secret exists) the cert will not be re-generated. You can use this to provide your own
      ## certs ahead of time if you wish.
      ##
      annotations: {}
      #   argocd.argoproj.io/hook: PreSync
      #   argocd.argoproj.io/hook-delete-policy: HookSucceeded

      namespaceSelector: {}
      objectSelector: {}

      mutatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync

      validatingWebhookConfiguration:
        annotations: {}
        #   argocd.argoproj.io/hook: PreSync

      deployment:
        enabled: false

      patch:
        enabled: true
      # Use certmanager to generate webhook certs
      certManager:
        enabled: false
        # self-signed root certificate
        rootCert:
          duration: ""  # default to be 5y
        admissionCert:
          duration: ""  # default to be 1y
        # issuerRef:
        #   name: "issuer"
        #   kind: "ClusterIssuer"

    ## Namespaces to scope the interaction of the Prometheus Operator and the apiserver (allow list).
    ## This is mutually exclusive with denyNamespaces. Setting this to an empty object will disable the configuration
    ##
    namespaces: {}
      # releaseNamespace: true
      # additional:
      # - kube-system

    ## Namespaces not to scope the interaction of the Prometheus Operator (deny list).
    ##
    denyNamespaces: []

    ## Filter namespaces to look for prometheus-operator custom resources
    ##
    alertmanagerInstanceNamespaces: []
    alertmanagerConfigNamespaces: []
    prometheusInstanceNamespaces: []
    thanosRulerInstanceNamespaces: []

    ## The clusterDomain value will be added to the cluster.peer option of the alertmanager.
    ## Without this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated:9094 (default value)
    ## With this specified option cluster.peer will have value alertmanager-monitoring-alertmanager-0.alertmanager-operated.namespace.svc.cluster-domain:9094
    ##
    # clusterDomain: "cluster.local"

    networkPolicy:
      ## Enable creation of NetworkPolicy resources.
      ##
      enabled: false

      ## Flavor of the network policy to use.
      #  Can be:
      #  * kubernetes for networking.k8s.io/v1/NetworkPolicy
      #  * cilium     for cilium.io/v2/CiliumNetworkPolicy
      flavor: kubernetes

      # cilium:
      #   egress:

      ## match labels used in selector
      # matchLabels: {}

    ## Service account for Prometheus Operator to use.
    ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
    ##
    serviceAccount:
      create: true
      name: ""
      automountServiceAccountToken: true
      annotations: {}

    # -- terminationGracePeriodSeconds for container lifecycle hook
    terminationGracePeriodSeconds: 30
    # -- Specify lifecycle hooks for the  controller
    lifecycle: {}
    ## Configuration for Prometheus operator service
    ##
    service:
      ipDualStack:
        enabled: false

    ## Port to expose on each node
    ## Only used if service.type is 'NodePort'
    ##
      nodePort: 30080

      nodePortTls: 30443

    ## Additional ports to open for Prometheus operator service
    ## ref: https://kubernetes.io/docs/concepts/services-networking/service/#multi-port-services
    ##
      additionalPorts: []

    ## Loadbalancer IP
    ## Only use if service.type is "LoadBalancer"
    ##
      loadBalancerIP: ""
      loadBalancerSourceRanges: []

      ## Denotes if this Service desires to route external traffic to node-local or cluster-wide endpoints
      ##
      externalTrafficPolicy: Cluster

    ## Service type
    ## NodePort, ClusterIP, LoadBalancer
    ##
      type: ClusterIP

      ## List of IP addresses at which the Prometheus server service is available
      ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
      ##
      externalIPs: []


    ## Assign a PriorityClassName to pods if set
    # priorityClassName: ""

    ## Define Log Format
    # Use logfmt (default) or json logging
    # logFormat: logfmt

    ## Decrease log verbosity to errors only
    # logLevel: error

    kubeletService:
      ## If true, the operator will create and maintain a service for scraping kubelets
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/helm/prometheus-operator/README.md
      ##
      enabled: true
      namespace: kube-system
      selector: ""
      ## Use '{{ template "kube-prometheus-stack.fullname" . }}-kubelet' by default
      name: ""

    ## Create Endpoints objects for kubelet targets.
    kubeletEndpointsEnabled: true
    ## Create EndpointSlice objects for kubelet targets.
    kubeletEndpointSliceEnabled: true

    ## Extra arguments to pass to prometheusOperator
    # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/operator.md
    extraArgs: []
    #  - --labels="cluster=talos-cluster"

    ## Create a servicemonitor for the operator
    ##
    serviceMonitor:
      ## If true, create a serviceMonitor for prometheus operator
      ##
      selfMonitor: false

  ## Deploy a Prometheus instance
  ##
  prometheus:
    enabled: true

    prometheusSpec:
      replicas: 0

  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  ##
  thanosRuler:

    ## Deploy thanosRuler
    ##
    enabled: false


prometheus-smartctl-exporter:
  
prometheus-ipmi-exporter:
  enabled: true