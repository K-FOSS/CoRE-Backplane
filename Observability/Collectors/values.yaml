alloy:
  # -- Overrides the chart's name. Used to change the infix in the resource names.
  nameOverride: null

  # -- Overrides the chart's computed fullname. Used to change the full prefix of
  # resource names.
  fullnameOverride: null

  ## Global properties for image pulling override the values defined under `image.registry` and `configReloader.image.registry`.
  ## If you want to override only one image registry, use the specific fields but if you want to override them all, use `global.image.registry`
  global:
    image:
      # -- Global image registry to use if it needs to be overriden for some specific use cases (e.g local registries, custom images, ...)
      registry: ""

      # -- Optional set of global image pull secrets.
      pullSecrets: []

    # -- Security context to apply to the Grafana Alloy pod.
    podSecurityContext: {}

  crds:
    # -- Whether to install CRDs for monitoring.
    create: true

  ## Various Alloy settings. For backwards compatibility with the grafana-agent
  ## chart, this field may also be called "agent". Naming this field "agent" is
  ## deprecated and will be removed in a future release.
  alloy:
    configMap:
      # -- Create a new ConfigMap for the config file.
      create: true
      # -- Content to assign to the new ConfigMap.  This is passed into `tpl` allowing for templating from values.
      content: | # js
        // Configure internal logging from Alloy
        logging {
          level = "info"
          format = "json"

          write_to = [loki.write.loki.receiver]
        }

        // Configure internal traces from Alloy
        tracing {
          sampling_fraction = 0.1

          write_to = []
        }


        //
        // Logs
        //

        //
        // SysLog Collector
        // 
        loki.source.syslog "syslog" {
          listener {
            address  = "0.0.0.0:1514"
            labels   = { 
              component = "loki.source.syslog", 
              protocol = "tcp",
              job = "syslog", 
            }
          }



          listener {
            address               = "0.0.0.0:1514"
            protocol              = "udp"

            syslog_format = "rfc3164"
            idle_timeout          = "1m0s"
            label_structured_data = true
            labels = {
              component = "loki.source.syslog",
              protocol = "udp",
              job = "syslog",
            }
            max_message_length = 0
          }
          forward_to    = [loki.write.loki.receiver]
          relabel_rules = discovery.relabel.syslog.rules
        }

        discovery.relabel "syslog" {
          targets = []

          rule {
                  source_labels = ["__syslog_message_hostname"]
                  target_label  = "host"
          }

          rule {
                  source_labels = ["__syslog_message_severity"]
                  target_label  = "severity"
          }

          rule {
                  source_labels = ["__syslog_message_app_name"]
                  target_label  = "app"
          }
        }

        // Begin Auto Discovery Configuration

        // discovery.kubernetes allows you to find scrape targets from Kubernetes resources.
        // It watches cluster state and ensures targets are continually synced with what is currently running in your cluster.
        discovery.kubernetes "pod" {
          role = "pod"

          selectors {
            role = "pod"
            label = "logs=loki-myloginspace"
          }
        }

        // discover all services, to be used later in this config
        discovery.kubernetes "services" {
          role = "service"
        }

        // End Auto Discovery

        // discovery.relabel rewrites the label set of the input targets by applying one or more relabeling rules.
        // If no rules are defined, then the input targets are exported as-is.
        discovery.relabel "pod_logs" {
          targets = discovery.kubernetes.pod.targets

          // Label creation - "namespace" field from "__meta_kubernetes_namespace"
          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            action = "replace"
            target_label = "namespace"
          }

          // Label creation - "pod" field from "__meta_kubernetes_pod_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            action = "replace"
            target_label = "pod"
          }

          // Label creation - "container" field from "__meta_kubernetes_pod_container_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "container"
          }

          // Label creation -  "app" field from "__meta_kubernetes_pod_label_app_kubernetes_io_name"
          rule {
            source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
            action = "replace"
            target_label = "app"
          }

          // Label creation -  "job" field from "__meta_kubernetes_namespace" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_namespace/__meta_kubernetes_pod_container_name
          rule {
            source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "job"
            separator = "/"
            replacement = "$1"
          }

          // Label creation - "container" field from "__meta_kubernetes_pod_uid" and "__meta_kubernetes_pod_container_name"
          // Concatenate values __meta_kubernetes_pod_uid/__meta_kubernetes_pod_container_name.log
          rule {
            source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
            action = "replace"
            target_label = "__path__"
            separator = "/"
            replacement = "/var/log/pods/*$1/*.log"
          }

          // Label creation -  "container_runtime" field from "__meta_kubernetes_pod_container_id"
          rule {
            source_labels = ["__meta_kubernetes_pod_container_id"]
            action = "replace"
            target_label = "container_runtime"
            regex = "^(\\S+):\\/\\/.+$"
            replacement = "$1"
          }
        }

        // loki.source.kubernetes tails logs from Kubernetes containers using the Kubernetes API.
        loki.source.kubernetes "pod_logs" {
          targets    = discovery.relabel.pod_logs.output
          forward_to = [loki.process.pod_logs.receiver]
        }

        // loki.process receives log entries from other Loki components, applies one or more processing stages,
        // and forwards the results to the list of receivers in the component's arguments.
        loki.process "pod_logs" {
          stage.static_labels {
              values = {
                cluster = sys.env("CLUSTER"),
                dc = sys.env("DC"),
              }
          }

          forward_to = [loki.write.loki.receiver]
        }

        // Setup a reciever specifically for Vector
        loki.source.api "listener" {
          http {
            listen_address = "0.0.0.0"
            listen_port = 9999
          }

          labels = { 
            component = "loki.source.api", 
          }

          forward_to = [loki.write.loki.receiver]
        }

        loki.write "loki" {
          endpoint {
            url = "http://10.44.5.179:3100/loki/api/v1/push"
          }
        }

        //
        // Metrics
        //
        
        // Monitor Self https://grafana.com/docs/alloy/latest/reference/components/prometheus/prometheus.exporter.self/
        prometheus.exporter.self "self_metrics" {
        }

        prometheus.relabel "tagged" {
        	rule {
        		source_labels = ["__address__"]
        		regex         = ".*"
        		replacement   = sys.env("CLUSTER")
        		target_label  = "cluster"
        	}

        	forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.operator.podmonitors "pods" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        // Monitor Services
        prometheus.operator.servicemonitors "services" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.operator.probes "probes" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        // Generic scrape of any pod with Annotation "prometheus.io/scrape: true"
        discovery.relabel "annotation_autodiscovery_pods" {
          targets = discovery.kubernetes.pod.targets
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_scrape"]
            regex = "true"
            action = "keep"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_job"]
            action = "replace"
            target_label = "job"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_instance"]
            action = "replace"
            target_label = "instance"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_path"]
            action = "replace"
            target_label = "__metrics_path__"
          }

          // Choose the pod port
          // The discovery generates a target for each declared container port of the pod.
          // If the metricsPortName annotation has value, keep only the target where the port name matches the one of the annotation.
          rule {
            source_labels = ["__meta_kubernetes_pod_container_port_name"]
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_portName"]
            regex = "(.+)"
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_container_port_name"]
            action = "keepequal"
            target_label = "__tmp_port"
          }

          // If the metrics port number annotation has a value, override the target address to use it, regardless whether it is
          // one of the declared ports on that Pod.
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_port", "__meta_kubernetes_pod_ip"]
            regex = "(\\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})"
            replacement = "[$2]:$1" // IPv6
            target_label = "__address__"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_port", "__meta_kubernetes_pod_ip"]
            regex = "(\\d+);((([0-9]+?)(\\.|$)){4})" // IPv4, takes priority over IPv6 when both exists
            replacement = "$2:$1"
            target_label = "__address__"
          }

          rule {
            source_labels = ["__meta_kubernetes_pod_annotation_prometheus_io_scheme"]
            action = "replace"
            target_label = "__scheme__"
          }


          // add labels
          rule {
            source_labels = ["__meta_kubernetes_pod_name"]
            target_label = "pod"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_container_name"]
            target_label = "container"
          }
          rule {
            source_labels = ["__meta_kubernetes_pod_controller_name"]
            target_label = "controller"
          }

          rule {
            source_labels = ["__meta_kubernetes_namespace"]
            target_label = "namespace"
          }


          rule {
            source_labels = ["__meta_kubernetes_pod_label_app"]
            target_label = "app"
          }

          // map all labels
          rule {
            action = "labelmap"
            regex  = "__meta_kubernetes_pod_label_(.+)"
          }
        }

        // Generic scrape of any service with
        // Annotation Autodiscovery
        discovery.relabel "annotation_autodiscovery_services" {
          targets = discovery.kubernetes.services.targets
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_scrape"]
            regex = "true"
            action = "keep"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_job"]
            action = "replace"
            target_label = "job"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_instance"]
            action = "replace"
            target_label = "instance"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_path"]
            action = "replace"
            target_label = "__metrics_path__"
          }

          // Choose the service port
          rule {
            source_labels = ["__meta_kubernetes_service_port_name"]
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_portName"]
            regex = "(.+)"
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_port_name"]
            action = "keepequal"
            target_label = "__tmp_port"
          }

          rule {
            source_labels = ["__meta_kubernetes_service_port_number"]
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_port"]
            regex = "(.+)"
            target_label = "__tmp_port"
          }
          rule {
            source_labels = ["__meta_kubernetes_service_port_number"]
            action = "keepequal"
            target_label = "__tmp_port"
          }

          rule {
            source_labels = ["__meta_kubernetes_service_annotation_prometheus_io_scheme"]
            action = "replace"
            target_label = "__scheme__"
          }
        }

        prometheus.exporter.unix "default" { /* use defaults */ }

        prometheus.scrape "metrics" {
          job_name   = "integrations/autodiscovery_metrics"
          targets  = concat(discovery.relabel.annotation_autodiscovery_pods.output, discovery.relabel.annotation_autodiscovery_services.output, prometheus.exporter.unix.default.targets)
          honor_labels = true
          clustering {
            enabled = true
          }
          forward_to = [prometheus.relabel.metrics_service.receiver]
        }

        prometheus.relabel "metrics_service" {
          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        // PSQL monitoring
        // read the credentials secret for remote_write authorization

        prometheus.scrape "exporters" {
          targets = concat(
            prometheus.exporter.self.self_metrics.targets,
          )

          job_name = "integrations"
          scrape_interval = "10s"
          scrape_timeout  = "10s"

          forward_to = [prometheus.remote_write.mimir.receiver]
        }

        prometheus.remote_write "mimir" {
          endpoint {
            url = "http://10.44.4.77:8080/api/v1/push"
          }

          external_labels = {
            cluster = sys.env("CLUSTER"),
            dc = sys.env("DC"),
          }
        }

        //
        // Tracing
        //
        otelcol.receiver.otlp "default" {
          http {
            endpoint = "0.0.0.0:4318"
          }
          grpc {
            endpoint = "0.0.0.0:4317"
          }


          output {
            traces  = [otelcol.processor.k8sattributes.default.input]
          }
        }

        otelcol.processor.k8sattributes "default" {
          extract {
            metadata = [
              "k8s.namespace.name",
              "k8s.pod.name",
              "k8s.container.name",
            ]
          }

          output {
            traces = []
          }
        }

        otelcol.receiver.jaeger "local" {
          protocols {
            grpc {
              endpoint = "0.0.0.0:14250"
            }
            thrift_http {
              endpoint = "0.0.0.0:14268"
            }
            thrift_binary {
              endpoint = "0.0.0.0:6832"
            }
            thrift_compact {
              endpoint = "0.0.0.0:6831"
            }
          }

          output {
            traces  = []
          }
        }

      # -- Name of existing ConfigMap to use. Used when create is false.
      name: null
      # -- Key in ConfigMap to get config from.
      key: null

    clustering:
      # -- Deploy Alloy in a cluster to allow for load distribution.
      enabled: false

      # -- Name for the Alloy cluster. Used for differentiating between clusters.
      name: ""

      # -- Name for the port used for clustering, useful if running inside an Istio Mesh
      portName: http

    # -- Minimum stability level of components and behavior to enable. Must be
    # one of "experimental", "public-preview", or "generally-available".
    stabilityLevel: "generally-available"

    # -- Path to where Grafana Alloy stores data (for example, the Write-Ahead Log).
    # By default, data is lost between reboots.
    storagePath: /tmp/alloy

    # -- Address to listen for traffic on. 0.0.0.0 exposes the UI to other
    # containers.
    listenAddr: 0.0.0.0

    # -- Port to listen for traffic on.
    listenPort: 12345

    # -- Scheme is needed for readiness probes. If enabling tls in your configs, set to "HTTPS"
    listenScheme: HTTP

    # --  Base path where the UI is exposed.
    uiPathPrefix: /

    # -- Enables sending Grafana Labs anonymous usage stats to help improve Grafana
    # Alloy.
    enableReporting: true

    # -- Maps all the keys on a ConfigMap or Secret as environment variables. https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.24/#envfromsource-v1-core
    envFrom: []

    # -- Extra args to pass to `alloy run`: https://grafana.com/docs/alloy/latest/reference/cli/run/
    extraArgs: []

    # -- Extra ports to expose on the Alloy container.
    extraPorts:
      - name: 'syslog'
        port: 514
        targetPort: 1514
        protocol: 'UDP'
        appProtocol: 'syslog'

      - name: 'otel-grpc'
        port: 4317
        targetPort: 4317
        protocol: 'TCP'

      - name: 'otel-http'
        port: 4318
        targetPort: 4318
        protocol: 'TCP'

      - name: 'jaeger-grpc'
        port: 14250
        targetPort: 14250
        protocol: 'TCP'

      - name: 'jaeger-http'
        port: 14268
        targetPort: 14268
        protocol: 'TCP'

      - name: 'jaeger-binary'
        port: 6832
        targetPort: 6832
        protocol: 'TCP'

      - name: 'jaeger-compact'
        port: 6831
        targetPort: 6831
        protocol: 'TCP'

      - name: 'loki-write'
        port: 9999
        targetPort: 9999
        protocol: 'TCP'
        appProtocol: 'http'

    # -- Host aliases to add to the Alloy container.
    hostAliases: []
    # - ip: "20.21.22.23"
    #   hostnames:
    #     - "company.grafana.net"

    mounts:
      # -- Mount /var/log from the host into the container for log collection.
      varlog: false
      # -- Mount /var/lib/docker/containers from the host into the container for log
      # collection.
      dockercontainers: false

      # -- Extra volume mounts to add into the Grafana Alloy container. Does not
      # affect the watch container.
      extra: []

    # -- Security context to apply to the Grafana Alloy container.
    securityContext: {}

    # -- Resource requests and limits to apply to the Grafana Alloy container.
    resources: {}

    # -- Set lifecycle hooks for the Grafana Alloy container.
    lifecycle: {}
      # preStop:
      #   exec:
      #     command:
      #     - /bin/sleep
      #     - "10"

  image:
    # -- Grafana Alloy image registry (defaults to docker.io)
    registry: "docker.io"
    # -- Grafana Alloy image repository.
    repository: grafana/alloy
    # -- (string) Grafana Alloy image tag. When empty, the Chart's appVersion is
    # used.
    tag: null
    # -- Grafana Alloy image's SHA256 digest (either in format "sha256:XYZ" or "XYZ"). When set, will override `image.tag`.
    digest: null
    # -- Grafana Alloy image pull policy.
    pullPolicy: IfNotPresent
    # -- Optional set of image pull secrets.
    pullSecrets: []

  rbac:
    # -- Whether to create RBAC resources for Alloy.
    create: true

  serviceAccount:
    # -- Whether to create a service account for the Grafana Alloy deployment.
    create: true
    # -- Additional labels to add to the created service account.
    additionalLabels: {}
    # -- Annotations to add to the created service account.
    annotations: {}
    # -- The name of the existing service account to use when
    # serviceAccount.create is false.
    name: null

  # Options for the extra controller used for config reloading.
  configReloader:
    # -- Enables automatically reloading when the Alloy config changes.
    enabled: true
    image:
      # -- Config reloader image registry (defaults to docker.io)
      registry: "ghcr.io"
      # -- Repository to get config reloader image from.
      repository: jimmidyson/configmap-reload
      # -- Tag of image to use for config reloading.
      tag: v0.14.0
      # -- SHA256 digest of image to use for config reloading (either in format "sha256:XYZ" or "XYZ"). When set, will override `configReloader.image.tag`
      digest: ""
    # -- Override the args passed to the container.
    customArgs: []
    # -- Resource requests and limits to apply to the config reloader container.
    resources:
      requests:
        cpu: "1m"
        memory: "5Mi"
    # -- Security context to apply to the Grafana configReloader container.
    securityContext: {}

  controller:
    # -- Type of controller to use for deploying Grafana Alloy in the cluster.
    # Must be one of 'daemonset', 'deployment', or 'statefulset'.
    type: 'daemonset'

    # -- Number of pods to deploy. Ignored when controller.type is 'daemonset'.
    replicas: 1

    # -- Annotations to add to controller.
    extraAnnotations: {}

    # -- Whether to deploy pods in parallel. Only used when controller.type is
    # 'statefulset'.
    parallelRollout: true

    # -- Configures Pods to use the host network. When set to true, the ports that will be used must be specified.
    hostNetwork: false

    # -- Configures Pods to use the host PID namespace.
    hostPID: false

    # -- Configures the DNS policy for the pod. https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy
    dnsPolicy: ClusterFirst

    # -- Termination grace period in seconds for the Grafana Alloy pods.
    # The default value used by Kubernetes if unspecifed is 30 seconds.
    terminationGracePeriodSeconds: null

    # -- Update strategy for updating deployed Pods.
    updateStrategy: {}

    # -- nodeSelector to apply to Grafana Alloy pods.
    nodeSelector: {}

    # -- Tolerations to apply to Grafana Alloy pods.
    tolerations: []

    # -- Topology Spread Constraints to apply to Grafana Alloy pods.
    topologySpreadConstraints: []

    # -- priorityClassName to apply to Grafana Alloy pods.
    priorityClassName: ''

    # -- Extra pod annotations to add.
    podAnnotations: {}

    # -- Extra pod labels to add.
    podLabels:
      logs: loki-myloginspace

    # -- PodDisruptionBudget configuration.
    podDisruptionBudget:
      # -- Whether to create a PodDisruptionBudget for the controller.
      enabled: false
      # -- Minimum number of pods that must be available during a disruption.
      # Note: Only one of minAvailable or maxUnavailable should be set.
      minAvailable: null
      # -- Maximum number of pods that can be unavailable during a disruption.
      # Note: Only one of minAvailable or maxUnavailable should be set.
      maxUnavailable: null

    # -- Whether to enable automatic deletion of stale PVCs due to a scale down operation, when controller.type is 'statefulset'.
    enableStatefulSetAutoDeletePVC: false

    autoscaling:
      # -- Creates a HorizontalPodAutoscaler for controller type deployment.
      enabled: false
      # -- The lower limit for the number of replicas to which the autoscaler can scale down.
      minReplicas: 1
      # -- The upper limit for the number of replicas to which the autoscaler can scale up.
      maxReplicas: 5
      # -- Average CPU utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetCPUUtilizationPercentage` to 0 will disable CPU scaling.
      targetCPUUtilizationPercentage: 0
      # -- Average Memory utilization across all relevant pods, a percentage of the requested value of the resource for the pods. Setting `targetMemoryUtilizationPercentage` to 0 will disable Memory scaling.
      targetMemoryUtilizationPercentage: 80

      scaleDown:
        # -- List of policies to determine the scale-down behavior.
        policies: []
          # - type: Pods
          #   value: 4
          #   periodSeconds: 60
        # -- Determines which of the provided scaling-down policies to apply if multiple are specified.
        selectPolicy: Max
        # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling down.
        stabilizationWindowSeconds: 300

      scaleUp:
        # -- List of policies to determine the scale-up behavior.
        policies: []
          # - type: Pods
          #   value: 4
          #   periodSeconds: 60
        # -- Determines which of the provided scaling-up policies to apply if multiple are specified.
        selectPolicy: Max
        # -- The duration that the autoscaling mechanism should look back on to make decisions about scaling up.
        stabilizationWindowSeconds: 0

    # -- Affinity configuration for pods.
    affinity: {}

    volumes:
      # -- Extra volumes to add to the Grafana Alloy pod.
      extra: []

    # -- volumeClaimTemplates to add when controller.type is 'statefulset'.
    volumeClaimTemplates: []

    ## -- Additional init containers to run.
    ## ref: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/
    ##
    initContainers: []

    # -- Additional containers to run alongside the Alloy container and initContainers.
    extraContainers: []

  service:
    # -- Creates a Service for the controller's pods.
    enabled: true
    # -- Service type
    type: ClusterIP
    # -- NodePort port. Only takes effect when `service.type: NodePort`
    nodePort: 31128
    # -- Cluster IP, can be set to None, empty "" or an IP address
    clusterIP: ''
    # -- Value for internal traffic policy. 'Cluster' or 'Local'
    internalTrafficPolicy: Cluster

  serviceMonitor:
    enabled: false
    # -- Additional labels for the service monitor.
    additionalLabels: {}
    # -- Scrape interval. If not set, the Prometheus default scrape interval is used.
    interval: ""
    # -- MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    metricRelabelings: []
    # - action: keep
    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
    #   sourceLabels: [__name__]

    # -- Customize tls parameters for the service monitor
    tlsConfig: {}

    # -- RelabelConfigs to apply to samples before scraping
    # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    relabelings: []
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace
  ingress:
    # -- Enables ingress for Alloy (Faro port)
    enabled: false
    # For Kubernetes >= 1.18 you should specify the ingress-controller via the field ingressClassName
    # See https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/#specifying-the-class-of-an-ingress
    # ingressClassName: nginx
    # Values can be templated
    annotations:
      {}
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
    labels: {}
    path: /
    faroPort: 12347

    # pathType is only for k8s >= 1.1=
    pathType: Prefix

    hosts:
      - chart-example.local
    ## Extra paths to prepend to every host configuration. This is useful when working with annotation based services.
    extraPaths: []
    # - path: /*
    #   backend:
    #     serviceName: ssl-redirect
    #     servicePort: use-annotation
    ## Or for k8s > 1.19
    # - path: /*
    #   pathType: Prefix
    #   backend:
    #     service:
    #       name: ssl-redirect
    #       port:
    #         name: use-annotation

    tls: []
    #  - secretName: chart-example-tls
    #    hosts:
    #      - chart-example.local

  # -- Extra k8s manifests to deploy
  extraObjects: []
  # - apiVersion: v1
  #   kind: Secret
  #   metadata:
  #     name: grafana-cloud
  #   stringData:
  #     PROMETHEUS_HOST: 'https://prometheus-us-central1.grafana.net/api/prom/push'
  #     PROMETHEUS_USERNAME: '123456'


vector:

  enabled: false
  # Default values for Vector
  # See Vector helm documentation to learn more:
  # https://vector.dev/docs/setup/installation/package-managers/helm/

  # nameOverride -- Override the name of resources.
  nameOverride: ""

  # fullnameOverride -- Override the full name of resources.
  fullnameOverride: ""

  # role -- [Role](https://vector.dev/docs/setup/deployment/roles/) for this Vector instance, valid options are:
  # "Agent", "Aggregator", and "Stateless-Aggregator".

  # Each role is created with the following workloads:
  # Agent = DaemonSet
  # Aggregator = StatefulSet
  # Stateless-Aggregator = Deployment
  role: "Stateless-Aggregator"

  # replicas -- Specify the number of Pods to create. Valid for the "Aggregator" and "Stateless-Aggregator" roles.
  replicas: 1

  # Adding additional entries with hostAliases
  hostAliases: []
  # - ip: "127.0.0.1"
  #   hostnames:
  #   - "foo.local"
  #   - "bar.local"


  # podManagementPolicy -- Specify the [podManagementPolicy](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#pod-management-policies)
  # for the StatefulSet. Valid for the "Aggregator" role.
  podManagementPolicy: OrderedReady


  # podLabels -- Set labels on Vector Pods.
  podLabels:
    vector.dev/exclude: "true"

  # podPriorityClassName -- Set the [priorityClassName](https://kubernetes.io/docs/concepts/scheduling-eviction/pod-priority-preemption/#priorityclass)
  # on Vector Pods.
  podPriorityClassName: ""
  # Configuration for Vector's Service.
  service:
    # service.enabled -- If true, create and provide a Service resource for Vector.
    enabled: false
    # service.type -- Set the type for Vector's Service.
    type: "ClusterIP"
    # service.annotations -- Set annotations on Vector's Service.
    annotations: {}
    # service.topologyKeys -- Specify the [topologyKeys](https://kubernetes.io/docs/concepts/services-networking/service-topology/#using-service-topology)
    # field on Vector's Service.
    topologyKeys: []
    #   - "kubernetes.io/hostname"
    #   - "topology.kubernetes.io/zone"
    #   - "topology.kubernetes.io/region"
    #   - "*"
    # service.ports -- Manually set the Service ports, overriding automated generation of Service ports.
    ports: []
    # service.externalTrafficPolicy -- Specify the [externalTrafficPolicy](https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip).
    externalTrafficPolicy: ""
    # service.internalTrafficPolicy -- Specify the [internalTrafficPolicy](https://kubernetes.io/docs/concepts/services-networking/service-traffic-policy).
    internalTrafficPolicy: ""
    # service.loadBalancerIP -- Specify the [loadBalancerIP](https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer).
    loadBalancerIP: ""
    # service.ipFamilyPolicy -- Configure [IPv4/IPv6 dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/).
    ipFamilyPolicy: ""
    # service.ipFamilies -- Configure [IPv4/IPv6 dual-stack](https://kubernetes.io/docs/concepts/services-networking/dual-stack/).
    ipFamilies: []

  # Configuration for Vector's Headless Service.
  serviceHeadless:
    # serviceHeadless.enabled -- If true, create and provide a Headless Service resource for Vector.
    enabled: false

  # customConfig -- Override Vector's default configs, if used **all** options need to be specified. This section supports
  # using helm templates to populate dynamic values. See Vector's [configuration documentation](https://vector.dev/docs/reference/configuration/)
  # for all options.
  customConfig:
    data_dir: /vector-data-dir
    api:
      enabled: false
      address: 127.0.0.1:8686
      playground: false
    sources:
      syslog:
        type: socket
        address: 0.0.0.0:1514
        mode: udp

      talos_kernel_logs:
        type: socket
        address: 0.0.0.0:6050
        mode: udp
        max_length: 102400
        decoding:
          codec: json
        host_key: __host

      talos_service_logs:
        type: socket
        address: 0.0.0.0:6051
        mode: udp
        max_length: 102400
        decoding:
          codec: json
        host_key: __host

    transforms:
      cisco_parser:
        type: remap
        inputs:
          - syslog
        source: |
          . = parse_grok!(.message, "^<%{NUMBER:syslog_pri}>%{NUMBER:seq_num}: %{DATA:device}: %{MONTH:month} %{NUMBER:day} %{TIME:time}: %%{DATA:log_identifier}: %{GREEDYDATA:message}")

      talos_kernel_logs_xform:
        type: remap
        inputs:
          - talos_kernel_logs
        source: |-
          .__host = replace!(.__host, r'172\.16\.20\..+', "infra2")
          .__host = replace!(.__host, r'172\.31\.241\..+', "node2")

      talos_service_logs_xform:
        type: remap
        inputs:
          - talos_service_logs
        source: |-
          .__host = replace!(.__host, r'172\.16\.20\..+', "infra2")
          .__host = replace(.__host, r'172\.31\.241\..+', "node2")


    sinks:
      loki:
        type: loki
        inputs:
          - cisco_parser
        compression: snappy
        endpoint: http://dc1-k3s-node1-collectors-alloy.core-prod.svc.cluster.local:9999
        encoding:
          codec: json
        labels:
          job: syslog

      talos_kernel:
        type: loki
        inputs:
          - talos_kernel_logs_xform
        endpoint: http://dc1-k3s-node1-collectors-alloy.core-prod.svc.cluster.local:9999
        encoding:
          codec: json
          except_fields:
            - __host
        batch:
          max_bytes: 1048576
        out_of_order_action: rewrite_timestamp
        labels:
          hostname: >-
                    {{`{{ __host }}`}}
          facility: >-
                    {{`{{ facility }}`}}

      talos_service:
        type: loki
        inputs:
          - talos_service_logs_xform
        endpoint: http://dc1-k3s-node1-collectors-alloy.core-prod.svc.cluster.local:9999
        encoding:
          codec: json
          except_fields:
            - __host
        batch:
          max_bytes: 400000
        out_of_order_action: rewrite_timestamp
        labels:
          hostname: >-
                    {{`{{ __host }}`}}
          service: >-
                    {{`{{ "talos-service" }}`}}

  # defaultVolumes -- Default volumes that are mounted into pods. In most cases, these should not be changed.
  # Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.
  # @default -- See `values.yaml`
  defaultVolumes: []

  # defaultVolumeMounts -- Default volume mounts. Corresponds to `volumes`.
  # @default -- See `values.yaml`
  defaultVolumeMounts: []

  # extraVolumes -- Additional Volumes to use with Vector Pods.
  extraVolumes: []

  # extraVolumeMounts -- Additional Volume to mount into Vector Containers.
  extraVolumeMounts: []

  # initContainers -- Init Containers to be added to the Vector Pods.
  initContainers: []

  # extraContainers -- Extra Containers to be added to the Vector Pods.
  extraContainers: []

  # Configuration for Vector's data persistence.
  persistence:
    # persistence.enabled -- If true, create and use PersistentVolumeClaims.
    enabled: false

    hostPath:
      # persistence.hostPath.enabled -- If true, use hostPath persistence. Valid for the "Agent" role, if it's disabled
      # the "Agent" role will use emptyDir.
      enabled: false

  # dnsPolicy -- Specify the [dnsPolicy](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-s-dns-policy)
  # for Vector Pods.
  dnsPolicy: ClusterFirst

  # dnsConfig -- Specify the [dnsConfig](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#pod-dns-config)
  # options for Vector Pods.
  dnsConfig: {}
    # nameservers:
    #   - 1.2.3.4
    # searches:
    #   - ns1.svc.cluster-domain.example
    #   - my.dns.search.suffix
    # options:
    #   - name: ndots
    #     value: "2"
    #   - name: edns0

  # shareProcessNamespace -- Specify the [shareProcessNamespace](https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/)
  # options for Vector Pods.
  shareProcessNamespace: false

  # livenessProbe -- Override default liveness probe settings. If customConfig is used, requires customConfig.api.enabled
  # to be set to true.
  livenessProbe: {}
    # httpGet:
    #   path: /health
    #   port: api

  # readinessProbe -- Override default readiness probe settings. If customConfig is used,
  # requires customConfig.api.enabled to be set to true.
  readinessProbe: {}
    # httpGet:
    #   path: /health
    #   port: api

  # Configure a PodMonitor for Vector, requires the PodMonitor CRD to be installed.
  podMonitor:
    # podMonitor.enabled -- If true, create a PodMonitor for Vector.
    enabled: false
    # podMonitor.jobLabel -- Override the label to retrieve the job name from.
    jobLabel: app.kubernetes.io/name
    # podMonitor.port -- Override the port to scrape.
    port: prom-exporter
    # podMonitor.path -- Override the path to scrape.
    path: /metrics
    # podMonitor.interval -- Override the interval at which metrics should be scraped.
    interval:
    # podMonitor.scrapeTimeout -- Override the timeout after which the scrape is ended.
    scrapeTimeout:
    # podMonitor.relabelings -- [RelabelConfigs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config)
    # to apply to samples before scraping.
    relabelings: []
    # podMonitor.metricRelabelings -- [MetricRelabelConfigs](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#metric_relabel_configs)
    # to apply to samples before ingestion.
    metricRelabelings: []
    # podMonitor.podTargetLabels -- [podTargetLabels](https://prometheus-operator.dev/docs/operator/api/#monitoring.coreos.com/v1.PodMonitorSpec)
    # transfers labels on the Kubernetes Pod onto the target.
    podTargetLabels: []
    # podMonitor.additionalLabels -- Adds additional labels to the PodMonitor.
    additionalLabels: {}
    # podMonitor.honorLabels -- If true, honor_labels is set to true in the [scrape config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).
    honorLabels: false
    # podMonitor.honorTimestamps -- If true, honor_timestamps is set to true in the [scrape config](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config).
    honorTimestamps: true

  # Log level for Vector.
  logLevel: "info"

  # Optional built-in HAProxy load balancer.
  haproxy:
    # haproxy.enabled -- If true, create a HAProxy load balancer.
    enabled: false

  # extraObjects -- Create extra manifests via values. Would be passed through `tpl` for templating.
  extraObjects: []
    # - apiVersion: v1
    #   kind: ConfigMap
    #   metadata:
    #     name: vector-dashboards
    #     labels:
    #       grafana_dashboard: "1"
    #   data:
    #     vector.json: |
    #       {{ .Files.Get "dashboards/vector.json" | fromJson | toJson }}
